{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "2242a5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3 import PPO, A2C, DDPG, SAC, DQN\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize, VecMonitor\n",
    "from stable_baselines3.common.vec_env.subproc_vec_env import SubprocVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "import torch\n",
    "import optuna\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import IntProgress, HTML, VBox\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "2517c3c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA GPU is available. Using the GPU for training and evaluation: NVIDIA GeForce RTX 5070\n"
     ]
    }
   ],
   "source": [
    "# Check for and use a CUDA-enabled GPU if available.\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    print(f\"CUDA GPU is available. Using the GPU for training and evaluation: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(\"CUDA GPU not found. Using the CPU for training and evaluation.\")\n",
    "\n",
    "# --- Environment for Continuous Action Spaces (PPO, A2C, DDPG, SAC) ---\n",
    "# NOTE: This is the user-provided TEGEnvironment class which fixed the NaN problem,\n",
    "# with updates to the `reset` and `step` methods to be compatible with Gym v0.26+.\n",
    "class TEGEnvironment(gym.Env):\n",
    "    \"\"\"\n",
    "    A custom Gym environment for a Thermoelectric Generator (TEG) system with a continuous action space.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(TEGEnvironment, self).__init__()\n",
    "\n",
    "        # Action space: Charge rate, Store rate, Idle rate (continuous, proportions)\n",
    "        self.action_space = spaces.Box(low=0, high=1, shape=(3,), dtype=np.float32)\n",
    "\n",
    "        # State space: Voltage, Current, Battery Level, Buffer Level, Temp Gradient,\n",
    "        # Energy Demand, Battery Health\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=np.array([0, 0, 0, 0, 0, 0, 0]),\n",
    "            high=np.array([10, 10, 100, 100, 100, 50, 100]),\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "        self.logs = []\n",
    "        self.np_random = np.random.RandomState()\n",
    "        self.max_steps = 1000  # Define a maximum number of steps for truncation\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        \"\"\"\n",
    "        Resets the environment. Updated to return `observation, info` as per Gym v0.26+.\n",
    "        \"\"\"\n",
    "        if seed is not None:\n",
    "            self.np_random.seed(seed)\n",
    "            random.seed(seed)\n",
    "            pass\n",
    "\n",
    "        self.voltage = random.uniform(5, 7)\n",
    "        self.current = self.voltage / 10\n",
    "        self.battery_level = 50\n",
    "        self.buffer_level = 10\n",
    "        self.temperature_gradient = random.uniform(40, 60)\n",
    "        self.energy_demand = 10\n",
    "        self.battery_health = 100\n",
    "        self.current_step = 0\n",
    "        self.logs = []\n",
    "        \n",
    "        # Return observation and an empty info dictionary\n",
    "        return self._get_observation(), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Takes a step in the environment. Updated to return 5 values as per Gym v0.26+.\n",
    "        \"\"\"\n",
    "        self.current_step += 1\n",
    "\n",
    "        # Normalize actions\n",
    "        total_action = np.sum(action)\n",
    "        if total_action > 1:\n",
    "            action = action / total_action\n",
    "\n",
    "        charge_action, store_action, idle_action = action\n",
    "        available_energy = self.voltage * self.current\n",
    "\n",
    "        # Loss factors and efficiencies\n",
    "        charge_efficiency = 0.85\n",
    "        store_efficiency = 0.80\n",
    "        load_efficiency = 0.75\n",
    "        system_losses = 0.1\n",
    "\n",
    "        # Calculate energy allocations\n",
    "        energy_to_charge = available_energy * charge_action * charge_efficiency\n",
    "        energy_to_store = available_energy * store_action * store_efficiency\n",
    "        delivered_energy = available_energy * idle_action * load_efficiency\n",
    "\n",
    "        # Update battery and buffer levels\n",
    "        self.battery_level += energy_to_charge\n",
    "        self.battery_level = np.clip(self.battery_level, 0, 100)\n",
    "\n",
    "        self.buffer_level += energy_to_store\n",
    "        self.buffer_level = np.clip(self.buffer_level, 0, 100)\n",
    "\n",
    "        # Update battery health\n",
    "        self.battery_health -= charge_action * 0.2\n",
    "        self.battery_health = np.clip(self.battery_health, 0, 100)\n",
    "\n",
    "        # Calculate efficiency (Carnot limit)\n",
    "        thot = self.temperature_gradient\n",
    "        tcold = thot - 5\n",
    "        carnot_efficiency = (thot - tcold) / thot\n",
    "        net_efficiency = carnot_efficiency * load_efficiency * (1 - system_losses)\n",
    "\n",
    "        # Calculate reward\n",
    "        unmet_demand = max(self.energy_demand - delivered_energy, 0)\n",
    "        reward = -unmet_demand\n",
    "        reward += net_efficiency * 10\n",
    "        reward -= max(0, self.battery_level - 95)\n",
    "\n",
    "        # Calculate maximum possible reward (assumes perfect efficiency and no unmet demand)\n",
    "        max_possible_reward = 0\n",
    "        max_possible_reward += 10 * net_efficiency\n",
    "\n",
    "        # Calculate regret (difference between max possible and actual reward)\n",
    "        regret = max_possible_reward - reward\n",
    "\n",
    "        # Update environment variables\n",
    "        self.voltage = max(5 + random.gauss(0, 0.5), 0)\n",
    "        self.temperature_gradient = max(50 + random.gauss(0, 5), 0)\n",
    "        self.energy_demand = max(10 + random.gauss(0, 2), 0)\n",
    "\n",
    "        self._log_data(net_efficiency, reward, regret, delivered_energy)\n",
    "\n",
    "        # Check termination conditions\n",
    "        terminated = self.battery_health <= 0 or self.battery_level <= 0\n",
    "        truncated = self.current_step >= self.max_steps\n",
    "        \n",
    "        # Return observation, reward, terminated, truncated, and an empty info dictionary\n",
    "        return self._get_observation(), reward, terminated, truncated, {}\n",
    "\n",
    "    def _get_observation(self):\n",
    "        return np.array([\n",
    "            self.voltage, self.current, self.battery_level,\n",
    "            self.buffer_level, self.temperature_gradient,\n",
    "            self.energy_demand, self.battery_health\n",
    "        ], dtype=np.float32) # \n",
    "\n",
    "    def _log_data(self, efficiency, reward, regret, delivered_energy):\n",
    "        log_entry = {\n",
    "            \"Thot\": self.temperature_gradient,\n",
    "            \"Power\": self.voltage * self.current,\n",
    "            \"Qhot\": self.battery_level,\n",
    "            \"Qcold\": self.buffer_level,\n",
    "            \"Efficiency\": efficiency * 100,\n",
    "            \"Reward\": reward,\n",
    "            \"Regret\": regret,\n",
    "            \"Battery Health\": self.battery_health,\n",
    "            # \"Energy Demand Fulfilled\": max(self.energy_demand - reward, 0),\n",
    "            \"Energy Demand\": self.energy_demand, # Log energy demand to calculate fulfillment rate per episode\n",
    "            \"Energy Demand Fulfilled\": delivered_energy\n",
    "        }\n",
    "        self.logs.append(log_entry)\n",
    "\n",
    "    def get_logs(self):\n",
    "        return pd.DataFrame(self.logs)\n",
    "\n",
    "# --- Environment for Discrete Action Space (DQN) ---\n",
    "# This is a new version of the discrete environment based on your new TEG logic.\n",
    "class TEGDiscreteEnvironment(gym.Env):\n",
    "    \"\"\"\n",
    "    A custom Gym environment for the TEG system with a discrete action space,\n",
    "    designed to be compatible with DQN.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(TEGDiscreteEnvironment, self).__init__()\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=np.array([0, 0, 0, 0, 0, 0, 0]),\n",
    "            high=np.array([10, 10, 100, 100, 100, 50, 100]),\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "        # Discrete action space: 0 = high charge, 1 = low charge, 2 = idle\n",
    "        self.action_space = spaces.Discrete(3)\n",
    "        self.logs = []\n",
    "        self.np_random = np.random.RandomState()\n",
    "        self.max_steps = 1000\n",
    "\n",
    "        # Map discrete actions to continuous rates.\n",
    "        self.action_map = {\n",
    "            0: [0.8, 0.1, 0.1],  # High charge rate\n",
    "            1: [0.3, 0.4, 0.3],  # Low charge rate\n",
    "            2: [0.1, 0.1, 0.8]   # Idle (prioritize buffering)\n",
    "        }\n",
    "        self.reset()\n",
    "\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        if seed is not None:\n",
    "            self.np_random.seed(seed)\n",
    "            random.seed(seed)\n",
    "\n",
    "        self.voltage = random.uniform(5, 7)\n",
    "        self.current = self.voltage / 10\n",
    "        self.battery_level = 50\n",
    "        self.buffer_level = 10\n",
    "        self.temperature_gradient = random.uniform(40, 60)\n",
    "        self.energy_demand = 10\n",
    "        self.battery_health = 100\n",
    "        self.current_step = 0\n",
    "        self.logs = []\n",
    "        \n",
    "        return self._get_observation(), {}\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        self.current_step += 1\n",
    "        \n",
    "        # Get the continuous action from the discrete action map\n",
    "        charge_action, store_action, idle_action = self.action_map[action]\n",
    "        available_energy = self.voltage * self.current\n",
    "\n",
    "        # Loss factors and efficiencies\n",
    "        charge_efficiency = 0.85\n",
    "        store_efficiency = 0.80\n",
    "        load_efficiency = 0.75\n",
    "        system_losses = 0.1\n",
    "\n",
    "        # Calculate energy allocations\n",
    "        energy_to_charge = available_energy * charge_action * charge_efficiency\n",
    "        energy_to_store = available_energy * store_action * store_efficiency\n",
    "        delivered_energy = available_energy * idle_action * load_efficiency\n",
    "\n",
    "        # Update battery and buffer levels\n",
    "        self.battery_level += energy_to_charge\n",
    "        self.battery_level = np.clip(self.battery_level, 0, 100)\n",
    "\n",
    "        self.buffer_level += energy_to_store\n",
    "        self.buffer_level = np.clip(self.buffer_level, 0, 100)\n",
    "\n",
    "        # Update battery health\n",
    "        self.battery_health -= charge_action * 0.2\n",
    "        self.battery_health = np.clip(self.battery_health, 0, 100)\n",
    "\n",
    "        # Calculate efficiency (Carnot limit)\n",
    "        thot = self.temperature_gradient\n",
    "        tcold = thot - 5\n",
    "        carnot_efficiency = (thot - tcold) / thot\n",
    "        net_efficiency = carnot_efficiency * load_efficiency * (1 - system_losses)\n",
    "\n",
    "        # Calculate reward\n",
    "        unmet_demand = max(self.energy_demand - delivered_energy, 0)\n",
    "        reward = -unmet_demand\n",
    "        reward += net_efficiency * 10\n",
    "        reward -= max(0, self.battery_level - 95)\n",
    "\n",
    "        # Calculate maximum possible reward (assumes perfect efficiency and no unmet demand)\n",
    "        max_possible_reward = 0\n",
    "        max_possible_reward += 10 * net_efficiency\n",
    "\n",
    "        # Calculate regret (difference between max possible and actual reward)\n",
    "        regret = max_possible_reward - reward\n",
    "\n",
    "        # Update environment variables\n",
    "        self.voltage = max(5 + random.gauss(0, 0.5), 0)\n",
    "        self.temperature_gradient = max(50 + random.gauss(0, 5), 0)\n",
    "        self.energy_demand = max(10 + random.gauss(0, 2), 0)\n",
    "\n",
    "        self._log_data(net_efficiency, reward, regret, delivered_energy)\n",
    "\n",
    "        terminated = self.battery_health <= 0 or self.battery_level <= 0\n",
    "        truncated = self.current_step >= self.max_steps\n",
    "        \n",
    "        return self._get_observation(), reward, terminated, truncated, {}\n",
    "\n",
    "\n",
    "    def _get_observation(self):\n",
    "        return np.array([\n",
    "            self.voltage, self.current, self.battery_level,\n",
    "            self.buffer_level, self.temperature_gradient,\n",
    "            self.energy_demand, self.battery_health\n",
    "        ], dtype=np.float32)\n",
    "\n",
    "    def _log_data(self, efficiency, reward, regret, delivered_energy):\n",
    "        log_entry = {\n",
    "            \"Thot\": self.temperature_gradient,\n",
    "            \"Power\": self.voltage * self.current,\n",
    "            \"Qhot\": self.battery_level,\n",
    "            \"Qcold\": self.buffer_level,\n",
    "            \"Efficiency\": efficiency * 100,\n",
    "            \"Reward\": reward,\n",
    "            \"Regret\": regret,\n",
    "            \"Battery Health\": self.battery_health,\n",
    "            #\"Energy Demand Fulfilled\": max(self.energy_demand - reward, 0)\n",
    "            \"Energy Demand\": self.energy_demand, # Log energy demand to calculate fulfillment rate per episode\n",
    "            \"Energy Demand Fulfilled\": delivered_energy\n",
    "        }\n",
    "        self.logs.append(log_entry)\n",
    "\n",
    "    def get_logs(self):\n",
    "        return pd.DataFrame(self.logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ad954c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(df):\n",
    "    \"\"\"Calculates key performance metrics from a DataFrame of evaluation logs.\"\"\"\n",
    "    metrics = {\n",
    "        \"Average Reward\": df[\"Reward\"].mean() if \"Reward\" in df.columns else None,\n",
    "        \"Average Battery Health\": df[\"Battery Health\"].mean() if \"Battery Health\" in df.columns else None,\n",
    "        \"Average Efficiency\": df[\"Efficiency\"].mean() if \"Efficiency\" in df.columns else None,\n",
    "        \"Average Regret\": df[\"Regret\"].mean() if \"Regret\" in df.columns else None,\n",
    "        \"Energy Fulfillment Rate\": (df[\"Energy Demand Fulfilled\"].sum() / df[\"Energy Demand\"].sum()) * 100 if \"Energy Demand\" in df.columns and \"Energy Demand Fulfilled\" in df.columns and df[\"Energy Demand\"].sum() > 0 else None,\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "def plot_results(df: pd.DataFrame, title_prefix: str, save_path: str, max_steps: int):\n",
    "    \"\"\"Generates a series of plots from evaluation logs and saves them.\"\"\"\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "    x = [i for i in range(len(df))]\n",
    "\n",
    "    # Plot 1: Efficiency Over Evaluation Steps\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(x,  df[\"Efficiency\"], label=\"Efficiency (%)\", color='b', linewidth=0.5)\n",
    "    plt.title(f\"Efficiency Over Evaluation Steps ({title_prefix})\", fontsize=16)\n",
    "    plt.xlabel(\"Steps\", fontsize=14)\n",
    "    plt.ylabel(\"Efficiency (%)\", fontsize=14)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_path, \"efficiency_plot.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # Plot 2: Power vs. Temperature Gradient (remains a scatter plot)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.scatter(df[\"Thot\"], df[\"Power\"], label=\"Power (W)\", c=\"r\", alpha=0.7, s=50)\n",
    "    plt.title(f\"Power vs. Temperature Gradient ({title_prefix})\", fontsize=16)\n",
    "    plt.xlabel(\"Temperature Gradient (Thot)\", fontsize=14)\n",
    "    plt.ylabel(\"Power (W)\", fontsize=14)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_path, \"power_vs_temp_plot.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # Plot 3: Battery and Buffer Levels Over Evaluation Steps\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(x, df[\"Qhot\"], label=\"Battery Level\", color='g', linewidth=2)\n",
    "    plt.plot(x, df[\"Qcold\"], label=\"Buffer Level\", color='orange', linewidth=2)\n",
    "    plt.title(f\"Battery and Buffer Levels Over Evaluation Steps ({title_prefix})\", fontsize=16)\n",
    "    plt.xlabel(\"Steps\", fontsize=14)\n",
    "    plt.ylabel(\"Energy Levels\", fontsize=14)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_path, \"battery_buffer_levels_plot.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # Plot 4: Cumulative Reward Over Evaluation Steps (remains cumulative)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot([i for i in range(len(df[\"Reward\"].cumsum()))], df[\"Reward\"].cumsum(), label=\"Cumulative Reward\", color='b', alpha=0.8, linewidth=2)\n",
    "    plt.title(f\"Cumulative Reward Over Evaluation Steps ({title_prefix})\", fontsize=16)\n",
    "    plt.xlabel(\"Steps\", fontsize=14)\n",
    "    plt.ylabel(\"Cumulative Reward\", fontsize=14)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_path, \"cumulative_reward_plot.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # Plot 5: Battery Health Over Evaluation Steps\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(x, df[\"Battery Health\"], label=\"Battery Health\", color='purple', linewidth=2)\n",
    "    plt.title(f\"Battery Health Over Evaluation Steps ({title_prefix})\", fontsize=16)\n",
    "    plt.xlabel(\"Steps\", fontsize=14)\n",
    "    plt.ylabel(\"Battery Health (%)\", fontsize=14)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_path, \"battery_health_plot.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # Plot 6: Regret Over Evaluation Steps\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(x, df[\"Regret\"], label=\"Regret\", color='r', linewidth=0.5)\n",
    "    plt.fill_between(x, df[\"Regret\"], color='red', alpha=0.4)\n",
    "    plt.title(f\"Regret Over Time ({title_prefix})\", fontsize=16)\n",
    "    plt.xlabel(\"Steps\", fontsize=14)\n",
    "    plt.ylabel(\"Regret\", fontsize=14)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_path, \"regret_over_time_plot.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # Plot 7: Energy Demand Fulfillment Rate Over Evaluation Steps\n",
    "    # Calculate the fulfillment rate for each step\n",
    "    fulfillment_rate_per_step = (df[\"Energy Demand Fulfilled\"] / df[\"Energy Demand\"]) * 100\n",
    "    fulfillment_rate_per_step.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    fulfillment_rate_per_step.fillna(100, inplace=True) # Assume 100% fulfillment if no demand\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(x, fulfillment_rate_per_step, color='cyan', label='Fulfillment Rate', linewidth=0.5)\n",
    "    plt.fill_between(x, fulfillment_rate_per_step, color='cyan', alpha=0.4)\n",
    "    plt.title(f\"Energy Demand Fulfillment Rate Over Evaluation Steps ({title_prefix})\", fontsize=16)\n",
    "    plt.xlabel(\"Steps\", fontsize=14)\n",
    "    plt.ylabel(\"Fulfillment Rate (%)\", fontsize=14)\n",
    "    plt.ylim(0)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_path, \"demand_fulfillment_rate_plot.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # Plot 8: Battery Level vs. Time (Single Episode)\n",
    "    single_episode_df = df.iloc[:max_steps]\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(single_episode_df.index, single_episode_df[\"Qhot\"], label=\"Battery Level\", color='green', linewidth=2)\n",
    "    plt.title(f\"Battery Level Over a Single Episode ({title_prefix})\", fontsize=16)\n",
    "    plt.xlabel(\"Steps\", fontsize=14)\n",
    "    plt.ylabel(\"Battery Level\", fontsize=14)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_path, \"single_episode_battery_level_plot.png\"))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec1a8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_comparison_results(df_tuned: pd.DataFrame, df_benchmark: pd.DataFrame, title_prefix: str, save_path: str):\n",
    "    \"\"\"Generates a series of comparison plots from evaluation logs and saves them.\"\"\"\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "    # Ensure the save path for comparison plots exists\n",
    "    comparison_save_path = os.path.join(save_path, \"comparison_plots\")\n",
    "    os.makedirs(comparison_save_path, exist_ok=True)\n",
    "\n",
    "    # Common x-axis for time-series plots\n",
    "    len_tuned = len(df_tuned)\n",
    "    len_benchmark = len(df_benchmark)\n",
    "    x_tuned = range(len_tuned)\n",
    "    x_benchmark = range(len_benchmark)\n",
    "\n",
    "    # Plot 1: Cumulative Reward Over Evaluation Steps\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(x_tuned, df_tuned[\"Reward\"].cumsum(), label=\"Tuned Model\", color='blue', linewidth=2)\n",
    "    plt.plot(x_benchmark, df_benchmark[\"Reward\"].cumsum(), label=\"Benchmark Model\", color='red', linestyle='--', linewidth=2)\n",
    "    plt.title(f\"Comparison: Cumulative Reward ({title_prefix})\", fontsize=16)\n",
    "    plt.xlabel(\"Steps\", fontsize=14)\n",
    "    plt.ylabel(\"Cumulative Reward\", fontsize=14)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(comparison_save_path, \"comparison_cumulative_reward_plot.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # Plot 2: Battery Health Over Evaluation Steps\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(x_tuned, df_tuned[\"Battery Health\"], label=\"Tuned Model\", color='green', linewidth=1.5)\n",
    "    plt.plot(x_benchmark, df_benchmark[\"Battery Health\"], label=\"Benchmark Model\", color='orange', linestyle='--', linewidth=1.5)\n",
    "    plt.title(f\"Comparison: Battery Health Over Steps ({title_prefix})\", fontsize=16)\n",
    "    plt.xlabel(\"Steps\", fontsize=14)\n",
    "    plt.ylabel(\"Battery Health (%)\", fontsize=14)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(comparison_save_path, \"comparison_battery_health_plot.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # Plot 3: Efficiency Over Evaluation Steps\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(x_tuned, df_tuned[\"Efficiency\"], label=\"Tuned Model\", color='purple', linewidth=1)\n",
    "    plt.plot(x_benchmark, df_benchmark[\"Efficiency\"], label=\"Benchmark Model\", color='brown', linestyle='--', linewidth=1)\n",
    "    plt.title(f\"Comparison: Efficiency Over Steps ({title_prefix})\", fontsize=16)\n",
    "    plt.xlabel(\"Steps\", fontsize=14)\n",
    "    plt.ylabel(\"Efficiency (%)\", fontsize=14)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(comparison_save_path, \"comparison_efficiency_plot.png\"))\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot 4: Energy Demand Fulfillment Rate\n",
    "    fulfillment_tuned = (df_tuned[\"Energy Demand Fulfilled\"] / df_tuned[\"Energy Demand\"]).fillna(1) * 100\n",
    "    fulfillment_benchmark = (df_benchmark[\"Energy Demand Fulfilled\"] / df_benchmark[\"Energy Demand\"]).fillna(1) * 100\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(x_tuned, fulfillment_tuned.rolling(window=50).mean(), label=\"Tuned Model (50-step avg)\", color='cyan', linewidth=2)\n",
    "    plt.plot(x_benchmark, fulfillment_benchmark.rolling(window=50).mean(), label=\"Benchmark Model (50-step avg)\", color='magenta', linestyle='--', linewidth=2)\n",
    "    plt.title(f\"Comparison: Energy Demand Fulfillment Rate ({title_prefix})\", fontsize=16)\n",
    "    plt.xlabel(\"Steps\", fontsize=14)\n",
    "    plt.ylabel(\"Fulfillment Rate (%)\", fontsize=14)\n",
    "    plt.ylim(0, 105)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(comparison_save_path, \"comparison_fulfillment_rate_plot.png\"))\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"Comparison plots saved to: {comparison_save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "637847d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "HYPERPARAMETER_SPACES = {\n",
    "    \"PPO\": lambda trial: {\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-5, 1e-3, log=True),\n",
    "        'n_steps': trial.suggest_categorical('n_steps', [512, 1024, 2048, 4096]),\n",
    "        'batch_size': trial.suggest_categorical('batch_size', [64, 128, 256, 512]),\n",
    "        'gamma': trial.suggest_float('gamma', 0.9, 0.999),\n",
    "        'gae_lambda': trial.suggest_float('gae_lambda', 0.9, 0.99),\n",
    "        'clip_range': trial.suggest_float('clip_range', 0.1, 0.4),\n",
    "        'n_epochs': trial.suggest_int('n_epochs', 5, 20)\n",
    "        # 'learning_rate': 0.0003, 'n_steps': 2048, 'batch_size': 64, 'ent_coef': 0.01\n",
    "    },\n",
    "    \"A2C\": lambda trial: {\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-5, 1e-3, log=True),\n",
    "        'n_steps': trial.suggest_int('n_steps', 5, 50),\n",
    "        'gamma': trial.suggest_float('gamma', 0.9, 0.999),\n",
    "        'gae_lambda': trial.suggest_float('gae_lambda', 0.9, 1.0),\n",
    "        'ent_coef': trial.suggest_float('ent_coef', 1e-8, 1e-1, log=True),\n",
    "        'vf_coef': trial.suggest_float('vf_coef', 0.1, 1.0)\n",
    "    },\n",
    "    \"DDPG\": lambda trial: {\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-5, 1e-3, log=True),\n",
    "        'buffer_size': trial.suggest_int('buffer_size', 10000, 100000),\n",
    "        'learning_starts': trial.suggest_int('learning_starts', 100, 1000),\n",
    "        'tau': trial.suggest_float('tau', 0.001, 0.01),\n",
    "        'gamma': trial.suggest_float('gamma', 0.9, 0.999)\n",
    "    },\n",
    "    \"SAC\": lambda trial: {\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-5, 1e-3, log=True),\n",
    "        'buffer_size': trial.suggest_int('buffer_size', 10000, 100000),\n",
    "        'learning_starts': trial.suggest_int('learning_starts', 100, 1000),\n",
    "        'gamma': trial.suggest_float('gamma', 0.9, 0.999),\n",
    "        'tau': trial.suggest_float('tau', 0.001, 0.01),\n",
    "        'ent_coef': trial.suggest_float('ent_coef', 1e-8, 1e-1, log=True)\n",
    "    },\n",
    "    \"DQN\": lambda trial: {\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-5, 1e-3, log=True),\n",
    "        'buffer_size': trial.suggest_int('buffer_size', 10000, 100000),\n",
    "        'learning_starts': trial.suggest_int('learning_starts', 100, 1000),\n",
    "        'gamma': trial.suggest_float('gamma', 0.9, 0.999),\n",
    "        'exploration_fraction': trial.suggest_float('exploration_fraction', 0.1, 0.5),\n",
    "        'exploration_final_eps': trial.suggest_float('exploration_final_eps', 0.01, 0.1),\n",
    "        'train_freq': trial.suggest_int('train_freq', 1, 10),\n",
    "        'target_update_interval': trial.suggest_int('target_update_interval', 100, 1000)\n",
    "    }\n",
    "}\n",
    "\n",
    "def objective(trial: optuna.Trial, model_name: str, timesteps: int) -> float:\n",
    "    \"\"\"\n",
    "    Defines the objective function for Optuna to optimize a given RL model.\n",
    "    It suggests hyperparameters, trains a model, and returns its average reward.\n",
    "    \"\"\"\n",
    "    if model_name == \"DQN\":\n",
    "        env_class = TEGDiscreteEnvironment\n",
    "        policy_name = \"MlpPolicy\"\n",
    "    else:\n",
    "        env_class = TEGEnvironment\n",
    "        policy_name = \"MlpPolicy\"\n",
    "        \n",
    "    hyperparams = HYPERPARAMETER_SPACES[model_name](trial)\n",
    "\n",
    "    # PPO-specific check to ensure batch_size is a factor of n_steps\n",
    "    if model_name == \"PPO\":\n",
    "        n_steps = hyperparams['n_steps']\n",
    "        batch_size = hyperparams['batch_size']\n",
    "        if n_steps % batch_size != 0:\n",
    "            return -np.inf # Prune this trial\n",
    "    \n",
    "    model_class = globals()[model_name]\n",
    "\n",
    "    try:\n",
    "        env = make_vec_env(lambda: env_class(), n_envs=1)\n",
    "        \n",
    "        model = model_class(\n",
    "            policy_name,\n",
    "            env,\n",
    "            **hyperparams,\n",
    "            verbose=0,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        model.learn(total_timesteps=timesteps)\n",
    "\n",
    "        eval_env = make_vec_env(lambda: env_class(), n_envs=1)\n",
    "        mean_reward, _ = evaluate_policy(model, eval_env, n_eval_episodes=10)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Trial for {model_name} failed with error: {e}\")\n",
    "        return -np.inf\n",
    "\n",
    "    return mean_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999358a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Default Hyperparameters for Benchmark Models ---\n",
    "DEFAULT_HYPERPARAMS = {\n",
    "    \"PPO\": {},\n",
    "    \"A2C\": {},\n",
    "    \"DDPG\": {},\n",
    "    \"SAC\": {},\n",
    "    \"DQN\": {}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "75e03511",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptunaProgressCallback:\n",
    "    \"\"\"Callback to update a progress bar during Optuna optimization.\"\"\"\n",
    "    def __init__(self, progress_bar, desc_widget, total_trials):\n",
    "        self.progress_bar = progress_bar\n",
    "        self.desc_widget = desc_widget\n",
    "        self.total_trials = total_trials\n",
    "\n",
    "    def __call__(self, study, trial):\n",
    "        self.progress_bar.value = trial.number + 1\n",
    "        self.desc_widget.value = f'Tuning model: <b>{study.study_name.split(\"_\")[0]}</b>, Trial {trial.number + 1}/{self.total_trials}'\n",
    "\n",
    "class TrainingProgressCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback to update a progress bar during Stable-Baselines3 training.\n",
    "    The max value is set in _on_training_start, where total_timesteps is available.\n",
    "    \"\"\"\n",
    "    def __init__(self, progress_bar, verbose=0):\n",
    "        super(TrainingProgressCallback, self).__init__(verbose)\n",
    "        self.progress_bar = progress_bar\n",
    "\n",
    "    def _on_training_start(self) -> None:\n",
    "        \"\"\"Called once at the beginning of training.\"\"\"\n",
    "        self.progress_bar.max = self.locals['total_timesteps']\n",
    "        self.progress_bar.value = self.num_timesteps\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        \"\"\"Called every step.\"\"\"\n",
    "        self.progress_bar.value = self.num_timesteps\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cdc788",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "647c79b151394d9997c0117097e98144",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='Overall Progress: <b>0</b>/5 models completed.'), IntProgress(value=0, description=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Optuna optimization for PPO with 50 trials ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7eee588ea9e4ae48ab00e6953ce42e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='Tuning model: <b>PPO</b>, Trial 0/50'), IntProgress(value=0, description='Tuning PP…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-08 14:39:26,830] A new study created in memory with name: PPO_optimization\n",
      "/mnt/c/Data/python_venv/ML_GPU_DL_PyTorch/lib/python3.12/site-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n",
      "[I 2025-08-08 14:39:57,172] Trial 0 finished with value: -9311.428436499998 and parameters: {'learning_rate': 0.0003698918825149074, 'n_steps': 4096, 'batch_size': 64, 'gamma': 0.9345577526796096, 'gae_lambda': 0.9148641183683719, 'clip_range': 0.11266113105354143, 'n_epochs': 15}. Best is trial 0 with value: -9311.428436499998.\n",
      "[I 2025-08-08 14:40:15,682] Trial 1 finished with value: -8994.1130931 and parameters: {'learning_rate': 0.000436938990553852, 'n_steps': 2048, 'batch_size': 64, 'gamma': 0.9466581992922739, 'gae_lambda': 0.9753799099343455, 'clip_range': 0.34568782800139725, 'n_epochs': 5}. Best is trial 1 with value: -8994.1130931.\n",
      "[I 2025-08-08 14:40:33,629] Trial 2 finished with value: -9284.945012700002 and parameters: {'learning_rate': 1.4764379768804826e-05, 'n_steps': 4096, 'batch_size': 512, 'gamma': 0.953886759437041, 'gae_lambda': 0.9748548774041051, 'clip_range': 0.17870428043355974, 'n_epochs': 7}. Best is trial 1 with value: -8994.1130931.\n",
      "[I 2025-08-08 14:41:02,155] Trial 3 finished with value: -9053.171397199998 and parameters: {'learning_rate': 5.057866824111238e-05, 'n_steps': 2048, 'batch_size': 64, 'gamma': 0.9533984043815039, 'gae_lambda': 0.9040053403571734, 'clip_range': 0.16960507301853703, 'n_epochs': 19}. Best is trial 1 with value: -8994.1130931.\n",
      "[I 2025-08-08 14:41:23,296] Trial 4 finished with value: -9326.367478600001 and parameters: {'learning_rate': 0.00017997311601434835, 'n_steps': 2048, 'batch_size': 128, 'gamma': 0.9017844932205878, 'gae_lambda': 0.9317331185038724, 'clip_range': 0.2973676956121172, 'n_epochs': 14}. Best is trial 1 with value: -8994.1130931.\n",
      "[I 2025-08-08 14:41:47,913] Trial 5 finished with value: -8865.907178600002 and parameters: {'learning_rate': 1.1617423392986079e-05, 'n_steps': 4096, 'batch_size': 128, 'gamma': 0.9538959650761957, 'gae_lambda': 0.9083711605329936, 'clip_range': 0.25363960781782213, 'n_epochs': 20}. Best is trial 5 with value: -8865.907178600002.\n",
      "[I 2025-08-08 14:42:02,715] Trial 6 finished with value: -11179.095736 and parameters: {'learning_rate': 0.0009765262713563897, 'n_steps': 512, 'batch_size': 256, 'gamma': 0.9701362078731545, 'gae_lambda': 0.9534440301096119, 'clip_range': 0.2616515103245929, 'n_epochs': 5}. Best is trial 5 with value: -8865.907178600002.\n",
      "[I 2025-08-08 14:42:20,565] Trial 7 finished with value: -8049.851854199999 and parameters: {'learning_rate': 0.00012728640590128597, 'n_steps': 1024, 'batch_size': 128, 'gamma': 0.9115592830213375, 'gae_lambda': 0.9484841040101061, 'clip_range': 0.3028875667315164, 'n_epochs': 11}. Best is trial 7 with value: -8049.851854199999.\n",
      "[I 2025-08-08 14:42:40,922] Trial 8 finished with value: -12694.504653 and parameters: {'learning_rate': 0.00017657504455173252, 'n_steps': 1024, 'batch_size': 128, 'gamma': 0.9424273815312181, 'gae_lambda': 0.9612410362954317, 'clip_range': 0.2150720411719082, 'n_epochs': 16}. Best is trial 7 with value: -8049.851854199999.\n",
      "[I 2025-08-08 14:43:00,327] Trial 9 finished with value: -13418.862837700002 and parameters: {'learning_rate': 0.00022474276527181364, 'n_steps': 1024, 'batch_size': 128, 'gamma': 0.9669938816234658, 'gae_lambda': 0.9859915036057949, 'clip_range': 0.3893454751628237, 'n_epochs': 14}. Best is trial 7 with value: -8049.851854199999.\n",
      "[I 2025-08-08 14:43:16,014] Trial 10 finished with value: -8163.251141399999 and parameters: {'learning_rate': 5.343340268419719e-05, 'n_steps': 1024, 'batch_size': 256, 'gamma': 0.9000597397020365, 'gae_lambda': 0.9313705187983362, 'clip_range': 0.331480947801307, 'n_epochs': 10}. Best is trial 7 with value: -8049.851854199999.\n",
      "[I 2025-08-08 14:43:31,857] Trial 11 finished with value: -8531.6825393 and parameters: {'learning_rate': 5.060752845452482e-05, 'n_steps': 1024, 'batch_size': 256, 'gamma': 0.9067923597389704, 'gae_lambda': 0.9335877267298667, 'clip_range': 0.31334187830891186, 'n_epochs': 10}. Best is trial 7 with value: -8049.851854199999.\n",
      "[I 2025-08-08 14:43:47,818] Trial 12 finished with value: -8588.752262 and parameters: {'learning_rate': 5.945663524689138e-05, 'n_steps': 1024, 'batch_size': 256, 'gamma': 0.9216896083320674, 'gae_lambda': 0.9377542092286791, 'clip_range': 0.3711350644118213, 'n_epochs': 11}. Best is trial 7 with value: -8049.851854199999.\n",
      "[I 2025-08-08 14:44:02,991] Trial 13 finished with value: -9207.4556565 and parameters: {'learning_rate': 2.903547063933235e-05, 'n_steps': 1024, 'batch_size': 512, 'gamma': 0.922402518843615, 'gae_lambda': 0.9201637750579039, 'clip_range': 0.316533478188218, 'n_epochs': 9}. Best is trial 7 with value: -8049.851854199999.\n",
      "[I 2025-08-08 14:44:19,574] Trial 14 finished with value: -7928.4254771000005 and parameters: {'learning_rate': 8.84857669197869e-05, 'n_steps': 512, 'batch_size': 256, 'gamma': 0.9179988552335991, 'gae_lambda': 0.948374862000395, 'clip_range': 0.3491065735470075, 'n_epochs': 12}. Best is trial 14 with value: -7928.4254771000005.\n",
      "[I 2025-08-08 14:44:38,342] Trial 15 finished with value: -12755.9563488 and parameters: {'learning_rate': 0.0001022764864161075, 'n_steps': 512, 'batch_size': 128, 'gamma': 0.9949810240851389, 'gae_lambda': 0.9507978216614597, 'clip_range': 0.27953689021751027, 'n_epochs': 12}. Best is trial 14 with value: -7928.4254771000005.\n",
      "[I 2025-08-08 14:44:55,860] Trial 16 finished with value: -8687.593022699999 and parameters: {'learning_rate': 0.00010408214104619219, 'n_steps': 512, 'batch_size': 256, 'gamma': 0.9165514336048926, 'gae_lambda': 0.9630897602155275, 'clip_range': 0.3490155049052581, 'n_epochs': 17}. Best is trial 14 with value: -7928.4254771000005.\n",
      "[I 2025-08-08 14:45:11,823] Trial 17 finished with value: -9267.576685 and parameters: {'learning_rate': 2.801453382421955e-05, 'n_steps': 512, 'batch_size': 512, 'gamma': 0.9315396734332279, 'gae_lambda': 0.9435045875701296, 'clip_range': 0.22097162176437202, 'n_epochs': 8}. Best is trial 14 with value: -7928.4254771000005.\n",
      "[I 2025-08-08 14:45:29,044] Trial 18 finished with value: -7414.1173085 and parameters: {'learning_rate': 0.00038096853736227276, 'n_steps': 512, 'batch_size': 256, 'gamma': 0.9124676591349445, 'gae_lambda': 0.960714349941338, 'clip_range': 0.398073381008349, 'n_epochs': 13}. Best is trial 18 with value: -7414.1173085.\n",
      "[I 2025-08-08 14:45:45,735] Trial 19 finished with value: -7011.537005900001 and parameters: {'learning_rate': 0.0008604066316960873, 'n_steps': 512, 'batch_size': 256, 'gamma': 0.9295150242395958, 'gae_lambda': 0.9626841661543701, 'clip_range': 0.3905623505793241, 'n_epochs': 13}. Best is trial 19 with value: -7011.537005900001.\n",
      "[I 2025-08-08 14:46:03,273] Trial 20 finished with value: -7092.8036431 and parameters: {'learning_rate': 0.0008116902470686473, 'n_steps': 512, 'batch_size': 256, 'gamma': 0.9320504298638215, 'gae_lambda': 0.9668253568884416, 'clip_range': 0.39790401320574437, 'n_epochs': 14}. Best is trial 19 with value: -7011.537005900001.\n",
      "[I 2025-08-08 14:46:20,738] Trial 21 finished with value: -7194.5497755999995 and parameters: {'learning_rate': 0.0009423624867575974, 'n_steps': 512, 'batch_size': 256, 'gamma': 0.9366683651539515, 'gae_lambda': 0.9684757335350895, 'clip_range': 0.39430832185521747, 'n_epochs': 14}. Best is trial 19 with value: -7011.537005900001.\n",
      "[I 2025-08-08 14:46:37,787] Trial 22 finished with value: -7083.7211412999995 and parameters: {'learning_rate': 0.0009723881035988358, 'n_steps': 512, 'batch_size': 256, 'gamma': 0.9331014098436509, 'gae_lambda': 0.9718959739234208, 'clip_range': 0.3745677794025861, 'n_epochs': 17}. Best is trial 19 with value: -7011.537005900001.\n",
      "[I 2025-08-08 14:46:56,222] Trial 23 finished with value: -7984.709217799999 and parameters: {'learning_rate': 0.0006056612427221627, 'n_steps': 512, 'batch_size': 256, 'gamma': 0.9283329643951449, 'gae_lambda': 0.9874977942389984, 'clip_range': 0.3691385524023266, 'n_epochs': 18}. Best is trial 19 with value: -7011.537005900001.\n",
      "[I 2025-08-08 14:47:13,458] Trial 24 finished with value: -11946.730321899999 and parameters: {'learning_rate': 0.0006158137999009062, 'n_steps': 512, 'batch_size': 256, 'gamma': 0.9261262685374366, 'gae_lambda': 0.97812727478242, 'clip_range': 0.3686011085237359, 'n_epochs': 16}. Best is trial 19 with value: -7011.537005900001.\n",
      "[I 2025-08-08 14:47:30,677] Trial 25 finished with value: -7909.3736462 and parameters: {'learning_rate': 0.0006449971929795517, 'n_steps': 512, 'batch_size': 256, 'gamma': 0.9417899487151074, 'gae_lambda': 0.9675618267020918, 'clip_range': 0.37285545547653565, 'n_epochs': 16}. Best is trial 19 with value: -7011.537005900001.\n",
      "[I 2025-08-08 14:47:47,872] Trial 26 finished with value: -8156.7819718 and parameters: {'learning_rate': 0.00030042230842613923, 'n_steps': 512, 'batch_size': 256, 'gamma': 0.959794631079328, 'gae_lambda': 0.9571647420841275, 'clip_range': 0.34047804944721854, 'n_epochs': 18}. Best is trial 19 with value: -7011.537005900001.\n",
      "[I 2025-08-08 14:48:05,027] Trial 27 finished with value: -7478.764866499999 and parameters: {'learning_rate': 0.0007976053553973297, 'n_steps': 512, 'batch_size': 256, 'gamma': 0.9379223386294924, 'gae_lambda': 0.9807639962388401, 'clip_range': 0.3964321444833493, 'n_epochs': 13}. Best is trial 19 with value: -7011.537005900001.\n",
      "[I 2025-08-08 14:48:22,957] Trial 28 finished with value: -8099.000578199999 and parameters: {'learning_rate': 0.0005123965392032334, 'n_steps': 512, 'batch_size': 256, 'gamma': 0.9854234560299509, 'gae_lambda': 0.9698095015511577, 'clip_range': 0.3644981794799505, 'n_epochs': 15}. Best is trial 19 with value: -7011.537005900001.\n",
      "[I 2025-08-08 14:48:53,717] Trial 29 finished with value: -9263.0331389 and parameters: {'learning_rate': 0.0002821624101320004, 'n_steps': 4096, 'batch_size': 64, 'gamma': 0.9325171145146618, 'gae_lambda': 0.9711936095750033, 'clip_range': 0.10575110345984978, 'n_epochs': 15}. Best is trial 19 with value: -7011.537005900001.\n",
      "[I 2025-08-08 14:49:10,647] Trial 30 finished with value: -9379.495638299999 and parameters: {'learning_rate': 0.0007834735170818226, 'n_steps': 512, 'batch_size': 512, 'gamma': 0.946366824892874, 'gae_lambda': 0.9818578694908647, 'clip_range': 0.2887599621806912, 'n_epochs': 17}. Best is trial 19 with value: -7011.537005900001.\n",
      "[I 2025-08-08 14:49:28,576] Trial 31 finished with value: -7156.8147379 and parameters: {'learning_rate': 0.0009708187047937049, 'n_steps': 512, 'batch_size': 256, 'gamma': 0.9405063060555088, 'gae_lambda': 0.9669195846424262, 'clip_range': 0.38541793567865645, 'n_epochs': 14}. Best is trial 19 with value: -7011.537005900001.\n",
      "[I 2025-08-08 14:49:46,650] Trial 32 finished with value: -12258.4027576 and parameters: {'learning_rate': 0.00045949923580084867, 'n_steps': 512, 'batch_size': 256, 'gamma': 0.9350706121601277, 'gae_lambda': 0.9652607122255934, 'clip_range': 0.38169888223465753, 'n_epochs': 14}. Best is trial 19 with value: -7011.537005900001.\n",
      "[I 2025-08-08 14:50:12,239] Trial 33 finished with value: -8402.7524722 and parameters: {'learning_rate': 0.0007679447749253951, 'n_steps': 2048, 'batch_size': 64, 'gamma': 0.9271992543873366, 'gae_lambda': 0.9566835185912997, 'clip_range': 0.33166899596668026, 'n_epochs': 15}. Best is trial 19 with value: -7011.537005900001.\n",
      "[I 2025-08-08 14:50:31,306] Trial 34 finished with value: -8906.904758699999 and parameters: {'learning_rate': 0.000988051553841956, 'n_steps': 4096, 'batch_size': 256, 'gamma': 0.9474273015529723, 'gae_lambda': 0.9741396776575686, 'clip_range': 0.3576645961424184, 'n_epochs': 13}. Best is trial 19 with value: -7011.537005900001.\n",
      "[I 2025-08-08 14:50:56,453] Trial 35 finished with value: -12797.3059602 and parameters: {'learning_rate': 0.0004806454138438192, 'n_steps': 2048, 'batch_size': 64, 'gamma': 0.9603118246155066, 'gae_lambda': 0.976649663715787, 'clip_range': 0.15428685652893187, 'n_epochs': 17}. Best is trial 19 with value: -7011.537005900001.\n",
      "[I 2025-08-08 14:51:13,845] Trial 36 finished with value: -8707.4739624 and parameters: {'learning_rate': 0.0003057530207135797, 'n_steps': 512, 'batch_size': 256, 'gamma': 0.9518118605793552, 'gae_lambda': 0.957187817020295, 'clip_range': 0.13487698914549118, 'n_epochs': 20}. Best is trial 19 with value: -7011.537005900001.\n",
      "[I 2025-08-08 14:51:30,092] Trial 37 finished with value: -7543.119676499999 and parameters: {'learning_rate': 0.0006328593006150831, 'n_steps': 512, 'batch_size': 256, 'gamma': 0.9409462662689552, 'gae_lambda': 0.9723730314613913, 'clip_range': 0.3797961263024609, 'n_epochs': 12}. Best is trial 19 with value: -7011.537005900001.\n",
      "[I 2025-08-08 14:51:48,420] Trial 38 finished with value: -8341.037936 and parameters: {'learning_rate': 0.00040603446377294376, 'n_steps': 4096, 'batch_size': 512, 'gamma': 0.9223909896650956, 'gae_lambda': 0.9648570904429821, 'clip_range': 0.33021698924719595, 'n_epochs': 19}. Best is trial 19 with value: -7011.537005900001.\n",
      "[I 2025-08-08 14:52:04,291] Trial 39 finished with value: -9157.7412723 and parameters: {'learning_rate': 0.0007715402457085138, 'n_steps': 2048, 'batch_size': 256, 'gamma': 0.9315312029439464, 'gae_lambda': 0.9847289117022765, 'clip_range': 0.20998824798214777, 'n_epochs': 11}. Best is trial 19 with value: -7011.537005900001.\n",
      "[I 2025-08-08 14:52:31,013] Trial 40 finished with value: -8843.489680200002 and parameters: {'learning_rate': 0.0005452398208449825, 'n_steps': 512, 'batch_size': 64, 'gamma': 0.9582618440115951, 'gae_lambda': 0.9897544344473229, 'clip_range': 0.3566255803292355, 'n_epochs': 15}. Best is trial 19 with value: -7011.537005900001.\n",
      "[I 2025-08-08 14:52:48,464] Trial 41 finished with value: -8551.0486915 and parameters: {'learning_rate': 0.0009832921410193483, 'n_steps': 512, 'batch_size': 256, 'gamma': 0.9378194793517948, 'gae_lambda': 0.9670454765779084, 'clip_range': 0.3946865740212282, 'n_epochs': 14}. Best is trial 19 with value: -7011.537005900001.\n",
      "[I 2025-08-08 14:53:06,363] Trial 42 finished with value: -8684.2610281 and parameters: {'learning_rate': 0.0009790205485930605, 'n_steps': 512, 'batch_size': 256, 'gamma': 0.9444177530536235, 'gae_lambda': 0.9605371730662629, 'clip_range': 0.39971779419470627, 'n_epochs': 14}. Best is trial 19 with value: -7011.537005900001.\n",
      "[I 2025-08-08 14:53:23,852] Trial 43 finished with value: -7568.1956838 and parameters: {'learning_rate': 0.000764169712121924, 'n_steps': 512, 'batch_size': 256, 'gamma': 0.9379186884642574, 'gae_lambda': 0.9756900727609342, 'clip_range': 0.38127526672211143, 'n_epochs': 13}. Best is trial 19 with value: -7011.537005900001.\n",
      "[I 2025-08-08 14:53:41,757] Trial 44 finished with value: -7684.7439373 and parameters: {'learning_rate': 0.0007358381488026699, 'n_steps': 512, 'batch_size': 256, 'gamma': 0.9492124352445831, 'gae_lambda': 0.9694776950269899, 'clip_range': 0.3880953895013704, 'n_epochs': 16}. Best is trial 19 with value: -7011.537005900001.\n",
      "[I 2025-08-08 14:53:57,472] Trial 45 finished with value: -8353.7394318 and parameters: {'learning_rate': 0.00021336690676583594, 'n_steps': 512, 'batch_size': 256, 'gamma': 0.9279449853610009, 'gae_lambda': 0.9537156723307888, 'clip_range': 0.38047226434001713, 'n_epochs': 6}. Best is trial 19 with value: -7011.537005900001.\n",
      "[I 2025-08-08 14:54:15,347] Trial 46 finished with value: -7057.848857500001 and parameters: {'learning_rate': 0.0008593144231169232, 'n_steps': 512, 'batch_size': 128, 'gamma': 0.9348567734753364, 'gae_lambda': 0.940515591966034, 'clip_range': 0.3431195190309121, 'n_epochs': 10}. Best is trial 19 with value: -7011.537005900001.\n",
      "[I 2025-08-08 14:54:36,963] Trial 47 finished with value: -8255.587481899998 and parameters: {'learning_rate': 0.00038420266679729494, 'n_steps': 4096, 'batch_size': 128, 'gamma': 0.9178452185110911, 'gae_lambda': 0.925141988964581, 'clip_range': 0.31548229028542923, 'n_epochs': 9}. Best is trial 19 with value: -7011.537005900001.\n",
      "[I 2025-08-08 14:54:55,904] Trial 48 finished with value: -8624.2006387 and parameters: {'learning_rate': 0.0005237382069255577, 'n_steps': 2048, 'batch_size': 128, 'gamma': 0.9079243305713116, 'gae_lambda': 0.9414770210222649, 'clip_range': 0.341007056183581, 'n_epochs': 11}. Best is trial 19 with value: -7011.537005900001.\n",
      "[I 2025-08-08 14:55:15,005] Trial 49 finished with value: -8750.381746899999 and parameters: {'learning_rate': 3.192173117467159e-05, 'n_steps': 512, 'batch_size': 128, 'gamma': 0.9225177706958128, 'gae_lambda': 0.9385486640303441, 'clip_range': 0.3567744457759667, 'n_epochs': 10}. Best is trial 19 with value: -7011.537005900001.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- PPO Optimization Finished ---\n",
      "Number of finished trials:  50\n",
      "Best trial:\n",
      "  Value (Average Reward):  -7011.537005900001\n",
      "  Params: \n",
      "    learning_rate: 0.0008604066316960873\n",
      "    n_steps: 512\n",
      "    batch_size: 256\n",
      "    gamma: 0.9295150242395958\n",
      "    gae_lambda: 0.9626841661543701\n",
      "    clip_range: 0.3905623505793241\n",
      "    n_epochs: 13\n",
      "\n",
      "==================================================\n",
      "\n",
      "--- Training the final PPO model with best hyperparameters ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1324591e199346b4b401b2555485fe83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='Training final model: <b>PPO</b>'), IntProgress(value=0, description='Training PPO:…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved to: Output/PPO/best_model.pt\n",
      "\n",
      "--- Evaluating the final PPO model ---\n",
      "Evaluation logs saved to: Output/PPO/evaluation_logs.csv\n",
      "\n",
      "Final PPO Metrics:\n",
      "Average Reward: -7.1392\n",
      "Average Battery Health: 100.0000\n",
      "Average Efficiency: 6.8251\n",
      "Average Regret: 7.8217\n",
      "Energy Fulfillment Rate: 21.8949\n",
      "Evaluation plots saved to: Output/PPO\n",
      "\n",
      "==================================================\n",
      "\n",
      "--- Starting Optuna optimization for A2C with 50 trials ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c5ce52332644c7881d7cb9af898317a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='Tuning model: <b>A2C</b>, Trial 0/50'), IntProgress(value=0, description='Tuning A2…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-08 14:55:48,471] A new study created in memory with name: A2C_optimization\n",
      "/mnt/c/Data/python_venv/ML_GPU_DL_PyTorch/lib/python3.12/site-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run A2C on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n",
      "[I 2025-08-08 14:56:05,052] Trial 0 finished with value: -9229.933847600001 and parameters: {'learning_rate': 3.433938326839536e-05, 'n_steps': 25, 'gamma': 0.9370882484703047, 'gae_lambda': 0.9713879162852699, 'ent_coef': 9.441063000227592e-06, 'vf_coef': 0.25770411864692044}. Best is trial 0 with value: -9229.933847600001.\n",
      "[I 2025-08-08 14:56:20,252] Trial 1 finished with value: -9324.2141225 and parameters: {'learning_rate': 0.0001939414707220086, 'n_steps': 48, 'gamma': 0.9409891592899519, 'gae_lambda': 0.9786975529423152, 'ent_coef': 0.00010289443206054509, 'vf_coef': 0.9863923979481194}. Best is trial 0 with value: -9229.933847600001.\n",
      "[I 2025-08-08 14:56:36,331] Trial 2 finished with value: -9312.53586 and parameters: {'learning_rate': 1.0930808230032253e-05, 'n_steps': 31, 'gamma': 0.9893242253455418, 'gae_lambda': 0.9010467521923611, 'ent_coef': 0.0014721191710618854, 'vf_coef': 0.6694134401918086}. Best is trial 0 with value: -9229.933847600001.\n",
      "[I 2025-08-08 14:56:52,613] Trial 3 finished with value: -9322.680575499999 and parameters: {'learning_rate': 0.00036333993308947815, 'n_steps': 45, 'gamma': 0.9536677687880905, 'gae_lambda': 0.950438494099847, 'ent_coef': 1.2682097275240152e-08, 'vf_coef': 0.17908481436009338}. Best is trial 0 with value: -9229.933847600001.\n",
      "[I 2025-08-08 14:57:10,411] Trial 4 finished with value: -9343.630221300002 and parameters: {'learning_rate': 0.000559342672639399, 'n_steps': 19, 'gamma': 0.9425965145704831, 'gae_lambda': 0.9982068768510943, 'ent_coef': 3.2555616396682064e-05, 'vf_coef': 0.2192323965557405}. Best is trial 0 with value: -9229.933847600001.\n",
      "[I 2025-08-08 14:57:32,537] Trial 5 finished with value: -13095.050982800001 and parameters: {'learning_rate': 9.198891441767105e-05, 'n_steps': 5, 'gamma': 0.9551101171535981, 'gae_lambda': 0.9983419048755879, 'ent_coef': 2.006970405028735e-08, 'vf_coef': 0.5550204967488634}. Best is trial 0 with value: -9229.933847600001.\n",
      "[I 2025-08-08 14:57:46,763] Trial 6 finished with value: -12053.614299600002 and parameters: {'learning_rate': 0.0008168010164098894, 'n_steps': 47, 'gamma': 0.9663189914553707, 'gae_lambda': 0.9019281696468933, 'ent_coef': 9.686838836685752e-05, 'vf_coef': 0.7946131622766306}. Best is trial 0 with value: -9229.933847600001.\n",
      "[I 2025-08-08 14:58:03,069] Trial 7 finished with value: -13262.980067 and parameters: {'learning_rate': 0.0001972793123587909, 'n_steps': 17, 'gamma': 0.9552767273056708, 'gae_lambda': 0.95802403547668, 'ent_coef': 6.4577994093407005e-06, 'vf_coef': 0.21836350817855227}. Best is trial 0 with value: -9229.933847600001.\n",
      "[I 2025-08-08 14:58:18,035] Trial 8 finished with value: -9351.2825175 and parameters: {'learning_rate': 0.00048241740465888205, 'n_steps': 45, 'gamma': 0.9518953809565267, 'gae_lambda': 0.9851239765604645, 'ent_coef': 0.0005142793756009339, 'vf_coef': 0.11898069647838991}. Best is trial 0 with value: -9229.933847600001.\n",
      "[I 2025-08-08 14:58:35,948] Trial 9 finished with value: -8179.9661884 and parameters: {'learning_rate': 0.000954101822337279, 'n_steps': 11, 'gamma': 0.9611042395663253, 'gae_lambda': 0.958868874407202, 'ent_coef': 0.07557642066310581, 'vf_coef': 0.18047889286278496}. Best is trial 9 with value: -8179.9661884.\n",
      "[I 2025-08-08 14:58:56,879] Trial 10 finished with value: -9330.374718800002 and parameters: {'learning_rate': 5.672111997401067e-05, 'n_steps': 6, 'gamma': 0.9177170839158089, 'gae_lambda': 0.9340699446900759, 'ent_coef': 0.07920863503575559, 'vf_coef': 0.4157927799099216}. Best is trial 9 with value: -8179.9661884.\n",
      "[I 2025-08-08 14:59:12,535] Trial 11 finished with value: -9866.4648553 and parameters: {'learning_rate': 2.6588945761096307e-05, 'n_steps': 30, 'gamma': 0.9166047974242307, 'gae_lambda': 0.9626134246437938, 'ent_coef': 0.03694269707102661, 'vf_coef': 0.4176803359119667}. Best is trial 9 with value: -8179.9661884.\n",
      "[I 2025-08-08 14:59:29,162] Trial 12 finished with value: -9328.2137382 and parameters: {'learning_rate': 3.484698534891427e-05, 'n_steps': 19, 'gamma': 0.9805414516916934, 'gae_lambda': 0.938480416774566, 'ent_coef': 8.792187868725097e-07, 'vf_coef': 0.3682056532715927}. Best is trial 9 with value: -8179.9661884.\n",
      "[I 2025-08-08 14:59:46,641] Trial 13 finished with value: -9324.9867451 and parameters: {'learning_rate': 1.3315529505908687e-05, 'n_steps': 13, 'gamma': 0.9276607635236089, 'gae_lambda': 0.9720853452825394, 'ent_coef': 4.27833634772814e-07, 'vf_coef': 0.2979469240012444}. Best is trial 9 with value: -8179.9661884.\n",
      "[I 2025-08-08 15:00:03,526] Trial 14 finished with value: -13110.1659315 and parameters: {'learning_rate': 2.7310553748777946e-05, 'n_steps': 35, 'gamma': 0.9700426879151636, 'gae_lambda': 0.9242230878479936, 'ent_coef': 0.005295228174120777, 'vf_coef': 0.10745625155519928}. Best is trial 9 with value: -8179.9661884.\n",
      "[I 2025-08-08 15:00:20,298] Trial 15 finished with value: -9200.2402769 and parameters: {'learning_rate': 0.0001394967463050878, 'n_steps': 25, 'gamma': 0.9267671464696641, 'gae_lambda': 0.9662682458159694, 'ent_coef': 2.2097678682510296e-06, 'vf_coef': 0.5610549223809111}. Best is trial 9 with value: -8179.9661884.\n",
      "[I 2025-08-08 15:00:38,249] Trial 16 finished with value: -8696.3247509 and parameters: {'learning_rate': 0.00022148590325943984, 'n_steps': 10, 'gamma': 0.9083853045478972, 'gae_lambda': 0.9436455022117527, 'ent_coef': 2.4923251014014437e-07, 'vf_coef': 0.54660979656042}. Best is trial 9 with value: -8179.9661884.\n",
      "[I 2025-08-08 15:00:56,277] Trial 17 finished with value: -6974.2404259 and parameters: {'learning_rate': 0.0009822653834528215, 'n_steps': 11, 'gamma': 0.9018261059252334, 'gae_lambda': 0.9207349091931982, 'ent_coef': 1.3805247094965036e-07, 'vf_coef': 0.769143360103185}. Best is trial 17 with value: -6974.2404259.\n",
      "[I 2025-08-08 15:01:14,713] Trial 18 finished with value: -10867.8902039 and parameters: {'learning_rate': 0.0008541526279649442, 'n_steps': 12, 'gamma': 0.9679615676329973, 'gae_lambda': 0.9139954438520763, 'ent_coef': 1.2988915825036425e-07, 'vf_coef': 0.8356186478105854}. Best is trial 17 with value: -6974.2404259.\n",
      "[I 2025-08-08 15:01:32,063] Trial 19 finished with value: -8230.464730700001 and parameters: {'learning_rate': 0.000999250877548708, 'n_steps': 16, 'gamma': 0.9017216605418056, 'gae_lambda': 0.9255089962126446, 'ent_coef': 0.0088231103820822, 'vf_coef': 0.7398910549170712}. Best is trial 17 with value: -6974.2404259.\n",
      "[I 2025-08-08 15:01:47,732] Trial 20 finished with value: -13791.336971699999 and parameters: {'learning_rate': 0.0003094461504392001, 'n_steps': 36, 'gamma': 0.9920464041355213, 'gae_lambda': 0.948946261836928, 'ent_coef': 6.302918612790356e-08, 'vf_coef': 0.9859361423566471}. Best is trial 17 with value: -6974.2404259.\n",
      "[I 2025-08-08 15:02:04,756] Trial 21 finished with value: -7172.0494517 and parameters: {'learning_rate': 0.0009865660387293976, 'n_steps': 16, 'gamma': 0.9014491895995173, 'gae_lambda': 0.9256364672732398, 'ent_coef': 0.011608842998359709, 'vf_coef': 0.7567332647315768}. Best is trial 17 with value: -6974.2404259.\n",
      "[I 2025-08-08 15:02:24,117] Trial 22 finished with value: -11875.8025329 and parameters: {'learning_rate': 0.0005854183443996586, 'n_steps': 9, 'gamma': 0.900515938265968, 'gae_lambda': 0.9148836174278351, 'ent_coef': 0.020755854785918324, 'vf_coef': 0.677177135823412}. Best is trial 17 with value: -6974.2404259.\n",
      "[I 2025-08-08 15:02:40,242] Trial 23 finished with value: -8724.8876008 and parameters: {'learning_rate': 0.000655436394357111, 'n_steps': 22, 'gamma': 0.9132198622552467, 'gae_lambda': 0.9316037419468493, 'ent_coef': 0.0027841409149598653, 'vf_coef': 0.8632842205186452}. Best is trial 17 with value: -6974.2404259.\n",
      "[I 2025-08-08 15:02:57,197] Trial 24 finished with value: -8141.3114034 and parameters: {'learning_rate': 0.00041931125327102325, 'n_steps': 14, 'gamma': 0.927763291852427, 'gae_lambda': 0.9123677542699792, 'ent_coef': 0.00045137855697594117, 'vf_coef': 0.6283781456920301}. Best is trial 17 with value: -6974.2404259.\n",
      "[I 2025-08-08 15:03:13,798] Trial 25 finished with value: -8800.1699733 and parameters: {'learning_rate': 0.00039426652657981, 'n_steps': 15, 'gamma': 0.9259386824819729, 'gae_lambda': 0.9170703283904753, 'ent_coef': 0.00046745195938134023, 'vf_coef': 0.6284090087438239}. Best is trial 17 with value: -6974.2404259.\n",
      "[I 2025-08-08 15:03:30,329] Trial 26 finished with value: -8822.5919555 and parameters: {'learning_rate': 0.00025819540526100164, 'n_steps': 21, 'gamma': 0.9085470988503676, 'gae_lambda': 0.9080353701153882, 'ent_coef': 0.00098785230373799, 'vf_coef': 0.9125289192444193}. Best is trial 17 with value: -6974.2404259.\n",
      "[I 2025-08-08 15:03:49,898] Trial 27 finished with value: -8957.909008899998 and parameters: {'learning_rate': 0.00041838051941007614, 'n_steps': 8, 'gamma': 0.9215596109035986, 'gae_lambda': 0.923360401022169, 'ent_coef': 0.00026795388945814004, 'vf_coef': 0.7651310423871717}. Best is trial 17 with value: -6974.2404259.\n",
      "[I 2025-08-08 15:04:06,708] Trial 28 finished with value: -8240.7705573 and parameters: {'learning_rate': 0.0006789257712209835, 'n_steps': 13, 'gamma': 0.9331126745081152, 'gae_lambda': 0.9071724226659338, 'ent_coef': 0.012023076454276567, 'vf_coef': 0.6958304183588924}. Best is trial 17 with value: -6974.2404259.\n",
      "[I 2025-08-08 15:04:22,689] Trial 29 finished with value: -9072.724964899999 and parameters: {'learning_rate': 0.0001303300586256748, 'n_steps': 25, 'gamma': 0.9103790040716533, 'gae_lambda': 0.9298091942935619, 'ent_coef': 1.3598543883555319e-05, 'vf_coef': 0.6194147540821097}. Best is trial 17 with value: -6974.2404259.\n",
      "[I 2025-08-08 15:04:38,488] Trial 30 finished with value: -12960.5935561 and parameters: {'learning_rate': 0.0007150942632617322, 'n_steps': 23, 'gamma': 0.9343702677169826, 'gae_lambda': 0.918986699080907, 'ent_coef': 0.00011022427282275083, 'vf_coef': 0.49781803733792646}. Best is trial 17 with value: -6974.2404259.\n",
      "[I 2025-08-08 15:04:56,425] Trial 31 finished with value: -14122.166052999999 and parameters: {'learning_rate': 0.0009844684009993258, 'n_steps': 11, 'gamma': 0.9048644826662038, 'gae_lambda': 0.9399708306170128, 'ent_coef': 0.0641406256150779, 'vf_coef': 0.8222202630178957}. Best is trial 17 with value: -6974.2404259.\n",
      "[I 2025-08-08 15:05:15,650] Trial 32 finished with value: -8203.832546599999 and parameters: {'learning_rate': 0.000480712463151829, 'n_steps': 8, 'gamma': 0.9448727090223933, 'gae_lambda': 0.9527696944369827, 'ent_coef': 0.0034509889472967496, 'vf_coef': 0.7251476553440095}. Best is trial 17 with value: -6974.2404259.\n",
      "[I 2025-08-08 15:05:32,807] Trial 33 finished with value: -12875.272224700002 and parameters: {'learning_rate': 0.0009937269315803484, 'n_steps': 15, 'gamma': 0.9624194532881548, 'gae_lambda': 0.9094295710724973, 'ent_coef': 0.02158336298093204, 'vf_coef': 0.899814344565666}. Best is trial 17 with value: -6974.2404259.\n",
      "[I 2025-08-08 15:05:49,377] Trial 34 finished with value: -7101.636718300001 and parameters: {'learning_rate': 0.0007189398463869974, 'n_steps': 18, 'gamma': 0.9385817443203478, 'gae_lambda': 0.943390588757214, 'ent_coef': 0.0014371356377300807, 'vf_coef': 0.6242908823744799}. Best is trial 17 with value: -6974.2404259.\n",
      "[I 2025-08-08 15:06:05,761] Trial 35 finished with value: -9248.714830500001 and parameters: {'learning_rate': 0.00032121138682992325, 'n_steps': 19, 'gamma': 0.9386782059046956, 'gae_lambda': 0.9212220634921519, 'ent_coef': 0.0015538986139396072, 'vf_coef': 0.6187127915168433}. Best is trial 17 with value: -6974.2404259.\n",
      "[I 2025-08-08 15:06:19,593] Trial 36 finished with value: -7204.853261699999 and parameters: {'learning_rate': 0.0005560039597918102, 'n_steps': 28, 'gamma': 0.9203810611404921, 'gae_lambda': 0.9288746203032067, 'ent_coef': 5.6749702823350965e-05, 'vf_coef': 0.49355593713554424}. Best is trial 17 with value: -6974.2404259.\n",
      "[I 2025-08-08 15:06:35,961] Trial 37 finished with value: -7828.5720931999995 and parameters: {'learning_rate': 0.0005570614008096108, 'n_steps': 28, 'gamma': 0.920144270454892, 'gae_lambda': 0.9428706122043752, 'ent_coef': 3.317207901792e-05, 'vf_coef': 0.4856373051940065}. Best is trial 17 with value: -6974.2404259.\n",
      "[I 2025-08-08 15:06:52,876] Trial 38 finished with value: -9185.874287900002 and parameters: {'learning_rate': 0.0005751456402010786, 'n_steps': 35, 'gamma': 0.9061559470225181, 'gae_lambda': 0.9328889079910346, 'ent_coef': 2.848987030874945e-06, 'vf_coef': 0.7659939064635702}. Best is trial 17 with value: -6974.2404259.\n",
      "[I 2025-08-08 15:07:09,084] Trial 39 finished with value: -8522.891974600001 and parameters: {'learning_rate': 0.0007397581380715532, 'n_steps': 38, 'gamma': 0.9137147031074541, 'gae_lambda': 0.927434389053352, 'ent_coef': 8.23584370221231e-05, 'vf_coef': 0.4966338131594272}. Best is trial 17 with value: -6974.2404259.\n",
      "[I 2025-08-08 15:07:25,293] Trial 40 finished with value: -9336.7116266 and parameters: {'learning_rate': 0.0001540695407113316, 'n_steps': 31, 'gamma': 0.943151939403959, 'gae_lambda': 0.9362425668555758, 'ent_coef': 2.8209301525924876e-08, 'vf_coef': 0.5727760177860483}. Best is trial 17 with value: -6974.2404259.\n",
      "[I 2025-08-08 15:07:41,107] Trial 41 finished with value: -8707.3464444 and parameters: {'learning_rate': 0.0005184938174711204, 'n_steps': 29, 'gamma': 0.9207617148626674, 'gae_lambda': 0.9434705038941567, 'ent_coef': 3.0120418265108258e-05, 'vf_coef': 0.480086270313917}. Best is trial 17 with value: -6974.2404259.\n",
      "[I 2025-08-08 15:07:57,583] Trial 42 finished with value: -7099.6343555 and parameters: {'learning_rate': 0.0007572766309144829, 'n_steps': 27, 'gamma': 0.9000804478423117, 'gae_lambda': 0.9481752117228169, 'ent_coef': 1.1934481161153495e-05, 'vf_coef': 0.42881655933115714}. Best is trial 17 with value: -6974.2404259.\n",
      "[I 2025-08-08 15:08:13,430] Trial 43 finished with value: -9357.6694579 and parameters: {'learning_rate': 0.0007502570506836188, 'n_steps': 26, 'gamma': 0.9002220473761723, 'gae_lambda': 0.9485394392462476, 'ent_coef': 6.661499748436984e-06, 'vf_coef': 0.38141897432468164}. Best is trial 17 with value: -6974.2404259.\n",
      "[I 2025-08-08 15:08:29,646] Trial 44 finished with value: -9321.1896226 and parameters: {'learning_rate': 0.0007816918009755677, 'n_steps': 20, 'gamma': 0.9131168963786294, 'gae_lambda': 0.9523439633862258, 'ent_coef': 1.5133434493425015e-06, 'vf_coef': 0.3367161847045496}. Best is trial 17 with value: -6974.2404259.\n",
      "[I 2025-08-08 15:08:45,912] Trial 45 finished with value: -9304.387463199999 and parameters: {'learning_rate': 0.00047122295269509754, 'n_steps': 39, 'gamma': 0.9041368125417124, 'gae_lambda': 0.928451719500663, 'ent_coef': 0.00017849784845528645, 'vf_coef': 0.4247027299553931}. Best is trial 17 with value: -6974.2404259.\n",
      "[I 2025-08-08 15:09:02,107] Trial 46 finished with value: -10794.756112300001 and parameters: {'learning_rate': 9.25610954676514e-05, 'n_steps': 32, 'gamma': 0.9157670985150882, 'gae_lambda': 0.9575824552235637, 'ent_coef': 0.001155943754800565, 'vf_coef': 0.7045877169861834}. Best is trial 17 with value: -6974.2404259.\n",
      "[I 2025-08-08 15:09:19,171] Trial 47 finished with value: -13356.745518800002 and parameters: {'learning_rate': 0.0002942533282013279, 'n_steps': 17, 'gamma': 0.9097935465096202, 'gae_lambda': 0.9925068390306379, 'ent_coef': 1.48479225476732e-05, 'vf_coef': 0.6555548341354623}. Best is trial 17 with value: -6974.2404259.\n",
      "[I 2025-08-08 15:09:34,997] Trial 48 finished with value: -12588.4830705 and parameters: {'learning_rate': 0.0006573747853429174, 'n_steps': 23, 'gamma': 0.9472763709354562, 'gae_lambda': 0.938810646271171, 'ent_coef': 7.319080908815004e-07, 'vf_coef': 0.29597803415688917}. Best is trial 17 with value: -6974.2404259.\n",
      "[I 2025-08-08 15:09:51,475] Trial 49 finished with value: -14118.930833200002 and parameters: {'learning_rate': 0.0008636133256041603, 'n_steps': 18, 'gamma': 0.9054160667086506, 'gae_lambda': 0.9193886415636421, 'ent_coef': 6.11287518625777e-05, 'vf_coef': 0.4431588417587753}. Best is trial 17 with value: -6974.2404259.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- A2C Optimization Finished ---\n",
      "Number of finished trials:  50\n",
      "Best trial:\n",
      "  Value (Average Reward):  -6974.2404259\n",
      "  Params: \n",
      "    learning_rate: 0.0009822653834528215\n",
      "    n_steps: 11\n",
      "    gamma: 0.9018261059252334\n",
      "    gae_lambda: 0.9207349091931982\n",
      "    ent_coef: 1.3805247094965036e-07\n",
      "    vf_coef: 0.769143360103185\n",
      "\n",
      "==================================================\n",
      "\n",
      "--- Training the final A2C model with best hyperparameters ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76425c12897c4d1baeac8fbef576847c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='Training final model: <b>A2C</b>'), IntProgress(value=0, description='Training A2C:…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved to: Output/A2C/best_model.pt\n",
      "\n",
      "--- Evaluating the final A2C model ---\n",
      "Evaluation logs saved to: Output/A2C/evaluation_logs.csv\n",
      "\n",
      "Final A2C Metrics:\n",
      "Average Reward: -7.1789\n",
      "Average Battery Health: 100.0000\n",
      "Average Efficiency: 6.8247\n",
      "Average Regret: 7.8614\n",
      "Energy Fulfillment Rate: 21.6044\n",
      "Evaluation plots saved to: Output/A2C\n",
      "\n",
      "==================================================\n",
      "\n",
      "--- Starting Optuna optimization for DDPG with 50 trials ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1d6a797d5a347e09bc7cb19cc5b39ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='Tuning model: <b>DDPG</b>, Trial 0/50'), IntProgress(value=0, description='Tuning D…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-08 15:10:29,329] A new study created in memory with name: DDPG_optimization\n",
      "[I 2025-08-08 15:11:17,723] Trial 0 finished with value: -7080.514686499999 and parameters: {'learning_rate': 0.0006359545559868068, 'buffer_size': 59484, 'learning_starts': 601, 'tau': 0.005049254628082494, 'gamma': 0.9418483000241269}. Best is trial 0 with value: -7080.514686499999.\n",
      "[I 2025-08-08 15:12:05,958] Trial 1 finished with value: -7016.016583499999 and parameters: {'learning_rate': 4.5241176846644624e-05, 'buffer_size': 36407, 'learning_starts': 238, 'tau': 0.0048649185482042175, 'gamma': 0.9188743930756865}. Best is trial 1 with value: -7016.016583499999.\n",
      "[I 2025-08-08 15:12:52,867] Trial 2 finished with value: -7066.707883699999 and parameters: {'learning_rate': 1.0535119158558493e-05, 'buffer_size': 94451, 'learning_starts': 511, 'tau': 0.006663484686319702, 'gamma': 0.9127118753098777}. Best is trial 1 with value: -7016.016583499999.\n",
      "[I 2025-08-08 15:13:43,217] Trial 3 finished with value: -7135.711264799999 and parameters: {'learning_rate': 7.1043657108302e-05, 'buffer_size': 56804, 'learning_starts': 151, 'tau': 0.006747530548676035, 'gamma': 0.9936745198633786}. Best is trial 1 with value: -7016.016583499999.\n",
      "[I 2025-08-08 15:14:29,621] Trial 4 finished with value: -7016.203270499999 and parameters: {'learning_rate': 0.00021729659693464496, 'buffer_size': 48309, 'learning_starts': 756, 'tau': 0.007060062400788494, 'gamma': 0.9256729212344945}. Best is trial 1 with value: -7016.016583499999.\n",
      "[I 2025-08-08 15:15:19,463] Trial 5 finished with value: -7136.606081399999 and parameters: {'learning_rate': 6.840698005157705e-05, 'buffer_size': 71927, 'learning_starts': 237, 'tau': 0.004783780473143582, 'gamma': 0.9067969507004927}. Best is trial 1 with value: -7016.016583499999.\n",
      "[I 2025-08-08 15:16:06,371] Trial 6 finished with value: -9308.158765999999 and parameters: {'learning_rate': 5.768339248188522e-05, 'buffer_size': 16527, 'learning_starts': 473, 'tau': 0.007851591615898386, 'gamma': 0.9243895414293976}. Best is trial 1 with value: -7016.016583499999.\n",
      "[I 2025-08-08 15:16:57,231] Trial 7 finished with value: -8161.9716059 and parameters: {'learning_rate': 0.000790666769150824, 'buffer_size': 21881, 'learning_starts': 219, 'tau': 0.006398467432335184, 'gamma': 0.9470043589567014}. Best is trial 1 with value: -7016.016583499999.\n",
      "[I 2025-08-08 15:17:44,821] Trial 8 finished with value: -9312.7072936 and parameters: {'learning_rate': 0.0006107415485958787, 'buffer_size': 83534, 'learning_starts': 428, 'tau': 0.006566157806648364, 'gamma': 0.9194797523511216}. Best is trial 1 with value: -7016.016583499999.\n",
      "[I 2025-08-08 15:18:33,132] Trial 9 finished with value: -7032.0143707 and parameters: {'learning_rate': 0.0002094902015582207, 'buffer_size': 84203, 'learning_starts': 565, 'tau': 0.002977472052480564, 'gamma': 0.9564287936637202}. Best is trial 1 with value: -7016.016583499999.\n",
      "[I 2025-08-08 15:19:18,627] Trial 10 finished with value: -7070.6614976 and parameters: {'learning_rate': 1.8533238853418252e-05, 'buffer_size': 34912, 'learning_starts': 955, 'tau': 0.0018183052635326168, 'gamma': 0.9712175056481851}. Best is trial 1 with value: -7016.016583499999.\n",
      "[I 2025-08-08 15:20:05,312] Trial 11 finished with value: -7135.9027597 and parameters: {'learning_rate': 0.00022825458149711373, 'buffer_size': 39354, 'learning_starts': 801, 'tau': 0.009829555163748246, 'gamma': 0.9302844979638226}. Best is trial 1 with value: -7016.016583499999.\n",
      "[I 2025-08-08 15:20:47,643] Trial 12 finished with value: -7082.889304300001 and parameters: {'learning_rate': 0.00016835719807411869, 'buffer_size': 41062, 'learning_starts': 766, 'tau': 0.003739623773423895, 'gamma': 0.9356912449144736}. Best is trial 1 with value: -7016.016583499999.\n",
      "[I 2025-08-08 15:21:37,173] Trial 13 finished with value: -7091.800076000001 and parameters: {'learning_rate': 3.088820693351222e-05, 'buffer_size': 47900, 'learning_starts': 324, 'tau': 0.008515648597898273, 'gamma': 0.9012193353363929}. Best is trial 1 with value: -7016.016583499999.\n",
      "[I 2025-08-08 15:22:22,355] Trial 14 finished with value: -9340.881669900002 and parameters: {'learning_rate': 0.00012946551933082007, 'buffer_size': 26167, 'learning_starts': 708, 'tau': 0.004089630942925187, 'gamma': 0.9582295740067748}. Best is trial 1 with value: -7016.016583499999.\n",
      "[I 2025-08-08 15:23:07,502] Trial 15 finished with value: -7077.085115100001 and parameters: {'learning_rate': 0.0003480904773000913, 'buffer_size': 64948, 'learning_starts': 918, 'tau': 0.0010051536796842116, 'gamma': 0.9200906501238789}. Best is trial 1 with value: -7016.016583499999.\n",
      "[I 2025-08-08 15:23:54,205] Trial 16 finished with value: -6910.904861100001 and parameters: {'learning_rate': 3.38264058022996e-05, 'buffer_size': 29329, 'learning_starts': 360, 'tau': 0.008280679738572652, 'gamma': 0.9371439373276085}. Best is trial 16 with value: -6910.904861100001.\n",
      "[I 2025-08-08 15:24:48,293] Trial 17 finished with value: -8135.811146800001 and parameters: {'learning_rate': 3.228945097547177e-05, 'buffer_size': 11038, 'learning_starts': 362, 'tau': 0.009742440481102333, 'gamma': 0.973786484970883}. Best is trial 16 with value: -6910.904861100001.\n",
      "[I 2025-08-08 15:25:37,334] Trial 18 finished with value: -7096.076082500001 and parameters: {'learning_rate': 3.517509737761971e-05, 'buffer_size': 29695, 'learning_starts': 114, 'tau': 0.008559951443739887, 'gamma': 0.9377012281714944}. Best is trial 16 with value: -6910.904861100001.\n",
      "[I 2025-08-08 15:26:25,261] Trial 19 finished with value: -7157.2525605 and parameters: {'learning_rate': 1.663957850097927e-05, 'buffer_size': 32503, 'learning_starts': 300, 'tau': 0.005624541035635887, 'gamma': 0.9550376843563756}. Best is trial 16 with value: -6910.904861100001.\n",
      "[I 2025-08-08 15:27:13,526] Trial 20 finished with value: -7158.1006792 and parameters: {'learning_rate': 4.6138122910892026e-05, 'buffer_size': 45945, 'learning_starts': 381, 'tau': 0.0032485437979101724, 'gamma': 0.9134748251897324}. Best is trial 16 with value: -6910.904861100001.\n",
      "[I 2025-08-08 15:28:01,993] Trial 21 finished with value: -6979.133522000001 and parameters: {'learning_rate': 0.00010662452122186343, 'buffer_size': 48904, 'learning_starts': 602, 'tau': 0.007722321485919377, 'gamma': 0.928383248987331}. Best is trial 16 with value: -6910.904861100001.\n",
      "[I 2025-08-08 15:28:49,378] Trial 22 finished with value: -7045.0167398 and parameters: {'learning_rate': 9.502890174816427e-05, 'buffer_size': 24415, 'learning_starts': 658, 'tau': 0.00783939329473946, 'gamma': 0.9303150838130645}. Best is trial 16 with value: -6910.904861100001.\n",
      "[I 2025-08-08 15:29:34,506] Trial 23 finished with value: -13301.419439399999 and parameters: {'learning_rate': 2.2017760452010904e-05, 'buffer_size': 50951, 'learning_starts': 244, 'tau': 0.005667236283121964, 'gamma': 0.9429998137310753}. Best is trial 16 with value: -6910.904861100001.\n",
      "[I 2025-08-08 15:30:24,177] Trial 24 finished with value: -9331.8632558 and parameters: {'learning_rate': 0.00010088115392901927, 'buffer_size': 36339, 'learning_starts': 465, 'tau': 0.008979649880956614, 'gamma': 0.9145058805484333}. Best is trial 16 with value: -6910.904861100001.\n",
      "[I 2025-08-08 15:31:13,039] Trial 25 finished with value: -6997.2298343 and parameters: {'learning_rate': 4.265917794233009e-05, 'buffer_size': 68266, 'learning_starts': 631, 'tau': 0.007643247102015444, 'gamma': 0.9334256796165615}. Best is trial 16 with value: -6910.904861100001.\n",
      "[I 2025-08-08 15:31:58,131] Trial 26 finished with value: -6963.2251257 and parameters: {'learning_rate': 1.1216582392366862e-05, 'buffer_size': 71635, 'learning_starts': 625, 'tau': 0.007572640624815498, 'gamma': 0.963912573989916}. Best is trial 16 with value: -6910.904861100001.\n",
      "[I 2025-08-08 15:32:47,684] Trial 27 finished with value: -8214.0764327 and parameters: {'learning_rate': 1.0264629262377145e-05, 'buffer_size': 77943, 'learning_starts': 538, 'tau': 0.009015201085822605, 'gamma': 0.9678611412422325}. Best is trial 16 with value: -6910.904861100001.\n",
      "[I 2025-08-08 15:33:35,860] Trial 28 finished with value: -8184.6312683 and parameters: {'learning_rate': 1.4685654621185683e-05, 'buffer_size': 61045, 'learning_starts': 845, 'tau': 0.007449357654664415, 'gamma': 0.9863293323589118}. Best is trial 16 with value: -6910.904861100001.\n",
      "[I 2025-08-08 15:34:23,027] Trial 29 finished with value: -7077.510379800001 and parameters: {'learning_rate': 2.3105578202771665e-05, 'buffer_size': 93249, 'learning_starts': 598, 'tau': 0.008201256175865177, 'gamma': 0.9500579665495188}. Best is trial 16 with value: -6910.904861100001.\n",
      "[I 2025-08-08 15:35:12,780] Trial 30 finished with value: -7053.104686500001 and parameters: {'learning_rate': 1.3438729953698825e-05, 'buffer_size': 54999, 'learning_starts': 696, 'tau': 0.009280230078694787, 'gamma': 0.9654260566924904}. Best is trial 16 with value: -6910.904861100001.\n",
      "[I 2025-08-08 15:36:00,886] Trial 31 finished with value: -6909.856274 and parameters: {'learning_rate': 9.412968774710501e-05, 'buffer_size': 66311, 'learning_starts': 625, 'tau': 0.0073437024888160195, 'gamma': 0.9375535753039177}. Best is trial 31 with value: -6909.856274.\n",
      "[I 2025-08-08 15:36:49,034] Trial 32 finished with value: -13007.491848000001 and parameters: {'learning_rate': 9.5417049152095e-05, 'buffer_size': 72682, 'learning_starts': 573, 'tau': 0.005972113752247139, 'gamma': 0.9397351834395874}. Best is trial 31 with value: -6909.856274.\n",
      "[I 2025-08-08 15:37:34,413] Trial 33 finished with value: -7146.821263399999 and parameters: {'learning_rate': 0.00013799927117028588, 'buffer_size': 61890, 'learning_starts': 660, 'tau': 0.007198154804282735, 'gamma': 0.9809607339403554}. Best is trial 31 with value: -6909.856274.\n",
      "[I 2025-08-08 15:38:24,096] Trial 34 finished with value: -13282.9716323 and parameters: {'learning_rate': 5.904730644206094e-05, 'buffer_size': 78445, 'learning_starts': 516, 'tau': 0.008158397611786324, 'gamma': 0.9475057339599158}. Best is trial 31 with value: -6909.856274.\n",
      "[I 2025-08-08 15:39:10,502] Trial 35 finished with value: -7141.2783234 and parameters: {'learning_rate': 0.00036285812101532176, 'buffer_size': 54210, 'learning_starts': 441, 'tau': 0.006213783196008163, 'gamma': 0.9632887216504258}. Best is trial 31 with value: -6909.856274.\n",
      "[I 2025-08-08 15:39:55,656] Trial 36 finished with value: -8162.794010100001 and parameters: {'learning_rate': 2.4489796380727957e-05, 'buffer_size': 99865, 'learning_starts': 619, 'tau': 0.00709641797695815, 'gamma': 0.9437279479815286}. Best is trial 31 with value: -6909.856274.\n",
      "[I 2025-08-08 15:40:44,667] Trial 37 finished with value: -8221.8756018 and parameters: {'learning_rate': 7.03749342421101e-05, 'buffer_size': 67274, 'learning_starts': 701, 'tau': 0.005080643724471593, 'gamma': 0.9983255297438923}. Best is trial 31 with value: -6909.856274.\n",
      "[I 2025-08-08 15:41:34,862] Trial 38 finished with value: -6978.508845000001 and parameters: {'learning_rate': 4.988319702368672e-05, 'buffer_size': 74423, 'learning_starts': 508, 'tau': 0.006822413386824412, 'gamma': 0.9516909057129166}. Best is trial 31 with value: -6909.856274.\n",
      "[I 2025-08-08 15:42:23,322] Trial 39 finished with value: -7071.009224900001 and parameters: {'learning_rate': 4.6647407937074036e-05, 'buffer_size': 77465, 'learning_starts': 400, 'tau': 0.007090121388391279, 'gamma': 0.9771550822884696}. Best is trial 31 with value: -6909.856274.\n",
      "[I 2025-08-08 15:43:11,616] Trial 40 finished with value: -7093.603857600001 and parameters: {'learning_rate': 2.785084555431166e-05, 'buffer_size': 88513, 'learning_starts': 489, 'tau': 0.006694567638772958, 'gamma': 0.9517399980469775}. Best is trial 31 with value: -6909.856274.\n",
      "[I 2025-08-08 15:43:59,486] Trial 41 finished with value: -13272.249569000001 and parameters: {'learning_rate': 8.863714136903258e-05, 'buffer_size': 72803, 'learning_starts': 544, 'tau': 0.0083820500116084, 'gamma': 0.9594762243707708}. Best is trial 31 with value: -6909.856274.\n",
      "[I 2025-08-08 15:44:47,236] Trial 42 finished with value: -7078.298066500001 and parameters: {'learning_rate': 0.0001304345769650902, 'buffer_size': 63217, 'learning_starts': 602, 'tau': 0.007583203600326191, 'gamma': 0.9256287870339565}. Best is trial 31 with value: -6909.856274.\n",
      "[I 2025-08-08 15:45:35,458] Trial 43 finished with value: -9317.964549 and parameters: {'learning_rate': 5.9208064015841435e-05, 'buffer_size': 58322, 'learning_starts': 751, 'tau': 0.0068063220709838595, 'gamma': 0.9309352236215868}. Best is trial 31 with value: -6909.856274.\n",
      "[I 2025-08-08 15:46:21,382] Trial 44 finished with value: -7032.0637788 and parameters: {'learning_rate': 3.8149278001150736e-05, 'buffer_size': 69022, 'learning_starts': 496, 'tau': 0.007949768934374098, 'gamma': 0.9430575634932332}. Best is trial 31 with value: -6909.856274.\n",
      "[I 2025-08-08 15:47:08,050] Trial 45 finished with value: -9336.9012141 and parameters: {'learning_rate': 1.2500092112183766e-05, 'buffer_size': 85231, 'learning_starts': 661, 'tau': 0.008827584108142479, 'gamma': 0.9236281248454772}. Best is trial 31 with value: -6909.856274.\n",
      "[I 2025-08-08 15:47:56,861] Trial 46 finished with value: -14163.180037400001 and parameters: {'learning_rate': 7.864637661746568e-05, 'buffer_size': 17270, 'learning_starts': 560, 'tau': 0.006240734122508345, 'gamma': 0.9355029972977199}. Best is trial 31 with value: -6909.856274.\n",
      "[I 2025-08-08 15:48:48,217] Trial 47 finished with value: -9266.593880199998 and parameters: {'learning_rate': 0.00017694558493457907, 'buffer_size': 75095, 'learning_starts': 331, 'tau': 0.009436695766094336, 'gamma': 0.9532699102373285}. Best is trial 31 with value: -6909.856274.\n",
      "[I 2025-08-08 15:49:34,800] Trial 48 finished with value: -6982.0054086 and parameters: {'learning_rate': 0.00011401660293824791, 'buffer_size': 44265, 'learning_starts': 424, 'tau': 0.007522104848788256, 'gamma': 0.9449093848029346}. Best is trial 31 with value: -6909.856274.\n",
      "[I 2025-08-08 15:50:24,178] Trial 49 finished with value: -13082.380612699999 and parameters: {'learning_rate': 0.0002752368440170976, 'buffer_size': 51325, 'learning_starts': 815, 'tau': 0.006798985327796113, 'gamma': 0.9626155661075546}. Best is trial 31 with value: -6909.856274.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- DDPG Optimization Finished ---\n",
      "Number of finished trials:  50\n",
      "Best trial:\n",
      "  Value (Average Reward):  -6909.856274\n",
      "  Params: \n",
      "    learning_rate: 9.412968774710501e-05\n",
      "    buffer_size: 66311\n",
      "    learning_starts: 625\n",
      "    tau: 0.0073437024888160195\n",
      "    gamma: 0.9375535753039177\n",
      "\n",
      "==================================================\n",
      "\n",
      "--- Training the final DDPG model with best hyperparameters ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3656719509f74ba38d6d2164e59e5249",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='Training final model: <b>DDPG</b>'), IntProgress(value=0, description='Training DDP…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved to: Output/DDPG/best_model.pt\n",
      "\n",
      "--- Evaluating the final DDPG model ---\n",
      "Evaluation logs saved to: Output/DDPG/evaluation_logs.csv\n",
      "\n",
      "Final DDPG Metrics:\n",
      "Average Reward: -9.3090\n",
      "Average Battery Health: 100.0000\n",
      "Average Efficiency: 6.8060\n",
      "Average Regret: 9.9896\n",
      "Energy Fulfillment Rate: 0.0000\n",
      "Evaluation plots saved to: Output/DDPG\n",
      "\n",
      "==================================================\n",
      "\n",
      "--- Starting Optuna optimization for SAC with 50 trials ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8125658f0024d4eb2be1697ade2189a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='Tuning model: <b>SAC</b>, Trial 0/50'), IntProgress(value=0, description='Tuning SA…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-08 15:51:26,444] A new study created in memory with name: SAC_optimization\n",
      "[I 2025-08-08 15:53:05,321] Trial 0 finished with value: -7110.785841900001 and parameters: {'learning_rate': 0.00029524309463140123, 'buffer_size': 36897, 'learning_starts': 288, 'gamma': 0.9018592218493237, 'tau': 0.008611970989570523, 'ent_coef': 9.295132565782162e-05}. Best is trial 0 with value: -7110.785841900001.\n",
      "[I 2025-08-08 15:54:35,690] Trial 1 finished with value: -7090.2311755 and parameters: {'learning_rate': 0.0007180784114380303, 'buffer_size': 81137, 'learning_starts': 776, 'gamma': 0.9356968182604202, 'tau': 0.009724506074419052, 'ent_coef': 0.014324215057871901}. Best is trial 1 with value: -7090.2311755.\n",
      "[I 2025-08-08 15:56:00,755] Trial 2 finished with value: -6990.517753 and parameters: {'learning_rate': 2.8353152937301472e-05, 'buffer_size': 17440, 'learning_starts': 649, 'gamma': 0.9547376545129562, 'tau': 0.005347052470720036, 'ent_coef': 2.942978160017717e-06}. Best is trial 2 with value: -6990.517753.\n",
      "[I 2025-08-08 15:57:26,093] Trial 3 finished with value: -7002.0406258 and parameters: {'learning_rate': 6.350465396798909e-05, 'buffer_size': 50746, 'learning_starts': 738, 'gamma': 0.9488307759976663, 'tau': 0.009762767601424, 'ent_coef': 0.00029877048020516823}. Best is trial 2 with value: -6990.517753.\n",
      "[I 2025-08-08 15:58:51,237] Trial 4 finished with value: -13027.202066599999 and parameters: {'learning_rate': 0.0007499112201861742, 'buffer_size': 51547, 'learning_starts': 660, 'gamma': 0.9457484192569683, 'tau': 0.004403704135970108, 'ent_coef': 1.32018079514548e-08}. Best is trial 2 with value: -6990.517753.\n",
      "[I 2025-08-08 16:00:12,024] Trial 5 finished with value: -7114.512203099999 and parameters: {'learning_rate': 0.00023942214090902665, 'buffer_size': 26042, 'learning_starts': 142, 'gamma': 0.9677301798944307, 'tau': 0.0019245499280338809, 'ent_coef': 0.00013491315074413377}. Best is trial 2 with value: -6990.517753.\n",
      "[I 2025-08-08 16:01:45,406] Trial 6 finished with value: -7022.6803561 and parameters: {'learning_rate': 0.0006182216917172843, 'buffer_size': 49650, 'learning_starts': 158, 'gamma': 0.9529583571000897, 'tau': 0.008450276414515871, 'ent_coef': 1.9591022311477834e-07}. Best is trial 2 with value: -6990.517753.\n",
      "[I 2025-08-08 16:03:14,152] Trial 7 finished with value: -7077.106145899999 and parameters: {'learning_rate': 0.00013787025398246278, 'buffer_size': 70793, 'learning_starts': 766, 'gamma': 0.9300834623974538, 'tau': 0.0026151349947099453, 'ent_coef': 2.9262185628317375e-07}. Best is trial 2 with value: -6990.517753.\n",
      "[I 2025-08-08 16:04:42,366] Trial 8 finished with value: -6974.4979680999995 and parameters: {'learning_rate': 3.271120162892298e-05, 'buffer_size': 71389, 'learning_starts': 552, 'gamma': 0.9068786692273919, 'tau': 0.002358492099578379, 'ent_coef': 0.000271442149708352}. Best is trial 8 with value: -6974.4979680999995.\n",
      "[I 2025-08-08 16:06:14,131] Trial 9 finished with value: -7054.174063399999 and parameters: {'learning_rate': 0.00028977110021247946, 'buffer_size': 80738, 'learning_starts': 236, 'gamma': 0.9912828518731956, 'tau': 0.002252715714089482, 'ent_coef': 8.49497448233547e-06}. Best is trial 8 with value: -6974.4979680999995.\n",
      "[I 2025-08-08 16:07:41,235] Trial 10 finished with value: -10676.9728268 and parameters: {'learning_rate': 1.0080155142422318e-05, 'buffer_size': 99683, 'learning_starts': 954, 'gamma': 0.9093802815878598, 'tau': 0.0010188394722592847, 'ent_coef': 0.041931836138384145}. Best is trial 8 with value: -6974.4979680999995.\n",
      "[I 2025-08-08 16:09:14,416] Trial 11 finished with value: -6963.6181431 and parameters: {'learning_rate': 2.837593785242301e-05, 'buffer_size': 67599, 'learning_starts': 444, 'gamma': 0.9799940354420738, 'tau': 0.005325732891603072, 'ent_coef': 4.21718740528392e-06}. Best is trial 11 with value: -6963.6181431.\n",
      "[I 2025-08-08 16:10:57,383] Trial 12 finished with value: -8197.2697305 and parameters: {'learning_rate': 2.9638811137915946e-05, 'buffer_size': 67049, 'learning_starts': 472, 'gamma': 0.9940036854514841, 'tau': 0.0039284725202031225, 'ent_coef': 0.001344405576395949}. Best is trial 11 with value: -6963.6181431.\n",
      "[I 2025-08-08 16:12:30,968] Trial 13 finished with value: -7025.716076899999 and parameters: {'learning_rate': 2.8813772640085517e-05, 'buffer_size': 67828, 'learning_starts': 438, 'gamma': 0.9749934454620758, 'tau': 0.006776108378668606, 'ent_coef': 0.0029328206414301248}. Best is trial 11 with value: -6963.6181431.\n",
      "[I 2025-08-08 16:14:07,048] Trial 14 finished with value: -6969.9820734 and parameters: {'learning_rate': 1.2561362069161825e-05, 'buffer_size': 99559, 'learning_starts': 363, 'gamma': 0.9177990322753756, 'tau': 0.006983361759833943, 'ent_coef': 1.0475077935424067e-05}. Best is trial 11 with value: -6963.6181431.\n",
      "[I 2025-08-08 16:15:41,301] Trial 15 finished with value: -7066.7978007 and parameters: {'learning_rate': 1.0710890499974183e-05, 'buffer_size': 95126, 'learning_starts': 335, 'gamma': 0.920192889607583, 'tau': 0.006698391753337831, 'ent_coef': 1.956551672996544e-06}. Best is trial 11 with value: -6963.6181431.\n",
      "[I 2025-08-08 16:17:26,319] Trial 16 finished with value: -7036.0576535 and parameters: {'learning_rate': 1.5140006127750553e-05, 'buffer_size': 87378, 'learning_starts': 404, 'gamma': 0.9781502703496502, 'tau': 0.006646806399261013, 'ent_coef': 2.2877968102922637e-05}. Best is trial 11 with value: -6963.6181431.\n",
      "[I 2025-08-08 16:18:58,304] Trial 17 finished with value: -7042.7929691 and parameters: {'learning_rate': 6.02572246699095e-05, 'buffer_size': 34577, 'learning_starts': 550, 'gamma': 0.9653156419419123, 'tau': 0.005275097607424285, 'ent_coef': 3.052805175056962e-07}. Best is trial 11 with value: -6963.6181431.\n",
      "[I 2025-08-08 16:20:33,063] Trial 18 finished with value: -7011.6913527000015 and parameters: {'learning_rate': 1.8201176041016167e-05, 'buffer_size': 60905, 'learning_starts': 358, 'gamma': 0.9193271217743881, 'tau': 0.007907869283657094, 'ent_coef': 2.0856542446030244e-08}. Best is trial 11 with value: -6963.6181431.\n",
      "[I 2025-08-08 16:22:07,319] Trial 19 finished with value: -7106.1338136 and parameters: {'learning_rate': 5.784535103309529e-05, 'buffer_size': 85675, 'learning_starts': 249, 'gamma': 0.9339524667953943, 'tau': 0.003941225275383519, 'ent_coef': 1.497430860988634e-06}. Best is trial 11 with value: -6963.6181431.\n",
      "[I 2025-08-08 16:23:39,101] Trial 20 finished with value: -7766.5196662 and parameters: {'learning_rate': 1.7926909453364814e-05, 'buffer_size': 92031, 'learning_starts': 516, 'gamma': 0.9797400675501748, 'tau': 0.007536505504166113, 'ent_coef': 1.7999070675064334e-05}. Best is trial 11 with value: -6963.6181431.\n",
      "[I 2025-08-08 16:25:15,479] Trial 21 finished with value: -7080.191268600001 and parameters: {'learning_rate': 3.615331885351141e-05, 'buffer_size': 73066, 'learning_starts': 617, 'gamma': 0.9009622918416604, 'tau': 0.0029051256268602735, 'ent_coef': 0.0006812373311702304}. Best is trial 11 with value: -6963.6181431.\n",
      "[I 2025-08-08 16:26:49,950] Trial 22 finished with value: -6967.134154499999 and parameters: {'learning_rate': 0.00010034579848960072, 'buffer_size': 60108, 'learning_starts': 448, 'gamma': 0.9155786984606811, 'tau': 0.0058372100271739955, 'ent_coef': 6.135549145700326e-05}. Best is trial 11 with value: -6963.6181431.\n",
      "[I 2025-08-08 16:28:38,034] Trial 23 finished with value: -7114.3454563 and parameters: {'learning_rate': 0.00012343684316895196, 'buffer_size': 59198, 'learning_starts': 382, 'gamma': 0.921006373154306, 'tau': 0.005997841583292169, 'ent_coef': 5.3941601032066094e-05}. Best is trial 11 with value: -6963.6181431.\n",
      "[I 2025-08-08 16:30:10,731] Trial 24 finished with value: -7014.805397300001 and parameters: {'learning_rate': 8.733189102784165e-05, 'buffer_size': 42604, 'learning_starts': 476, 'gamma': 0.9147309298834502, 'tau': 0.004752212128199039, 'ent_coef': 2.901446784707228e-06}. Best is trial 11 with value: -6963.6181431.\n",
      "[I 2025-08-08 16:31:46,955] Trial 25 finished with value: -7041.6308947 and parameters: {'learning_rate': 4.3542554656401684e-05, 'buffer_size': 61834, 'learning_starts': 293, 'gamma': 0.9295472346321976, 'tau': 0.0059524529272615145, 'ent_coef': 8.419972928524892e-06}. Best is trial 11 with value: -6963.6181431.\n",
      "[I 2025-08-08 16:33:20,119] Trial 26 finished with value: -6961.0473788 and parameters: {'learning_rate': 0.00015695936451477597, 'buffer_size': 76902, 'learning_starts': 203, 'gamma': 0.9416325892788451, 'tau': 0.0073237615437078555, 'ent_coef': 8.479177837995935e-08}. Best is trial 26 with value: -6961.0473788.\n",
      "[I 2025-08-08 16:34:56,704] Trial 27 finished with value: -7126.743259000001 and parameters: {'learning_rate': 0.00017632392835676105, 'buffer_size': 75098, 'learning_starts': 107, 'gamma': 0.9390522205581275, 'tau': 0.0058416517386219585, 'ent_coef': 7.372518599034089e-08}. Best is trial 26 with value: -6961.0473788.\n",
      "[I 2025-08-08 16:36:29,301] Trial 28 finished with value: -7135.0894737 and parameters: {'learning_rate': 0.0004481568465526845, 'buffer_size': 78120, 'learning_starts': 176, 'gamma': 0.942901070610956, 'tau': 0.0034399085899179524, 'ent_coef': 5.53257255715028e-08}. Best is trial 26 with value: -6961.0473788.\n",
      "[I 2025-08-08 16:37:54,189] Trial 29 finished with value: -6950.158318900001 and parameters: {'learning_rate': 0.00021505121545666557, 'buffer_size': 44746, 'learning_starts': 219, 'gamma': 0.960093224402277, 'tau': 0.008823414463173434, 'ent_coef': 7.849174917815664e-07}. Best is trial 29 with value: -6950.158318900001.\n",
      "[I 2025-08-08 16:39:28,229] Trial 30 finished with value: -8205.5969866 and parameters: {'learning_rate': 0.00043713584901701897, 'buffer_size': 38840, 'learning_starts': 240, 'gamma': 0.9604141859694338, 'tau': 0.009043105099999763, 'ent_coef': 5.516403971402164e-07}. Best is trial 29 with value: -6950.158318900001.\n",
      "[I 2025-08-08 16:40:56,551] Trial 31 finished with value: -7128.4653266 and parameters: {'learning_rate': 0.00019417085393552062, 'buffer_size': 43683, 'learning_starts': 301, 'gamma': 0.9865846742845785, 'tau': 0.008654399421539024, 'ent_coef': 9.75501617423295e-07}. Best is trial 29 with value: -6950.158318900001.\n",
      "[I 2025-08-08 16:42:29,337] Trial 32 finished with value: -7081.9000014 and parameters: {'learning_rate': 9.876655280794372e-05, 'buffer_size': 57472, 'learning_starts': 206, 'gamma': 0.9699085821654024, 'tau': 0.0073908276892267625, 'ent_coef': 1.0076621828125276e-07}. Best is trial 29 with value: -6950.158318900001.\n",
      "[I 2025-08-08 16:43:46,965] Trial 33 finished with value: -6999.0155969 and parameters: {'learning_rate': 0.00014497505700093548, 'buffer_size': 64741, 'learning_starts': 894, 'gamma': 0.9603926970878306, 'tau': 0.008107438253836799, 'ent_coef': 3.2061758380895697e-08}. Best is trial 29 with value: -6950.158318900001.\n",
      "[I 2025-08-08 16:45:15,664] Trial 34 finished with value: -7095.930573600001 and parameters: {'learning_rate': 7.999804585193171e-05, 'buffer_size': 30803, 'learning_starts': 434, 'gamma': 0.9982605801523415, 'tau': 0.009541306526265258, 'ent_coef': 4.570885004542671e-06}. Best is trial 29 with value: -6950.158318900001.\n",
      "[I 2025-08-08 16:46:44,655] Trial 35 finished with value: -7026.4540851 and parameters: {'learning_rate': 0.0002211234668274104, 'buffer_size': 53918, 'learning_starts': 304, 'gamma': 0.9570082183483952, 'tau': 0.008976049270120448, 'ent_coef': 5.093918220109672e-05}. Best is trial 29 with value: -6950.158318900001.\n",
      "[I 2025-08-08 16:48:24,721] Trial 36 finished with value: -7066.355210600001 and parameters: {'learning_rate': 0.00034852622601693324, 'buffer_size': 18197, 'learning_starts': 673, 'gamma': 0.9843394414312203, 'tau': 0.005186210275544212, 'ent_coef': 7.614997775601408e-07}. Best is trial 29 with value: -6950.158318900001.\n",
      "[I 2025-08-08 16:49:56,082] Trial 37 finished with value: -7079.121385699999 and parameters: {'learning_rate': 0.00011512897326758543, 'buffer_size': 47474, 'learning_starts': 102, 'gamma': 0.949589214129322, 'tau': 0.006324415313125057, 'ent_coef': 1.8374908067850318e-07}. Best is trial 29 with value: -6950.158318900001.\n",
      "[I 2025-08-08 16:51:27,229] Trial 38 finished with value: -7107.016191200001 and parameters: {'learning_rate': 0.00016517402250833135, 'buffer_size': 55754, 'learning_starts': 585, 'gamma': 0.9260539254491511, 'tau': 0.004739570070362452, 'ent_coef': 0.00012955708304127306}. Best is trial 29 with value: -6950.158318900001.\n",
      "[I 2025-08-08 16:52:57,814] Trial 39 finished with value: -7191.6399913000005 and parameters: {'learning_rate': 6.983437775822494e-05, 'buffer_size': 47518, 'learning_starts': 715, 'gamma': 0.9403745234761056, 'tau': 0.007679038458329561, 'ent_coef': 1.303056634648535e-08}. Best is trial 29 with value: -6950.158318900001.\n",
      "[I 2025-08-08 16:54:27,786] Trial 40 finished with value: -7156.044717300001 and parameters: {'learning_rate': 0.0003036063869309747, 'buffer_size': 82301, 'learning_starts': 843, 'gamma': 0.9707885271186772, 'tau': 0.007244915832033881, 'ent_coef': 4.191118469303213e-07}. Best is trial 29 with value: -6950.158318900001.\n",
      "[I 2025-08-08 16:56:00,235] Trial 41 finished with value: -8206.799593299998 and parameters: {'learning_rate': 0.0009965931752197166, 'buffer_size': 89368, 'learning_starts': 327, 'gamma': 0.9112800615872788, 'tau': 0.007093888202779704, 'ent_coef': 5.803201758497576e-06}. Best is trial 29 with value: -6950.158318900001.\n",
      "[I 2025-08-08 16:57:29,426] Trial 42 finished with value: -6962.5881738 and parameters: {'learning_rate': 4.9313751255369125e-05, 'buffer_size': 76342, 'learning_starts': 200, 'gamma': 0.9048332435995557, 'tau': 0.005698355371440659, 'ent_coef': 1.7166014954109526e-05}. Best is trial 29 with value: -6950.158318900001.\n",
      "[I 2025-08-08 16:58:59,736] Trial 43 finished with value: -7016.5246841 and parameters: {'learning_rate': 4.4103731575788814e-05, 'buffer_size': 75812, 'learning_starts': 199, 'gamma': 0.9051742684650953, 'tau': 0.004912460580444901, 'ent_coef': 3.3756852571008414e-05}. Best is trial 29 with value: -6950.158318900001.\n",
      "[I 2025-08-08 17:00:32,949] Trial 44 finished with value: -7080.0425143 and parameters: {'learning_rate': 2.2803636076650002e-05, 'buffer_size': 69702, 'learning_starts': 239, 'gamma': 0.9469580192202197, 'tau': 0.0056932721551533555, 'ent_coef': 0.00025974305752227454}. Best is trial 29 with value: -6950.158318900001.\n",
      "[I 2025-08-08 17:02:08,592] Trial 45 finished with value: -7095.414137900001 and parameters: {'learning_rate': 5.2462748665848534e-05, 'buffer_size': 64857, 'learning_starts': 154, 'gamma': 0.9247005475067599, 'tau': 0.00625256569083083, 'ent_coef': 1.6214274251035013e-05}. Best is trial 29 with value: -6950.158318900001.\n",
      "[I 2025-08-08 17:03:39,137] Trial 46 finished with value: -7110.0844786 and parameters: {'learning_rate': 9.972219565616082e-05, 'buffer_size': 82730, 'learning_starts': 266, 'gamma': 0.9059052170071237, 'tau': 0.009876726426792245, 'ent_coef': 2.7429041965151383e-06}. Best is trial 29 with value: -6950.158318900001.\n",
      "[I 2025-08-08 17:05:08,491] Trial 47 finished with value: -7183.0463537000005 and parameters: {'learning_rate': 0.0002227376173085543, 'buffer_size': 78133, 'learning_starts': 492, 'gamma': 0.9124179915017853, 'tau': 0.00407471595982627, 'ent_coef': 0.004793975044258687}. Best is trial 29 with value: -6950.158318900001.\n",
      "[I 2025-08-08 17:06:39,150] Trial 48 finished with value: -6981.936714999999 and parameters: {'learning_rate': 2.2370667097909618e-05, 'buffer_size': 53268, 'learning_starts': 427, 'gamma': 0.9335250023274198, 'tau': 0.00831348360037735, 'ent_coef': 8.261523737063624e-05}. Best is trial 29 with value: -6950.158318900001.\n",
      "[I 2025-08-08 17:08:09,121] Trial 49 finished with value: -7156.1017085 and parameters: {'learning_rate': 7.698310061396852e-05, 'buffer_size': 63439, 'learning_starts': 198, 'gamma': 0.9653228233928374, 'tau': 0.00551451723494221, 'ent_coef': 1.478578342296654e-07}. Best is trial 29 with value: -6950.158318900001.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- SAC Optimization Finished ---\n",
      "Number of finished trials:  50\n",
      "Best trial:\n",
      "  Value (Average Reward):  -6950.158318900001\n",
      "  Params: \n",
      "    learning_rate: 0.00021505121545666557\n",
      "    buffer_size: 44746\n",
      "    learning_starts: 219\n",
      "    gamma: 0.960093224402277\n",
      "    tau: 0.008823414463173434\n",
      "    ent_coef: 7.849174917815664e-07\n",
      "\n",
      "==================================================\n",
      "\n",
      "--- Training the final SAC model with best hyperparameters ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93ba768c7e764367a95f20085f61733f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='Training final model: <b>SAC</b>'), IntProgress(value=0, description='Training SAC:…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved to: Output/SAC/best_model.pt\n",
      "\n",
      "--- Evaluating the final SAC model ---\n",
      "Evaluation logs saved to: Output/SAC/evaluation_logs.csv\n",
      "\n",
      "Final SAC Metrics:\n",
      "Average Reward: -7.0480\n",
      "Average Battery Health: 100.0000\n",
      "Average Efficiency: 6.8174\n",
      "Average Regret: 7.7297\n",
      "Energy Fulfillment Rate: 22.8305\n",
      "Evaluation plots saved to: Output/SAC\n",
      "\n",
      "==================================================\n",
      "\n",
      "--- Starting Optuna optimization for DQN with 50 trials ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0e730ba752646a298b20ed8fc895d66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='Tuning model: <b>DQN</b>, Trial 0/50'), IntProgress(value=0, description='Tuning DQ…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-08 17:09:55,875] A new study created in memory with name: DQN_optimization\n",
      "[I 2025-08-08 17:10:02,877] Trial 0 finished with value: -12462.0847219 and parameters: {'learning_rate': 3.8554966439221894e-05, 'buffer_size': 95838, 'learning_starts': 802, 'gamma': 0.9493648614126189, 'exploration_fraction': 0.24383315711118325, 'exploration_final_eps': 0.08596279723202192, 'train_freq': 10, 'target_update_interval': 654}. Best is trial 0 with value: -12462.0847219.\n",
      "[I 2025-08-08 17:10:10,415] Trial 1 finished with value: -12121.6643026 and parameters: {'learning_rate': 1.5187998933053223e-05, 'buffer_size': 17854, 'learning_starts': 610, 'gamma': 0.9798725582002886, 'exploration_fraction': 0.3760887360627516, 'exploration_final_eps': 0.08753848931350781, 'train_freq': 9, 'target_update_interval': 796}. Best is trial 1 with value: -12121.6643026.\n",
      "[I 2025-08-08 17:10:19,054] Trial 2 finished with value: -12974.8621457 and parameters: {'learning_rate': 0.00012313943472490777, 'buffer_size': 93047, 'learning_starts': 817, 'gamma': 0.9622043241793745, 'exploration_fraction': 0.25564595931876394, 'exploration_final_eps': 0.0734876183182131, 'train_freq': 6, 'target_update_interval': 473}. Best is trial 1 with value: -12121.6643026.\n",
      "[I 2025-08-08 17:10:26,900] Trial 3 finished with value: -11632.7424022 and parameters: {'learning_rate': 0.0005647523483978675, 'buffer_size': 13405, 'learning_starts': 326, 'gamma': 0.9099737826134148, 'exploration_fraction': 0.2597014993738081, 'exploration_final_eps': 0.05889323832417426, 'train_freq': 8, 'target_update_interval': 542}. Best is trial 3 with value: -11632.7424022.\n",
      "[I 2025-08-08 17:10:34,988] Trial 4 finished with value: -11635.1058762 and parameters: {'learning_rate': 0.0006070995207834259, 'buffer_size': 93294, 'learning_starts': 741, 'gamma': 0.9855633235752502, 'exploration_fraction': 0.21798507181077512, 'exploration_final_eps': 0.04282633227533499, 'train_freq': 8, 'target_update_interval': 721}. Best is trial 3 with value: -11632.7424022.\n",
      "[I 2025-08-08 17:10:49,238] Trial 5 finished with value: -11557.958010400002 and parameters: {'learning_rate': 1.8272260216507888e-05, 'buffer_size': 90680, 'learning_starts': 251, 'gamma': 0.9608480948579429, 'exploration_fraction': 0.14799209223709073, 'exploration_final_eps': 0.03805846907863114, 'train_freq': 2, 'target_update_interval': 632}. Best is trial 5 with value: -11557.958010400002.\n",
      "[I 2025-08-08 17:10:58,080] Trial 6 finished with value: -11510.5873271 and parameters: {'learning_rate': 0.0007426388223770817, 'buffer_size': 55641, 'learning_starts': 670, 'gamma': 0.9511989545687669, 'exploration_fraction': 0.32648550797219555, 'exploration_final_eps': 0.029520593338244763, 'train_freq': 5, 'target_update_interval': 840}. Best is trial 6 with value: -11510.5873271.\n",
      "[I 2025-08-08 17:11:09,689] Trial 7 finished with value: -11549.430316799999 and parameters: {'learning_rate': 0.0002065064775514131, 'buffer_size': 96315, 'learning_starts': 140, 'gamma': 0.9184977228596745, 'exploration_fraction': 0.19247317228436678, 'exploration_final_eps': 0.09641124046968812, 'train_freq': 3, 'target_update_interval': 230}. Best is trial 6 with value: -11510.5873271.\n",
      "[I 2025-08-08 17:11:17,213] Trial 8 finished with value: -12875.858653700001 and parameters: {'learning_rate': 2.6693748140994312e-05, 'buffer_size': 28746, 'learning_starts': 204, 'gamma': 0.9511519962009544, 'exploration_fraction': 0.43465238662553707, 'exploration_final_eps': 0.06418242061108975, 'train_freq': 7, 'target_update_interval': 576}. Best is trial 6 with value: -11510.5873271.\n",
      "[I 2025-08-08 17:11:23,958] Trial 9 finished with value: -12470.279146500001 and parameters: {'learning_rate': 0.0005141409689344018, 'buffer_size': 58003, 'learning_starts': 796, 'gamma': 0.9538508205658534, 'exploration_fraction': 0.3577827278193666, 'exploration_final_eps': 0.036655785608475405, 'train_freq': 9, 'target_update_interval': 227}. Best is trial 6 with value: -11510.5873271.\n",
      "[I 2025-08-08 17:11:33,594] Trial 10 finished with value: -11619.421875 and parameters: {'learning_rate': 0.0002482571060821643, 'buffer_size': 58381, 'learning_starts': 471, 'gamma': 0.9302794738078289, 'exploration_fraction': 0.49472577716109345, 'exploration_final_eps': 0.011279111461461912, 'train_freq': 4, 'target_update_interval': 968}. Best is trial 6 with value: -11510.5873271.\n",
      "[I 2025-08-08 17:11:43,039] Trial 11 finished with value: -11847.105227 and parameters: {'learning_rate': 0.00018707017923918842, 'buffer_size': 74281, 'learning_starts': 539, 'gamma': 0.9006859379641847, 'exploration_fraction': 0.11494721667210034, 'exploration_final_eps': 0.017085405172541035, 'train_freq': 4, 'target_update_interval': 202}. Best is trial 6 with value: -11510.5873271.\n",
      "[I 2025-08-08 17:12:06,293] Trial 12 finished with value: -11638.8645134 and parameters: {'learning_rate': 6.135158307122138e-05, 'buffer_size': 43252, 'learning_starts': 101, 'gamma': 0.9274034432664817, 'exploration_fraction': 0.32775127928885966, 'exploration_final_eps': 0.09898243770741871, 'train_freq': 1, 'target_update_interval': 364}. Best is trial 6 with value: -11510.5873271.\n",
      "[I 2025-08-08 17:12:16,427] Trial 13 finished with value: -11978.5007453 and parameters: {'learning_rate': 0.0009391899423117193, 'buffer_size': 72295, 'learning_starts': 977, 'gamma': 0.9311056411110826, 'exploration_fraction': 0.17006245665328806, 'exploration_final_eps': 0.04883893838770509, 'train_freq': 4, 'target_update_interval': 992}. Best is trial 6 with value: -11510.5873271.\n",
      "[I 2025-08-08 17:12:28,817] Trial 14 finished with value: -11616.704802100001 and parameters: {'learning_rate': 0.00029851071080527807, 'buffer_size': 42421, 'learning_starts': 411, 'gamma': 0.9227424876608934, 'exploration_fraction': 0.184446759267331, 'exploration_final_eps': 0.02442809025198625, 'train_freq': 3, 'target_update_interval': 343}. Best is trial 6 with value: -11510.5873271.\n",
      "[I 2025-08-08 17:12:38,362] Trial 15 finished with value: -11762.408518200002 and parameters: {'learning_rate': 9.302521266919642e-05, 'buffer_size': 75490, 'learning_starts': 661, 'gamma': 0.9984971998309291, 'exploration_fraction': 0.29628937273746314, 'exploration_final_eps': 0.026080596387541215, 'train_freq': 5, 'target_update_interval': 842}. Best is trial 6 with value: -11510.5873271.\n",
      "[I 2025-08-08 17:12:53,719] Trial 16 finished with value: -11514.2803293 and parameters: {'learning_rate': 0.000347670677367996, 'buffer_size': 44443, 'learning_starts': 960, 'gamma': 0.9408198104335629, 'exploration_fraction': 0.4126360675104809, 'exploration_final_eps': 0.07061657107495901, 'train_freq': 2, 'target_update_interval': 117}. Best is trial 6 with value: -11510.5873271.\n",
      "[I 2025-08-08 17:13:14,591] Trial 17 finished with value: -11939.152954 and parameters: {'learning_rate': 0.0009825820631968768, 'buffer_size': 45738, 'learning_starts': 996, 'gamma': 0.9421304428443429, 'exploration_fraction': 0.4201519771395125, 'exploration_final_eps': 0.07346900848835143, 'train_freq': 1, 'target_update_interval': 862}. Best is trial 6 with value: -11510.5873271.\n",
      "[I 2025-08-08 17:13:22,139] Trial 18 finished with value: -9983.8597712 and parameters: {'learning_rate': 0.00039846792501639156, 'buffer_size': 30551, 'learning_starts': 898, 'gamma': 0.9704260371026998, 'exploration_fraction': 0.42882447366004917, 'exploration_final_eps': 0.06704927882225381, 'train_freq': 6, 'target_update_interval': 428}. Best is trial 18 with value: -9983.8597712.\n",
      "[I 2025-08-08 17:13:30,508] Trial 19 finished with value: -12512.9675575 and parameters: {'learning_rate': 0.0004303453026197313, 'buffer_size': 29728, 'learning_starts': 705, 'gamma': 0.9710826127468746, 'exploration_fraction': 0.4845155154631182, 'exploration_final_eps': 0.05208100924534405, 'train_freq': 6, 'target_update_interval': 435}. Best is trial 18 with value: -9983.8597712.\n",
      "[I 2025-08-08 17:13:39,071] Trial 20 finished with value: -11617.455148 and parameters: {'learning_rate': 0.00013104141395586845, 'buffer_size': 27629, 'learning_starts': 888, 'gamma': 0.9720562506358563, 'exploration_fraction': 0.31071095950844974, 'exploration_final_eps': 0.027995879969705244, 'train_freq': 5, 'target_update_interval': 754}. Best is trial 18 with value: -9983.8597712.\n",
      "[I 2025-08-08 17:13:46,998] Trial 21 finished with value: -12457.508676200001 and parameters: {'learning_rate': 0.0003394817354511747, 'buffer_size': 50277, 'learning_starts': 900, 'gamma': 0.9404028834039401, 'exploration_fraction': 0.4165865493789441, 'exploration_final_eps': 0.06733672139899932, 'train_freq': 7, 'target_update_interval': 112}. Best is trial 18 with value: -9983.8597712.\n",
      "[I 2025-08-08 17:14:00,898] Trial 22 finished with value: -11556.408159 and parameters: {'learning_rate': 0.000763250120901527, 'buffer_size': 36953, 'learning_starts': 899, 'gamma': 0.941178922549932, 'exploration_fraction': 0.45586999453838095, 'exploration_final_eps': 0.07935378243432935, 'train_freq': 2, 'target_update_interval': 338}. Best is trial 18 with value: -9983.8597712.\n",
      "[I 2025-08-08 17:14:08,598] Trial 23 finished with value: -11800.8711481 and parameters: {'learning_rate': 0.0004473009271167219, 'buffer_size': 63347, 'learning_starts': 577, 'gamma': 0.9623209607939951, 'exploration_fraction': 0.3795789436807974, 'exploration_final_eps': 0.056252534736341855, 'train_freq': 7, 'target_update_interval': 114}. Best is trial 18 with value: -9983.8597712.\n",
      "[I 2025-08-08 17:14:17,051] Trial 24 finished with value: -11615.7282641 and parameters: {'learning_rate': 0.0003405373803340123, 'buffer_size': 33241, 'learning_starts': 937, 'gamma': 0.969708805366685, 'exploration_fraction': 0.34078862977712154, 'exploration_final_eps': 0.06496265149974419, 'train_freq': 5, 'target_update_interval': 485}. Best is trial 18 with value: -9983.8597712.\n",
      "[I 2025-08-08 17:14:27,819] Trial 25 finished with value: -11572.521159299999 and parameters: {'learning_rate': 0.00017974794565562825, 'buffer_size': 23035, 'learning_starts': 853, 'gamma': 0.9866694496459174, 'exploration_fraction': 0.4004462075346798, 'exploration_final_eps': 0.07387766661892048, 'train_freq': 3, 'target_update_interval': 930}. Best is trial 18 with value: -9983.8597712.\n",
      "[I 2025-08-08 17:14:35,786] Trial 26 finished with value: -12394.323395299998 and parameters: {'learning_rate': 0.0007002317729130435, 'buffer_size': 66053, 'learning_starts': 763, 'gamma': 0.9377042716303816, 'exploration_fraction': 0.44755643090348946, 'exploration_final_eps': 0.08358022426819413, 'train_freq': 6, 'target_update_interval': 693}. Best is trial 18 with value: -9983.8597712.\n",
      "[I 2025-08-08 17:14:50,332] Trial 27 finished with value: -11637.976297 and parameters: {'learning_rate': 7.694538050502701e-05, 'buffer_size': 50009, 'learning_starts': 676, 'gamma': 0.9556160930813795, 'exploration_fraction': 0.2917039602746921, 'exploration_final_eps': 0.059699826638560716, 'train_freq': 2, 'target_update_interval': 300}. Best is trial 18 with value: -9983.8597712.\n",
      "[I 2025-08-08 17:15:00,195] Trial 28 finished with value: -11618.9248148 and parameters: {'learning_rate': 0.0003703415203626152, 'buffer_size': 52193, 'learning_starts': 950, 'gamma': 0.9480382140906768, 'exploration_fraction': 0.46357681751710295, 'exploration_final_eps': 0.046372894098920356, 'train_freq': 4, 'target_update_interval': 427}. Best is trial 18 with value: -9983.8597712.\n",
      "[I 2025-08-08 17:15:07,041] Trial 29 finished with value: -12453.8513052 and parameters: {'learning_rate': 4.971536900081784e-05, 'buffer_size': 35796, 'learning_starts': 846, 'gamma': 0.9470535550172515, 'exploration_fraction': 0.3793219883784136, 'exploration_final_eps': 0.03477997394597333, 'train_freq': 10, 'target_update_interval': 620}. Best is trial 18 with value: -9983.8597712.\n",
      "[I 2025-08-08 17:15:14,101] Trial 30 finished with value: -11649.9991854 and parameters: {'learning_rate': 0.00015129592489983025, 'buffer_size': 84831, 'learning_starts': 500, 'gamma': 0.9779008316359875, 'exploration_fraction': 0.39760757982973904, 'exploration_final_eps': 0.09105751155517146, 'train_freq': 8, 'target_update_interval': 546}. Best is trial 18 with value: -9983.8597712.\n",
      "[I 2025-08-08 17:15:25,470] Trial 31 finished with value: -11571.8902366 and parameters: {'learning_rate': 0.00024597786285157625, 'buffer_size': 84262, 'learning_starts': 403, 'gamma': 0.9183346207023582, 'exploration_fraction': 0.22336777662252377, 'exploration_final_eps': 0.09798115177747944, 'train_freq': 3, 'target_update_interval': 199}. Best is trial 18 with value: -9983.8597712.\n",
      "[I 2025-08-08 17:15:36,384] Trial 32 finished with value: -11571.8316821 and parameters: {'learning_rate': 0.0002357465969053008, 'buffer_size': 99053, 'learning_starts': 622, 'gamma': 0.9139135471895387, 'exploration_fraction': 0.2728060184686866, 'exploration_final_eps': 0.0895734655346879, 'train_freq': 3, 'target_update_interval': 167}. Best is trial 18 with value: -9983.8597712.\n",
      "[I 2025-08-08 17:15:50,996] Trial 33 finished with value: -11648.2898475 and parameters: {'learning_rate': 0.00020976929846960288, 'buffer_size': 19651, 'learning_starts': 109, 'gamma': 0.9062177633197078, 'exploration_fraction': 0.3521672360312015, 'exploration_final_eps': 0.07569210319164628, 'train_freq': 2, 'target_update_interval': 273}. Best is trial 18 with value: -9983.8597712.\n",
      "[I 2025-08-08 17:16:13,428] Trial 34 finished with value: -11718.090507699999 and parameters: {'learning_rate': 0.0004742882418337779, 'buffer_size': 40308, 'learning_starts': 798, 'gamma': 0.9332526495130815, 'exploration_fraction': 0.32419971790464736, 'exploration_final_eps': 0.08316532448667396, 'train_freq': 1, 'target_update_interval': 260}. Best is trial 18 with value: -9983.8597712.\n",
      "[I 2025-08-08 17:16:23,015] Trial 35 finished with value: -11560.4067088 and parameters: {'learning_rate': 0.0006996863583169892, 'buffer_size': 16624, 'learning_starts': 317, 'gamma': 0.9211469945082604, 'exploration_fraction': 0.23776442175120135, 'exploration_final_eps': 0.06790339911157053, 'train_freq': 5, 'target_update_interval': 162}. Best is trial 18 with value: -9983.8597712.\n",
      "[I 2025-08-08 17:16:31,173] Trial 36 finished with value: -11597.009194699998 and parameters: {'learning_rate': 0.00029949752323955544, 'buffer_size': 12623, 'learning_starts': 752, 'gamma': 0.9604605717997662, 'exploration_fraction': 0.2705195809712796, 'exploration_final_eps': 0.06005952177202953, 'train_freq': 6, 'target_update_interval': 395}. Best is trial 18 with value: -9983.8597712.\n",
      "[I 2025-08-08 17:16:40,490] Trial 37 finished with value: -11697.252440699998 and parameters: {'learning_rate': 0.00011334711032567104, 'buffer_size': 24606, 'learning_starts': 197, 'gamma': 0.9794313418637228, 'exploration_fraction': 0.1874689588660326, 'exploration_final_eps': 0.09455344461452393, 'train_freq': 4, 'target_update_interval': 500}. Best is trial 18 with value: -9983.8597712.\n",
      "[I 2025-08-08 17:16:51,730] Trial 38 finished with value: -11359.4986792 and parameters: {'learning_rate': 0.0005565887794356984, 'buffer_size': 67980, 'learning_starts': 627, 'gamma': 0.9453800873077485, 'exploration_fraction': 0.1273342750325308, 'exploration_final_eps': 0.07854894175278304, 'train_freq': 3, 'target_update_interval': 103}. Best is trial 18 with value: -9983.8597712.\n",
      "[I 2025-08-08 17:16:59,988] Trial 39 finished with value: -12200.0479604 and parameters: {'learning_rate': 0.0005493891405304477, 'buffer_size': 66342, 'learning_starts': 620, 'gamma': 0.9571069373666778, 'exploration_fraction': 0.10607435038340798, 'exploration_final_eps': 0.0802272288345741, 'train_freq': 7, 'target_update_interval': 787}. Best is trial 18 with value: -9983.8597712.\n",
      "[I 2025-08-08 17:17:13,656] Trial 40 finished with value: -12096.8249907 and parameters: {'learning_rate': 0.0007832664107783851, 'buffer_size': 58688, 'learning_starts': 712, 'gamma': 0.9661963934347201, 'exploration_fraction': 0.47336325421278463, 'exploration_final_eps': 0.07094549716180479, 'train_freq': 2, 'target_update_interval': 148}. Best is trial 18 with value: -9983.8597712.\n",
      "[I 2025-08-08 17:17:26,355] Trial 41 finished with value: -11556.698115 and parameters: {'learning_rate': 0.0005644230094634421, 'buffer_size': 86340, 'learning_starts': 540, 'gamma': 0.9476967292031936, 'exploration_fraction': 0.1351899902753726, 'exploration_final_eps': 0.08639519649182385, 'train_freq': 3, 'target_update_interval': 233}. Best is trial 18 with value: -9983.8597712.\n",
      "[I 2025-08-08 17:17:38,375] Trial 42 finished with value: -11558.1938955 and parameters: {'learning_rate': 0.0004008986229848586, 'buffer_size': 78653, 'learning_starts': 460, 'gamma': 0.9373152707885425, 'exploration_fraction': 0.14556451773105858, 'exploration_final_eps': 0.07793479237872566, 'train_freq': 3, 'target_update_interval': 128}. Best is trial 18 with value: -9983.8597712.\n",
      "[I 2025-08-08 17:18:03,090] Trial 43 finished with value: -11658.556709800001 and parameters: {'learning_rate': 0.00028903082644235765, 'buffer_size': 47097, 'learning_starts': 583, 'gamma': 0.9522855896773634, 'exploration_fraction': 0.17066350025476226, 'exploration_final_eps': 0.07078900312691619, 'train_freq': 1, 'target_update_interval': 192}. Best is trial 18 with value: -9983.8597712.\n",
      "[I 2025-08-08 17:18:12,755] Trial 44 finished with value: -10079.544109600001 and parameters: {'learning_rate': 0.0006113929796509091, 'buffer_size': 54769, 'learning_starts': 843, 'gamma': 0.927183341716048, 'exploration_fraction': 0.21272912181498216, 'exploration_final_eps': 0.06187056227270565, 'train_freq': 4, 'target_update_interval': 101}. Best is trial 18 with value: -9983.8597712.\n",
      "[I 2025-08-08 17:18:22,779] Trial 45 finished with value: -13344.243929499999 and parameters: {'learning_rate': 1.1042095705724653e-05, 'buffer_size': 54499, 'learning_starts': 820, 'gamma': 0.9257538822087331, 'exploration_fraction': 0.21458355633951712, 'exploration_final_eps': 0.061887402614446765, 'train_freq': 4, 'target_update_interval': 110}. Best is trial 18 with value: -9983.8597712.\n",
      "[I 2025-08-08 17:18:31,681] Trial 46 finished with value: -11593.671442800001 and parameters: {'learning_rate': 0.0006149826694358577, 'buffer_size': 61860, 'learning_starts': 943, 'gamma': 0.9346967877702661, 'exploration_fraction': 0.43461635229636386, 'exploration_final_eps': 0.04472952294520954, 'train_freq': 5, 'target_update_interval': 299}. Best is trial 18 with value: -9983.8597712.\n",
      "[I 2025-08-08 17:18:39,883] Trial 47 finished with value: -12142.4075201 and parameters: {'learning_rate': 0.0008055473377801043, 'buffer_size': 69423, 'learning_starts': 851, 'gamma': 0.9462503145434161, 'exploration_fraction': 0.12712111867008916, 'exploration_final_eps': 0.05603129104854404, 'train_freq': 6, 'target_update_interval': 604}. Best is trial 18 with value: -9983.8597712.\n",
      "[I 2025-08-08 17:18:49,489] Trial 48 finished with value: -13373.3600401 and parameters: {'learning_rate': 0.0008873083586959628, 'buffer_size': 55321, 'learning_starts': 772, 'gamma': 0.9432253175990197, 'exploration_fraction': 0.24926414374684486, 'exploration_final_eps': 0.05264984891936021, 'train_freq': 4, 'target_update_interval': 666}. Best is trial 18 with value: -9983.8597712.\n",
      "[I 2025-08-08 17:18:57,637] Trial 49 finished with value: -12170.3268379 and parameters: {'learning_rate': 2.79125367733106e-05, 'buffer_size': 46828, 'learning_starts': 659, 'gamma': 0.9932036745995269, 'exploration_fraction': 0.20733234843954457, 'exploration_final_eps': 0.010900000932377949, 'train_freq': 9, 'target_update_interval': 181}. Best is trial 18 with value: -9983.8597712.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- DQN Optimization Finished ---\n",
      "Number of finished trials:  50\n",
      "Best trial:\n",
      "  Value (Average Reward):  -9983.8597712\n",
      "  Params: \n",
      "    learning_rate: 0.00039846792501639156\n",
      "    buffer_size: 30551\n",
      "    learning_starts: 898\n",
      "    gamma: 0.9704260371026998\n",
      "    exploration_fraction: 0.42882447366004917\n",
      "    exploration_final_eps: 0.06704927882225381\n",
      "    train_freq: 6\n",
      "    target_update_interval: 428\n",
      "\n",
      "==================================================\n",
      "\n",
      "--- Training the final DQN model with best hyperparameters ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cae47922195c43e49db61e91ef4ed303",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='Training final model: <b>DQN</b>'), IntProgress(value=0, description='Training DQN:…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved to: Output/DQN/best_model.pt\n",
      "\n",
      "--- Evaluating the final DQN model ---\n",
      "Evaluation logs saved to: Output/DQN/evaluation_logs.csv\n",
      "\n",
      "Final DQN Metrics:\n",
      "Average Reward: -12.4476\n",
      "Average Battery Health: 74.9739\n",
      "Average Efficiency: 6.8221\n",
      "Average Regret: 13.1298\n",
      "Energy Fulfillment Rate: 9.9105\n",
      "Evaluation plots saved to: Output/DQN\n",
      "\n",
      "==================================================\n",
      "\n",
      "\n",
      "--- All models have been processed. The script has completed. ---\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    models_to_tune = ['PPO', 'A2C', 'DDPG', 'SAC', 'DQN']\n",
    "    n_trials = 50\n",
    "    timesteps_per_trial = 10000\n",
    "    evaluation_timesteps = 10000\n",
    "\n",
    "    # Create the main output directory\n",
    "    output_dir = \"Output\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # --- Setup Overall Progress Bar ---\n",
    "    overall_progress = IntProgress(min=0, max=len(models_to_tune) * 2, description='Overall Progress:')\n",
    "    overall_description = HTML('Overall Progress: <b>0</b>/10 models completed.')\n",
    "    overall_vbox = VBox([overall_description, overall_progress])\n",
    "    display(overall_vbox)\n",
    "\n",
    "    all_metrics = {}\n",
    "    all_dfs = {}\n",
    "\n",
    "    for i, model_name in enumerate(models_to_tune):\n",
    "        all_metrics[model_name] = {}\n",
    "        all_dfs[model_name] = {}\n",
    "        # Create a specific directory for the current model\n",
    "        model_output_dir = os.path.join(output_dir, model_name)\n",
    "        os.makedirs(model_output_dir, exist_ok=True)\n",
    "\n",
    "        overall_description.value = f'Overall Progress: Currently processing <b>{model_name} (Tuned)</b> ({i*2+1}/{len(models_to_tune)*2})'\n",
    "        \n",
    "        print(f\"--- Starting Optuna optimization for {model_name} with {n_trials} trials ---\")\n",
    "\n",
    "        optuna_progress = IntProgress(min=0, max=n_trials, description=f'Tuning {model_name}:')\n",
    "        optuna_description = HTML(f'Tuning model: <b>{model_name}</b>, Trial 0/{n_trials}')\n",
    "        optuna_vbox = VBox([optuna_description, optuna_progress])\n",
    "        display(optuna_vbox)\n",
    "\n",
    "        study = optuna.create_study(\n",
    "            direction='maximize',\n",
    "            study_name=f'{model_name}_optimization'\n",
    "        )\n",
    "        # Pass the timesteps variable to the objective function\n",
    "        func = lambda trial: objective(trial, model_name, timesteps_per_trial)\n",
    "        study.optimize(func, n_trials=n_trials, callbacks=[OptunaProgressCallback(optuna_progress, optuna_description, n_trials)])\n",
    "        \n",
    "        optuna_progress.value = n_trials\n",
    "        optuna_description.value = f'Tuning model: <b>{model_name}</b>, Trial {n_trials}/{n_trials} - Complete!'\n",
    "\n",
    "        print(f\"\\n--- {model_name} Optimization Finished ---\")\n",
    "        print(\"Number of finished trials: \", len(study.trials))\n",
    "        best_trial = study.best_trial\n",
    "        best_trial_params = best_trial.params\n",
    "\n",
    "        print(\"Best trial:\")\n",
    "        print(\"  Value (Average Reward): \", best_trial.value)\n",
    "        print(\"  Params: \")\n",
    "        for key, value in best_trial_params.items():\n",
    "            print(f\"    {key}: {value}\")\n",
    "\n",
    "        print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "        print(f\"--- Training the final {model_name} model with best hyperparameters ---\")\n",
    "\n",
    "        training_progress = IntProgress(min=0, max=evaluation_timesteps, description=f'Training {model_name}:')\n",
    "        training_description = HTML(f'Training final model: <b>{model_name}</b>')\n",
    "        training_vbox = VBox([training_description, training_progress])\n",
    "        display(training_vbox)\n",
    "\n",
    "        if model_name == \"DQN\":\n",
    "            env_class = TEGDiscreteEnvironment\n",
    "            policy_name = \"MlpPolicy\"\n",
    "        else:\n",
    "            env_class = TEGEnvironment\n",
    "            policy_name = \"MlpPolicy\"\n",
    "\n",
    "        model_class = globals()[model_name]\n",
    "\n",
    "        final_env = make_vec_env(lambda: env_class(), n_envs=1)\n",
    "        final_model = model_class(\n",
    "            policy_name,\n",
    "            final_env,\n",
    "            **best_trial_params,\n",
    "            verbose=0,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        final_model.learn(total_timesteps=evaluation_timesteps, callback=TrainingProgressCallback(training_progress))\n",
    "\n",
    "        training_progress.value = evaluation_timesteps\n",
    "        training_description.value = f'Training final model: <b>{model_name}</b> - Complete!'\n",
    "\n",
    "        # Save the best model\n",
    "        model_save_path = os.path.join(model_output_dir, \"best_model.pt\")\n",
    "        final_model.save(model_save_path)\n",
    "        print(f\"Best model saved to: {model_save_path}\")\n",
    "\n",
    "        print(f\"\\n--- Evaluating the final {model_name} model ---\")\n",
    "        eval_env_final = make_vec_env(lambda: env_class(), n_envs=1)\n",
    "        \n",
    "        reset_output = eval_env_final.reset()\n",
    "        if isinstance(reset_output, tuple):\n",
    "            obs, info = reset_output\n",
    "        else:\n",
    "            obs = reset_output\n",
    "            info = {}\n",
    "        \n",
    "        all_logs = []\n",
    "        for _ in range(evaluation_timesteps):\n",
    "            action, _states = final_model.predict(obs, deterministic=True)\n",
    "            \n",
    "            step_output = eval_env_final.step(action)\n",
    "            \n",
    "            if len(step_output) == 5:\n",
    "                obs, reward, terminated, truncated, info = step_output\n",
    "            else:\n",
    "                obs, reward, done, info = step_output\n",
    "                terminated = done\n",
    "                truncated = False\n",
    "\n",
    "            df_logs = eval_env_final.envs[0].env.get_logs()\n",
    "            if not df_logs.empty:\n",
    "                all_logs.append(df_logs.iloc[-1])\n",
    "            \n",
    "            if terminated or truncated:\n",
    "                reset_output = eval_env_final.reset()\n",
    "                if isinstance(reset_output, tuple):\n",
    "                    obs, info = reset_output\n",
    "                else:\n",
    "                    obs = reset_output\n",
    "                    info = {}\n",
    "        \n",
    "        if all_logs:\n",
    "            df_final = pd.DataFrame(all_logs)\n",
    "            all_dfs[model_name][\"tuned\"] = df_final\n",
    "\n",
    "            # Save the final logs to a CSV file\n",
    "            logs_save_path = os.path.join(model_output_dir, \"evaluation_logs.csv\")\n",
    "            df_final.to_csv(logs_save_path, index=False)\n",
    "            print(f\"Evaluation logs saved to: {logs_save_path}\")\n",
    "\n",
    "            metrics_final = calculate_metrics(df_final)\n",
    "            all_metrics[model_name][\"tuned\"] = metrics_final\n",
    "\n",
    "            print(f\"\\nFinal {model_name} Metrics:\")\n",
    "            for key, value in metrics_final.items():\n",
    "                if value is not None:\n",
    "                    print(f\"{key}: {value:.4f}\")\n",
    "            \n",
    "            # Save the plots\n",
    "            plot_results(df_final, f\"Final Optimized {model_name}\", model_output_dir, eval_env_final.envs[0].env.max_steps)\n",
    "            print(f\"Evaluation plots saved to: {model_output_dir}\")\n",
    "        else:\n",
    "            print(f\"No logs were collected for {model_name} due to early termination.\")\n",
    "\n",
    "        print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "        \n",
    "        overall_progress.value += 1\n",
    "        overall_description.value = f'Overall Progress: Finished <b>{model_name} (Tuned)</b> ({i*2+1}/{len(models_to_tune)*2})'\n",
    "\n",
    "        # --- BENCHMARK MODEL ---\n",
    "        overall_description.value = f'Overall Progress: Currently processing <b>{model_name} (Benchmark)</b> ({i*2+2}/{len(models_to_tune)*2})'\n",
    "        \n",
    "        print(f\"--- Training the benchmark {model_name} model with default hyperparameters ---\")\n",
    "        \n",
    "        benchmark_output_dir = os.path.join(output_dir, f\"{model_name}_Benchmark\")\n",
    "        os.makedirs(benchmark_output_dir, exist_ok=True)\n",
    "\n",
    "        training_progress = IntProgress(min=0, max=evaluation_timesteps, description=f'Training {model_name} (Benchmark):')\n",
    "        training_description = HTML(f'Training benchmark model: <b>{model_name}</b>')\n",
    "        training_vbox = VBox([training_description, training_progress])\n",
    "        display(training_vbox)\n",
    "\n",
    "        benchmark_env = make_vec_env(lambda: env_class(), n_envs=1)\n",
    "        benchmark_model = model_class(\n",
    "            policy_name,\n",
    "            benchmark_env,\n",
    "            **DEFAULT_HYPERPARAMS[model_name],\n",
    "            verbose=0,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        benchmark_model.learn(total_timesteps=evaluation_timesteps, callback=TrainingProgressCallback(training_progress))\n",
    "\n",
    "        training_progress.value = evaluation_timesteps\n",
    "        training_description.value = f'Training benchmark model: <b>{model_name}</b> - Complete!'\n",
    "\n",
    "        # Save the benchmark model\n",
    "        model_save_path = os.path.join(benchmark_output_dir, \"benchmark_model.pt\")\n",
    "        benchmark_model.save(model_save_path)\n",
    "        print(f\"Benchmark model saved to: {model_save_path}\")\n",
    "\n",
    "        print(f\"\\n--- Evaluating the benchmark {model_name} model ---\")\n",
    "        eval_env_benchmark = make_vec_env(lambda: env_class(), n_envs=1)\n",
    "        \n",
    "        reset_output = eval_env_benchmark.reset()\n",
    "        if isinstance(reset_output, tuple):\n",
    "            obs, info = reset_output\n",
    "        else:\n",
    "            obs = reset_output\n",
    "            info = {}\n",
    "            \n",
    "        all_logs = []\n",
    "        for _ in range(evaluation_timesteps):\n",
    "            action, _states = benchmark_model.predict(obs, deterministic=True)\n",
    "            \n",
    "            step_output = eval_env_benchmark.step(action)\n",
    "            \n",
    "            if len(step_output) == 5:\n",
    "                obs, reward, terminated, truncated, info = step_output\n",
    "            else:\n",
    "                obs, reward, done, info = step_output\n",
    "                terminated = done\n",
    "                truncated = False\n",
    "\n",
    "            df_logs = eval_env_benchmark.envs[0].env.get_logs()\n",
    "            if not df_logs.empty:\n",
    "                all_logs.append(df_logs.iloc[-1])\n",
    "            \n",
    "            if terminated or truncated:\n",
    "                reset_output = eval_env_benchmark.reset()\n",
    "                if isinstance(reset_output, tuple):\n",
    "                    obs, info = reset_output\n",
    "                else:\n",
    "                    obs = reset_output\n",
    "                    info = {}\n",
    "\n",
    "        if all_logs:\n",
    "            df_benchmark = pd.DataFrame(all_logs)\n",
    "            all_dfs[model_name][\"benchmark\"] = df_benchmark\n",
    "\n",
    "            # Save the benchmark logs to a CSV file\n",
    "            logs_save_path = os.path.join(benchmark_output_dir, \"benchmark_evaluation_logs.csv\")\n",
    "            df_benchmark.to_csv(logs_save_path, index=False)\n",
    "            print(f\"Benchmark evaluation logs saved to: {logs_save_path}\")\n",
    "\n",
    "            metrics_benchmark = calculate_metrics(df_benchmark)\n",
    "            all_metrics[model_name][\"benchmark\"] = metrics_benchmark\n",
    "\n",
    "            print(f\"\\nBenchmark {model_name} Metrics:\")\n",
    "            for key, value in metrics_benchmark.items():\n",
    "                if value is not None:\n",
    "                    print(f\"{key}: {value:.4f}\")\n",
    "            \n",
    "            # Save the plots\n",
    "            plot_results(df_benchmark, f\"Benchmark {model_name}\", benchmark_output_dir, eval_env_benchmark.envs[0].env.max_steps)\n",
    "            print(f\"Benchmark evaluation plots saved to: {benchmark_output_dir}\")\n",
    "        else:\n",
    "            print(f\"No logs were collected for benchmark {model_name} due to early termination.\")\n",
    "\n",
    "        print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "        \n",
    "        overall_progress.value += 1\n",
    "        overall_description.value = f'Overall Progress: Finished <b>{model_name} (Benchmark)</b> ({i*2+2}/{len(models_to_tune)*2})'\n",
    "\n",
    "        # --- Generate Comparison Plots ---\n",
    "        if \"tuned\" in all_dfs[model_name] and \"benchmark\" in all_dfs[model_name]:\n",
    "            plot_comparison_results(\n",
    "                all_dfs[model_name][\"tuned\"],\n",
    "                all_dfs[model_name][\"benchmark\"],\n",
    "                model_name,\n",
    "                output_dir\n",
    "            )\n",
    "\n",
    "    overall_description.value = f'Overall Progress: All {len(models_to_tune)} models completed!'\n",
    "    print(\"\\n--- All models have been processed. The script has completed. ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6282280b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Final Comparison of All Models ---\n",
    "\n",
    "def generate_comparison_table(metrics_data):\n",
    "    \"\"\"Generates a formatted DataFrame to compare tuned and benchmark models.\"\"\"\n",
    "    comparison_list = []\n",
    "    for model, types in metrics_data.items():\n",
    "        if \"tuned\" in types and \"benchmark\" in types:\n",
    "            tuned_metrics = types[\"tuned\"]\n",
    "            benchmark_metrics = types[\"benchmark\"]\n",
    "            \n",
    "            for metric_name in tuned_metrics.keys():\n",
    "                if tuned_metrics[metric_name] is not None and benchmark_metrics.get(metric_name) is not None:\n",
    "                    improvement = ((tuned_metrics[metric_name] - benchmark_metrics[metric_name]) / abs(benchmark_metrics[metric_name])) * 100 if benchmark_metrics[metric_name] != 0 else float('inf')\n",
    "                    comparison_list.append({\n",
    "                        \"Model\": model,\n",
    "                        \"Metric\": metric_name,\n",
    "                        \"Tuned\": tuned_metrics[metric_name],\n",
    "                        \"Benchmark\": benchmark_metrics[metric_name],\n",
    "                        \"Improvement (%)\": improvement\n",
    "                    })\n",
    "\n",
    "    if not comparison_list:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df_comparison = pd.DataFrame(comparison_list)\n",
    "    \n",
    "    # Pivot table for better readability\n",
    "    df_pivot = df_comparison.pivot(index='Model', columns='Metric', values=['Tuned', 'Benchmark', 'Improvement (%)'])\n",
    "    \n",
    "    # Style the DataFrame for better visualization\n",
    "    styled_df = df_pivot.style.background_gradient(\n",
    "        cmap='RdYlGn',\n",
    "        subset=[(col[0], col[1]) for col in df_pivot.columns if col[0] == 'Improvement (%)'],\n",
    "        axis=1\n",
    "    ).format(\"{:.2f}\")\n",
    "    \n",
    "    return styled_df\n",
    "\n",
    "print(\"--- Comparison of Tuned vs. Benchmark Models ---\")\n",
    "comparison_table = generate_comparison_table(all_metrics)\n",
    "display(comparison_table)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_GPU_DL_PyTorch (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
