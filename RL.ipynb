{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2242a5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3 import PPO, A2C, DDPG, SAC, DQN\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize, VecMonitor\n",
    "from stable_baselines3.common.vec_env.subproc_vec_env import SubprocVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "import torch\n",
    "import optuna\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import IntProgress, HTML, VBox\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2517c3c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA GPU is available. Using the GPU for training and evaluation: NVIDIA GeForce RTX 5070\n"
     ]
    }
   ],
   "source": [
    "# Check for and use a CUDA-enabled GPU if available.\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    print(f\"CUDA GPU is available. Using the GPU for training and evaluation: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(\"CUDA GPU not found. Using the CPU for training and evaluation.\")\n",
    "\n",
    "# --- Environment for Continuous Action Spaces (PPO, A2C, DDPG, SAC) ---\n",
    "# NOTE: This is the user-provided TEGEnvironment class which fixed the NaN problem,\n",
    "# with updates to the `reset` and `step` methods to be compatible with Gym v0.26+.\n",
    "class TEGEnvironment(gym.Env):\n",
    "    \"\"\"\n",
    "    A custom Gym environment for a Thermoelectric Generator (TEG) system with a continuous action space.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(TEGEnvironment, self).__init__()\n",
    "\n",
    "        # Action space: Charge rate, Store rate, Idle rate (continuous, proportions)\n",
    "        self.action_space = spaces.Box(low=0, high=1, shape=(3,), dtype=np.float32)\n",
    "\n",
    "        # State space: Voltage, Current, Battery Level, Buffer Level, Temp Gradient,\n",
    "        # Energy Demand, Battery Health\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=np.array([0, 0, 0, 0, 0, 0, 0]),\n",
    "            high=np.array([10, 10, 100, 100, 100, 50, 100]),\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "        self.logs = []\n",
    "        self.np_random = np.random.RandomState()\n",
    "        self.max_steps = 1000  # Define a maximum number of steps for truncation\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        \"\"\"\n",
    "        Resets the environment. Updated to return `observation, info` as per Gym v0.26+.\n",
    "        \"\"\"\n",
    "        if seed is not None:\n",
    "            self.np_random.seed(seed)\n",
    "            random.seed(seed)\n",
    "            pass\n",
    "\n",
    "        self.voltage = random.uniform(5, 7)\n",
    "        self.current = self.voltage / 10\n",
    "        self.battery_level = 50\n",
    "        self.buffer_level = 10\n",
    "        self.temperature_gradient = random.uniform(40, 60)\n",
    "        self.energy_demand = 10\n",
    "        self.battery_health = 100\n",
    "        self.current_step = 0\n",
    "        self.logs = []\n",
    "        \n",
    "        # Return observation and an empty info dictionary\n",
    "        return self._get_observation(), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Takes a step in the environment. Updated to return 5 values as per Gym v0.26+.\n",
    "        \"\"\"\n",
    "        self.current_step += 1\n",
    "\n",
    "        # Normalize actions\n",
    "        total_action = np.sum(action)\n",
    "        if total_action > 1:\n",
    "            action = action / total_action\n",
    "\n",
    "        charge_action, store_action, idle_action = action\n",
    "        available_energy = self.voltage * self.current\n",
    "\n",
    "        # Loss factors and efficiencies\n",
    "        charge_efficiency = 0.85\n",
    "        store_efficiency = 0.80\n",
    "        load_efficiency = 0.75\n",
    "        system_losses = 0.1\n",
    "\n",
    "        # Calculate energy allocations\n",
    "        energy_to_charge = available_energy * charge_action * charge_efficiency\n",
    "        energy_to_store = available_energy * store_action * store_efficiency\n",
    "        delivered_energy = available_energy * idle_action * load_efficiency\n",
    "\n",
    "        # Update battery and buffer levels\n",
    "        self.battery_level += energy_to_charge\n",
    "        self.battery_level = np.clip(self.battery_level, 0, 100)\n",
    "\n",
    "        self.buffer_level += energy_to_store\n",
    "        self.buffer_level = np.clip(self.buffer_level, 0, 100)\n",
    "\n",
    "        # Update battery health\n",
    "        self.battery_health -= charge_action * 0.2\n",
    "        self.battery_health = np.clip(self.battery_health, 0, 100)\n",
    "\n",
    "        # Calculate efficiency (Carnot limit)\n",
    "        thot = self.temperature_gradient\n",
    "        tcold = thot - 5\n",
    "        carnot_efficiency = (thot - tcold) / thot\n",
    "        net_efficiency = carnot_efficiency * load_efficiency * (1 - system_losses)\n",
    "\n",
    "        # Calculate reward\n",
    "        unmet_demand = max(self.energy_demand - delivered_energy, 0)\n",
    "        reward = -unmet_demand\n",
    "        reward += net_efficiency * 10\n",
    "        reward -= max(0, self.battery_level - 95)\n",
    "\n",
    "        # Calculate maximum possible reward (assumes perfect efficiency and no unmet demand)\n",
    "        max_possible_reward = 0\n",
    "        max_possible_reward += 10 * net_efficiency\n",
    "\n",
    "        # Calculate regret (difference between max possible and actual reward)\n",
    "        regret = max_possible_reward - reward\n",
    "\n",
    "        # Update environment variables\n",
    "        self.voltage = max(5 + random.gauss(0, 0.5), 0)\n",
    "        self.temperature_gradient = max(50 + random.gauss(0, 5), 0)\n",
    "        self.energy_demand = max(10 + random.gauss(0, 2), 0)\n",
    "\n",
    "        self._log_data(net_efficiency, reward, regret, delivered_energy)\n",
    "\n",
    "        # Check termination conditions\n",
    "        terminated = self.battery_health <= 0 or self.battery_level <= 0\n",
    "        truncated = self.current_step >= self.max_steps\n",
    "        \n",
    "        # Return observation, reward, terminated, truncated, and an empty info dictionary\n",
    "        return self._get_observation(), reward, terminated, truncated, {}\n",
    "\n",
    "    def _get_observation(self):\n",
    "        return np.array([\n",
    "            self.voltage, self.current, self.battery_level,\n",
    "            self.buffer_level, self.temperature_gradient,\n",
    "            self.energy_demand, self.battery_health\n",
    "        ], dtype=np.float32) # \n",
    "\n",
    "    def _log_data(self, efficiency, reward, regret, delivered_energy):\n",
    "        log_entry = {\n",
    "            \"Thot\": self.temperature_gradient,\n",
    "            \"Power\": self.voltage * self.current,\n",
    "            \"Qhot\": self.battery_level,\n",
    "            \"Qcold\": self.buffer_level,\n",
    "            \"Efficiency\": efficiency * 100,\n",
    "            \"Reward\": reward,\n",
    "            \"Regret\": regret,\n",
    "            \"Battery Health\": self.battery_health,\n",
    "            # \"Energy Demand Fulfilled\": max(self.energy_demand - reward, 0),\n",
    "            \"Energy Demand\": self.energy_demand, # Log energy demand to calculate fulfillment rate per episode\n",
    "            \"Energy Demand Fulfilled\": delivered_energy\n",
    "        }\n",
    "        self.logs.append(log_entry)\n",
    "\n",
    "    def get_logs(self):\n",
    "        return pd.DataFrame(self.logs)\n",
    "\n",
    "# --- Environment for Discrete Action Space (DQN) ---\n",
    "# This is a new version of the discrete environment based on your new TEG logic.\n",
    "class TEGDiscreteEnvironment(gym.Env):\n",
    "    \"\"\"\n",
    "    A custom Gym environment for the TEG system with a discrete action space,\n",
    "    designed to be compatible with DQN.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(TEGDiscreteEnvironment, self).__init__()\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=np.array([0, 0, 0, 0, 0, 0, 0]),\n",
    "            high=np.array([10, 10, 100, 100, 100, 50, 100]),\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "        # Discrete action space: 0 = high charge, 1 = low charge, 2 = idle\n",
    "        self.action_space = spaces.Discrete(3)\n",
    "        self.logs = []\n",
    "        self.np_random = np.random.RandomState()\n",
    "        self.max_steps = 1000\n",
    "\n",
    "        # Map discrete actions to continuous rates.\n",
    "        self.action_map = {\n",
    "            0: [0.8, 0.1, 0.1],  # High charge rate\n",
    "            1: [0.3, 0.4, 0.3],  # Low charge rate\n",
    "            2: [0.1, 0.1, 0.8]   # Idle (prioritize buffering)\n",
    "        }\n",
    "        self.reset()\n",
    "\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        if seed is not None:\n",
    "            self.np_random.seed(seed)\n",
    "            random.seed(seed)\n",
    "\n",
    "        self.voltage = random.uniform(5, 7)\n",
    "        self.current = self.voltage / 10\n",
    "        self.battery_level = 50\n",
    "        self.buffer_level = 10\n",
    "        self.temperature_gradient = random.uniform(40, 60)\n",
    "        self.energy_demand = 10\n",
    "        self.battery_health = 100\n",
    "        self.current_step = 0\n",
    "        self.logs = []\n",
    "        \n",
    "        return self._get_observation(), {}\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        self.current_step += 1\n",
    "        \n",
    "        # Get the continuous action from the discrete action map\n",
    "        charge_action, store_action, idle_action = self.action_map[action]\n",
    "        available_energy = self.voltage * self.current\n",
    "\n",
    "        # Loss factors and efficiencies\n",
    "        charge_efficiency = 0.85\n",
    "        store_efficiency = 0.80\n",
    "        load_efficiency = 0.75\n",
    "        system_losses = 0.1\n",
    "\n",
    "        # Calculate energy allocations\n",
    "        energy_to_charge = available_energy * charge_action * charge_efficiency\n",
    "        energy_to_store = available_energy * store_action * store_efficiency\n",
    "        delivered_energy = available_energy * idle_action * load_efficiency\n",
    "\n",
    "        # Update battery and buffer levels\n",
    "        self.battery_level += energy_to_charge\n",
    "        self.battery_level = np.clip(self.battery_level, 0, 100)\n",
    "\n",
    "        self.buffer_level += energy_to_store\n",
    "        self.buffer_level = np.clip(self.buffer_level, 0, 100)\n",
    "\n",
    "        # Update battery health\n",
    "        self.battery_health -= charge_action * 0.2\n",
    "        self.battery_health = np.clip(self.battery_health, 0, 100)\n",
    "\n",
    "        # Calculate efficiency (Carnot limit)\n",
    "        thot = self.temperature_gradient\n",
    "        tcold = thot - 5\n",
    "        carnot_efficiency = (thot - tcold) / thot\n",
    "        net_efficiency = carnot_efficiency * load_efficiency * (1 - system_losses)\n",
    "\n",
    "        # Calculate reward\n",
    "        unmet_demand = max(self.energy_demand - delivered_energy, 0)\n",
    "        reward = -unmet_demand\n",
    "        reward += net_efficiency * 10\n",
    "        reward -= max(0, self.battery_level - 95)\n",
    "\n",
    "        # Calculate maximum possible reward (assumes perfect efficiency and no unmet demand)\n",
    "        max_possible_reward = 0\n",
    "        max_possible_reward += 10 * net_efficiency\n",
    "\n",
    "        # Calculate regret (difference between max possible and actual reward)\n",
    "        regret = max_possible_reward - reward\n",
    "\n",
    "        # Update environment variables\n",
    "        self.voltage = max(5 + random.gauss(0, 0.5), 0)\n",
    "        self.temperature_gradient = max(50 + random.gauss(0, 5), 0)\n",
    "        self.energy_demand = max(10 + random.gauss(0, 2), 0)\n",
    "\n",
    "        self._log_data(net_efficiency, reward, regret, delivered_energy)\n",
    "\n",
    "        terminated = self.battery_health <= 0 or self.battery_level <= 0\n",
    "        truncated = self.current_step >= self.max_steps\n",
    "        \n",
    "        return self._get_observation(), reward, terminated, truncated, {}\n",
    "\n",
    "\n",
    "    def _get_observation(self):\n",
    "        return np.array([\n",
    "            self.voltage, self.current, self.battery_level,\n",
    "            self.buffer_level, self.temperature_gradient,\n",
    "            self.energy_demand, self.battery_health\n",
    "        ], dtype=np.float32)\n",
    "\n",
    "    def _log_data(self, efficiency, reward, regret, delivered_energy):\n",
    "        log_entry = {\n",
    "            \"Thot\": self.temperature_gradient,\n",
    "            \"Power\": self.voltage * self.current,\n",
    "            \"Qhot\": self.battery_level,\n",
    "            \"Qcold\": self.buffer_level,\n",
    "            \"Efficiency\": efficiency * 100,\n",
    "            \"Reward\": reward,\n",
    "            \"Regret\": regret,\n",
    "            \"Battery Health\": self.battery_health,\n",
    "            #\"Energy Demand Fulfilled\": max(self.energy_demand - reward, 0)\n",
    "            \"Energy Demand\": self.energy_demand, # Log energy demand to calculate fulfillment rate per episode\n",
    "            \"Energy Demand Fulfilled\": delivered_energy\n",
    "        }\n",
    "        self.logs.append(log_entry)\n",
    "\n",
    "    def get_logs(self):\n",
    "        return pd.DataFrame(self.logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1ad954c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(df):\n",
    "    \"\"\"Calculates key performance metrics from a DataFrame of evaluation logs.\"\"\"\n",
    "    metrics = {\n",
    "        \"Average Reward\": df[\"Reward\"].mean() if \"Reward\" in df.columns else None,\n",
    "        \"Average Battery Health\": df[\"Battery Health\"].mean() if \"Battery Health\" in df.columns else None,\n",
    "        \"Average Efficiency\": df[\"Efficiency\"].mean() if \"Efficiency\" in df.columns else None,\n",
    "        \"Average Regret\": df[\"Regret\"].mean() if \"Regret\" in df.columns else None,\n",
    "        \"Energy Fulfillment Rate\": (df[\"Energy Demand Fulfilled\"].sum() / df[\"Energy Demand\"].sum()) * 100 if \"Energy Demand\" in df.columns and \"Energy Demand Fulfilled\" in df.columns and df[\"Energy Demand\"].sum() > 0 else None,\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "def plot_results(df: pd.DataFrame, title_prefix: str, save_path: str, max_steps: int):\n",
    "    \"\"\"Generates a series of plots from evaluation logs and saves them.\"\"\"\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "    x = [i for i in range(len(df))]\n",
    "\n",
    "    # Plot 1: Efficiency Over Evaluation Steps\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(x,  df[\"Efficiency\"], label=\"Efficiency (%)\", color='b', linewidth=0.5)\n",
    "    plt.title(f\"Efficiency Over Evaluation Steps ({title_prefix})\", fontsize=16)\n",
    "    plt.xlabel(\"Steps\", fontsize=14)\n",
    "    plt.ylabel(\"Efficiency (%)\", fontsize=14)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_path, \"efficiency_plot.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # Plot 2: Power vs. Temperature Gradient (remains a scatter plot)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.scatter(df[\"Thot\"], df[\"Power\"], label=\"Power (W)\", c=\"r\", alpha=0.7, s=50)\n",
    "    plt.title(f\"Power vs. Temperature Gradient ({title_prefix})\", fontsize=16)\n",
    "    plt.xlabel(\"Temperature Gradient (Thot)\", fontsize=14)\n",
    "    plt.ylabel(\"Power (W)\", fontsize=14)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_path, \"power_vs_temp_plot.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # Plot 3: Battery and Buffer Levels Over Evaluation Steps\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(x, df[\"Qhot\"], label=\"Battery Level\", color='g', linewidth=2)\n",
    "    plt.plot(x, df[\"Qcold\"], label=\"Buffer Level\", color='orange', linewidth=2)\n",
    "    plt.title(f\"Battery and Buffer Levels Over Evaluation Steps ({title_prefix})\", fontsize=16)\n",
    "    plt.xlabel(\"Steps\", fontsize=14)\n",
    "    plt.ylabel(\"Energy Levels\", fontsize=14)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_path, \"battery_buffer_levels_plot.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # Plot 4: Cumulative Reward Over Evaluation Steps (remains cumulative)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot([i for i in range(len(df[\"Reward\"].cumsum()))], df[\"Reward\"].cumsum(), label=\"Cumulative Reward\", color='b', alpha=0.8, linewidth=2)\n",
    "    plt.title(f\"Cumulative Reward Over Evaluation Steps ({title_prefix})\", fontsize=16)\n",
    "    plt.xlabel(\"Steps\", fontsize=14)\n",
    "    plt.ylabel(\"Cumulative Reward\", fontsize=14)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_path, \"cumulative_reward_plot.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # Plot 5: Battery Health Over Evaluation Steps\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(x, df[\"Battery Health\"], label=\"Battery Health\", color='purple', linewidth=2)\n",
    "    plt.title(f\"Battery Health Over Evaluation Steps ({title_prefix})\", fontsize=16)\n",
    "    plt.xlabel(\"Steps\", fontsize=14)\n",
    "    plt.ylabel(\"Battery Health (%)\", fontsize=14)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_path, \"battery_health_plot.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # Plot 6: Regret Over Evaluation Steps\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(x, df[\"Regret\"], label=\"Regret\", color='r', linewidth=0.5)\n",
    "    plt.fill_between(x, df[\"Regret\"], color='red', alpha=0.4)\n",
    "    plt.title(f\"Regret Over Time ({title_prefix})\", fontsize=16)\n",
    "    plt.xlabel(\"Steps\", fontsize=14)\n",
    "    plt.ylabel(\"Regret\", fontsize=14)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_path, \"regret_over_time_plot.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # Plot 7: Energy Demand Fulfillment Rate Over Evaluation Steps\n",
    "    # Calculate the fulfillment rate for each step\n",
    "    fulfillment_rate_per_step = (df[\"Energy Demand Fulfilled\"] / df[\"Energy Demand\"]) * 100\n",
    "    fulfillment_rate_per_step.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    fulfillment_rate_per_step.fillna(100, inplace=True) # Assume 100% fulfillment if no demand\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(x, fulfillment_rate_per_step, color='cyan', label='Fulfillment Rate', linewidth=0.5)\n",
    "    plt.fill_between(x, fulfillment_rate_per_step, color='cyan', alpha=0.4)\n",
    "    plt.title(f\"Energy Demand Fulfillment Rate Over Evaluation Steps ({title_prefix})\", fontsize=16)\n",
    "    plt.xlabel(\"Steps\", fontsize=14)\n",
    "    plt.ylabel(\"Fulfillment Rate (%)\", fontsize=14)\n",
    "    plt.ylim(0)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_path, \"demand_fulfillment_rate_plot.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # Plot 8: Battery Level vs. Time (Single Episode)\n",
    "    single_episode_df = df.iloc[:max_steps]\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(single_episode_df.index, single_episode_df[\"Qhot\"], label=\"Battery Level\", color='green', linewidth=2)\n",
    "    plt.title(f\"Battery Level Over a Single Episode ({title_prefix})\", fontsize=16)\n",
    "    plt.xlabel(\"Steps\", fontsize=14)\n",
    "    plt.ylabel(\"Battery Level\", fontsize=14)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_path, \"single_episode_battery_level_plot.png\"))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1630586e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_comparison(df_a: pd.DataFrame, df_b: pd.DataFrame, labels: tuple, save_path: str, max_steps: int):\n",
    "    \"\"\"\n",
    "    Create comparison plots between two runs (e.g., Optimized vs Benchmark).\n",
    "    labels: (label_a, label_b)\n",
    "    Saves PNGs into save_path with prefix 'comparison_'.\n",
    "    \"\"\"\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "    # Align lengths for fair comparison\n",
    "    n = min(len(df_a), len(df_b))\n",
    "    if n == 0:\n",
    "        return\n",
    "    A = df_a.iloc[:n].reset_index(drop=True)\n",
    "    B = df_b.iloc[:n].reset_index(drop=True)\n",
    "    x = list(range(n))\n",
    "\n",
    "    la, lb = labels\n",
    "\n",
    "    # 1) Efficiency\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(x, A[\"Efficiency\"], label=f\"{la} Efficiency (%)\", color='tab:blue', linewidth=0.8)\n",
    "    plt.plot(x, B[\"Efficiency\"], label=f\"{lb} Efficiency (%)\", color='tab:orange', linewidth=0.8)\n",
    "    plt.title(f\"Efficiency Comparison: {la} vs {lb}\")\n",
    "    plt.xlabel(\"Steps\")\n",
    "    plt.ylabel(\"Efficiency (%)\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_path, \"comparison_efficiency.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # 2) Cumulative Reward\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(x, A[\"Reward\"].cumsum(), label=f\"{la} Cumulative Reward\", color='tab:blue', linewidth=1.5)\n",
    "    plt.plot(x, B[\"Reward\"].cumsum(), label=f\"{lb} Cumulative Reward\", color='tab:orange', linewidth=1.5)\n",
    "    plt.title(f\"Cumulative Reward: {la} vs {lb}\")\n",
    "    plt.xlabel(\"Steps\")\n",
    "    plt.ylabel(\"Cumulative Reward\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_path, \"comparison_cumulative_reward.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # 3) Battery Health\n",
    "    if \"Battery Health\" in A.columns and \"Battery Health\" in B.columns:\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.plot(x, A[\"Battery Health\"], label=f\"{la} Battery Health\", color='tab:green', linewidth=1.5)\n",
    "        plt.plot(x, B[\"Battery Health\"], label=f\"{lb} Battery Health\", color='tab:red', linewidth=1.5)\n",
    "        plt.title(f\"Battery Health: {la} vs {lb}\")\n",
    "        plt.xlabel(\"Steps\")\n",
    "        plt.ylabel(\"Battery Health (%)\")\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(save_path, \"comparison_battery_health.png\"))\n",
    "        plt.close()\n",
    "\n",
    "    # 4) Demand Fulfillment Rate per step\n",
    "    if \"Energy Demand\" in A.columns and \"Energy Demand Fulfilled\" in A.columns and \\\n",
    "       \"Energy Demand\" in B.columns and \"Energy Demand Fulfilled\" in B.columns:\n",
    "        fr_a = (A[\"Energy Demand Fulfilled\"] / A[\"Energy Demand\"]) * 100\n",
    "        fr_b = (B[\"Energy Demand Fulfilled\"] / B[\"Energy Demand\"]) * 100\n",
    "        fr_a.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        fr_b.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        fr_a.fillna(100, inplace=True)\n",
    "        fr_b.fillna(100, inplace=True)\n",
    "\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.plot(x, fr_a, label=f\"{la} Fulfillment Rate\", color='tab:cyan', linewidth=0.8)\n",
    "        plt.plot(x, fr_b, label=f\"{lb} Fulfillment Rate\", color='tab:pink', linewidth=0.8)\n",
    "        plt.title(f\"Energy Demand Fulfillment Rate: {la} vs {lb}\")\n",
    "        plt.xlabel(\"Steps\")\n",
    "        plt.ylabel(\"Fulfillment Rate (%)\")\n",
    "        plt.ylim(0)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(save_path, \"comparison_fulfillment_rate.png\"))\n",
    "        plt.close()\n",
    "\n",
    "    # 5) Regret\n",
    "    if \"Regret\" in A.columns and \"Regret\" in B.columns:\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.plot(x, A[\"Regret\"], label=f\"{la} Regret\", color='tab:purple', linewidth=0.6)\n",
    "        plt.plot(x, B[\"Regret\"], label=f\"{lb} Regret\", color='tab:brown', linewidth=0.6)\n",
    "        plt.title(f\"Regret: {la} vs {lb}\")\n",
    "        plt.xlabel(\"Steps\")\n",
    "        plt.ylabel(\"Regret\")\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(save_path, \"comparison_regret.png\"))\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "637847d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "HYPERPARAMETER_SPACES = {\n",
    "    \"PPO\": lambda trial: {\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-5, 1e-3, log=True),\n",
    "        'n_steps': trial.suggest_categorical('n_steps', [512, 1024, 2048, 4096]),\n",
    "        'batch_size': trial.suggest_categorical('batch_size', [64, 128, 256, 512]),\n",
    "        'gamma': trial.suggest_float('gamma', 0.9, 0.999),\n",
    "        'gae_lambda': trial.suggest_float('gae_lambda', 0.9, 0.99),\n",
    "        'clip_range': trial.suggest_float('clip_range', 0.1, 0.4),\n",
    "        'n_epochs': trial.suggest_int('n_epochs', 5, 20)\n",
    "        # 'learning_rate': 0.0003, 'n_steps': 2048, 'batch_size': 64, 'ent_coef': 0.01\n",
    "    },\n",
    "    \"A2C\": lambda trial: {\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-5, 1e-3, log=True),\n",
    "        'n_steps': trial.suggest_int('n_steps', 5, 50),\n",
    "        'gamma': trial.suggest_float('gamma', 0.9, 0.999),\n",
    "        'gae_lambda': trial.suggest_float('gae_lambda', 0.9, 1.0),\n",
    "        'ent_coef': trial.suggest_float('ent_coef', 1e-8, 1e-1, log=True),\n",
    "        'vf_coef': trial.suggest_float('vf_coef', 0.1, 1.0)\n",
    "    },\n",
    "    \"DDPG\": lambda trial: {\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-5, 1e-3, log=True),\n",
    "        'buffer_size': trial.suggest_int('buffer_size', 10000, 100000),\n",
    "        'learning_starts': trial.suggest_int('learning_starts', 100, 1000),\n",
    "        'tau': trial.suggest_float('tau', 0.001, 0.01),\n",
    "        'gamma': trial.suggest_float('gamma', 0.9, 0.999)\n",
    "    },\n",
    "    \"SAC\": lambda trial: {\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-5, 1e-3, log=True),\n",
    "        'buffer_size': trial.suggest_int('buffer_size', 10000, 100000),\n",
    "        'learning_starts': trial.suggest_int('learning_starts', 100, 1000),\n",
    "        'gamma': trial.suggest_float('gamma', 0.9, 0.999),\n",
    "        'tau': trial.suggest_float('tau', 0.001, 0.01),\n",
    "        'ent_coef': trial.suggest_float('ent_coef', 1e-8, 1e-1, log=True)\n",
    "    },\n",
    "    \"DQN\": lambda trial: {\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-5, 1e-3, log=True),\n",
    "        'buffer_size': trial.suggest_int('buffer_size', 10000, 100000),\n",
    "        'learning_starts': trial.suggest_int('learning_starts', 100, 1000),\n",
    "        'gamma': trial.suggest_float('gamma', 0.9, 0.999),\n",
    "        'exploration_fraction': trial.suggest_float('exploration_fraction', 0.1, 0.5),\n",
    "        'exploration_final_eps': trial.suggest_float('exploration_final_eps', 0.01, 0.1),\n",
    "        'train_freq': trial.suggest_int('train_freq', 1, 10),\n",
    "        'target_update_interval': trial.suggest_int('target_update_interval', 100, 1000)\n",
    "    }\n",
    "}\n",
    "\n",
    "def objective(trial: optuna.Trial, model_name: str, timesteps: int) -> float:\n",
    "    \"\"\"\n",
    "    Defines the objective function for Optuna to optimize a given RL model.\n",
    "    It suggests hyperparameters, trains a model, and returns its average reward.\n",
    "    \"\"\"\n",
    "    if model_name == \"DQN\":\n",
    "        env_class = TEGDiscreteEnvironment\n",
    "        policy_name = \"MlpPolicy\"\n",
    "    else:\n",
    "        env_class = TEGEnvironment\n",
    "        policy_name = \"MlpPolicy\"\n",
    "        \n",
    "    hyperparams = HYPERPARAMETER_SPACES[model_name](trial)\n",
    "\n",
    "    # PPO-specific check to ensure batch_size is a factor of n_steps\n",
    "    if model_name == \"PPO\":\n",
    "        n_steps = hyperparams['n_steps']\n",
    "        batch_size = hyperparams['batch_size']\n",
    "        if n_steps % batch_size != 0:\n",
    "            return -np.inf # Prune this trial\n",
    "    \n",
    "    model_class = globals()[model_name]\n",
    "\n",
    "    try:\n",
    "        env = make_vec_env(lambda: env_class(), n_envs=1)\n",
    "        \n",
    "        model = model_class(\n",
    "            policy_name,\n",
    "            env,\n",
    "            **hyperparams,\n",
    "            verbose=0,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        model.learn(total_timesteps=timesteps)\n",
    "\n",
    "        eval_env = make_vec_env(lambda: env_class(), n_envs=1)\n",
    "        mean_reward, _ = evaluate_policy(model, eval_env, n_eval_episodes=10)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Trial for {model_name} failed with error: {e}\")\n",
    "        return -np.inf\n",
    "\n",
    "    return mean_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75e03511",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptunaProgressCallback:\n",
    "    \"\"\"Callback to update a progress bar during Optuna optimization.\"\"\"\n",
    "    def __init__(self, progress_bar, desc_widget, total_trials):\n",
    "        self.progress_bar = progress_bar\n",
    "        self.desc_widget = desc_widget\n",
    "        self.total_trials = total_trials\n",
    "\n",
    "    def __call__(self, study, trial):\n",
    "        self.progress_bar.value = trial.number + 1\n",
    "        self.desc_widget.value = f'Tuning model: <b>{study.study_name.split(\"_\")[0]}</b>, Trial {trial.number + 1}/{self.total_trials}'\n",
    "\n",
    "class TrainingProgressCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback to update a progress bar during Stable-Baselines3 training.\n",
    "    The max value is set in _on_training_start, where total_timesteps is available.\n",
    "    \"\"\"\n",
    "    def __init__(self, progress_bar, verbose=0):\n",
    "        super(TrainingProgressCallback, self).__init__(verbose)\n",
    "        self.progress_bar = progress_bar\n",
    "\n",
    "    def _on_training_start(self) -> None:\n",
    "        \"\"\"Called once at the beginning of training.\"\"\"\n",
    "        self.progress_bar.max = self.locals['total_timesteps']\n",
    "        self.progress_bar.value = self.num_timesteps\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        \"\"\"Called every step.\"\"\"\n",
    "        self.progress_bar.value = self.num_timesteps\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4cdc788",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95aa62e522954ae8a016f9d81b041250",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='Overall Progress: <b>0</b>/5 models completed.'), IntProgress(value=0, description=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Optuna optimization for PPO with 50 trials ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c476633f0b84852a87805ff720aea45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='Tuning model: <b>PPO</b>, Trial 0/50'), IntProgress(value=0, description='Tuning PP…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-26 12:03:41,812] A new study created in memory with name: PPO_optimization\n",
      "/mnt/c/Data/python_venv/ML_GPU_DL_PyTorch/lib/python3.12/site-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n",
      "[I 2025-08-26 12:04:19,501] Trial 0 finished with value: -8946.347778799998 and parameters: {'learning_rate': 0.00013795866117382222, 'n_steps': 4096, 'batch_size': 512, 'gamma': 0.9976420130518263, 'gae_lambda': 0.9438731367469118, 'clip_range': 0.19651386847358984, 'n_epochs': 15}. Best is trial 0 with value: -8946.347778799998.\n",
      "[I 2025-08-26 12:04:47,367] Trial 1 finished with value: -7868.8911589 and parameters: {'learning_rate': 0.0007520772699118875, 'n_steps': 1024, 'batch_size': 64, 'gamma': 0.9948587316919916, 'gae_lambda': 0.9058921765339731, 'clip_range': 0.19650225597504692, 'n_epochs': 15}. Best is trial 1 with value: -7868.8911589.\n",
      "[I 2025-08-26 12:05:07,195] Trial 2 finished with value: -13320.635323999999 and parameters: {'learning_rate': 8.506436210975442e-05, 'n_steps': 2048, 'batch_size': 128, 'gamma': 0.9249212947519924, 'gae_lambda': 0.9649161836615869, 'clip_range': 0.15625884582948987, 'n_epochs': 12}. Best is trial 1 with value: -7868.8911589.\n",
      "[I 2025-08-26 12:05:24,097] Trial 3 finished with value: -9034.5974659 and parameters: {'learning_rate': 0.000168408741284008, 'n_steps': 2048, 'batch_size': 512, 'gamma': 0.9470680460252012, 'gae_lambda': 0.9862288743456504, 'clip_range': 0.11842710495012068, 'n_epochs': 11}. Best is trial 1 with value: -7868.8911589.\n",
      "[I 2025-08-26 12:05:44,227] Trial 4 finished with value: -9121.9539776 and parameters: {'learning_rate': 0.0007843456039157551, 'n_steps': 4096, 'batch_size': 512, 'gamma': 0.9511298151700058, 'gae_lambda': 0.9787338884077886, 'clip_range': 0.15179993718577064, 'n_epochs': 16}. Best is trial 1 with value: -7868.8911589.\n",
      "[I 2025-08-26 12:06:03,040] Trial 5 finished with value: -10232.4636106 and parameters: {'learning_rate': 2.08176833010607e-05, 'n_steps': 1024, 'batch_size': 256, 'gamma': 0.953086642585439, 'gae_lambda': 0.931845615786246, 'clip_range': 0.33173022930457197, 'n_epochs': 18}. Best is trial 1 with value: -7868.8911589.\n",
      "[I 2025-08-26 12:06:33,232] Trial 6 finished with value: -8994.654340100002 and parameters: {'learning_rate': 0.00019886646604150792, 'n_steps': 2048, 'batch_size': 64, 'gamma': 0.976361159164715, 'gae_lambda': 0.9361201225710661, 'clip_range': 0.1168430598635318, 'n_epochs': 19}. Best is trial 1 with value: -7868.8911589.\n",
      "[I 2025-08-26 12:06:53,041] Trial 7 finished with value: -9279.811725399999 and parameters: {'learning_rate': 0.0004330319028627673, 'n_steps': 4096, 'batch_size': 256, 'gamma': 0.9798331051924947, 'gae_lambda': 0.9033814184876737, 'clip_range': 0.21132563245362107, 'n_epochs': 13}. Best is trial 1 with value: -7868.8911589.\n",
      "[I 2025-08-26 12:07:09,508] Trial 8 finished with value: -9219.967855 and parameters: {'learning_rate': 3.374670160197455e-05, 'n_steps': 2048, 'batch_size': 512, 'gamma': 0.9864247566385664, 'gae_lambda': 0.9186377739418765, 'clip_range': 0.2546918246071984, 'n_epochs': 20}. Best is trial 1 with value: -7868.8911589.\n",
      "[I 2025-08-26 12:07:32,188] Trial 9 finished with value: -9117.849030200001 and parameters: {'learning_rate': 1.370911441181093e-05, 'n_steps': 1024, 'batch_size': 64, 'gamma': 0.9891140983038613, 'gae_lambda': 0.9298850604870937, 'clip_range': 0.2885243966702086, 'n_epochs': 11}. Best is trial 1 with value: -7868.8911589.\n",
      "[I 2025-08-26 12:07:52,655] Trial 10 finished with value: -7040.8620511 and parameters: {'learning_rate': 0.0009576913013592695, 'n_steps': 512, 'batch_size': 64, 'gamma': 0.9008087811516762, 'gae_lambda': 0.908744186755244, 'clip_range': 0.39222663794276436, 'n_epochs': 5}. Best is trial 10 with value: -7040.8620511.\n",
      "[I 2025-08-26 12:08:13,354] Trial 11 finished with value: -7676.2079447 and parameters: {'learning_rate': 0.0009502574947101832, 'n_steps': 512, 'batch_size': 64, 'gamma': 0.9022246990831547, 'gae_lambda': 0.9001327833099921, 'clip_range': 0.3942726054814004, 'n_epochs': 7}. Best is trial 10 with value: -7040.8620511.\n",
      "[I 2025-08-26 12:08:31,820] Trial 12 finished with value: -7202.269791100001 and parameters: {'learning_rate': 0.00037749893798435103, 'n_steps': 512, 'batch_size': 64, 'gamma': 0.9013174145550108, 'gae_lambda': 0.9001061464675907, 'clip_range': 0.3993308306428285, 'n_epochs': 5}. Best is trial 10 with value: -7040.8620511.\n",
      "[I 2025-08-26 12:08:50,675] Trial 13 finished with value: -6984.079571200001 and parameters: {'learning_rate': 0.0003576729952374777, 'n_steps': 512, 'batch_size': 64, 'gamma': 0.9012867736379573, 'gae_lambda': 0.9162514698519537, 'clip_range': 0.3977449967534895, 'n_epochs': 5}. Best is trial 13 with value: -6984.079571200001.\n",
      "[I 2025-08-26 12:09:09,487] Trial 14 finished with value: -7115.106333000001 and parameters: {'learning_rate': 0.00036160156318195547, 'n_steps': 512, 'batch_size': 128, 'gamma': 0.917923522743831, 'gae_lambda': 0.9235941737893779, 'clip_range': 0.3522888173530416, 'n_epochs': 8}. Best is trial 13 with value: -6984.079571200001.\n",
      "[I 2025-08-26 12:09:29,225] Trial 15 finished with value: -7693.732937399999 and parameters: {'learning_rate': 6.144375376866837e-05, 'n_steps': 512, 'batch_size': 64, 'gamma': 0.917559014141634, 'gae_lambda': 0.9143277126466383, 'clip_range': 0.3358978644307327, 'n_epochs': 5}. Best is trial 13 with value: -6984.079571200001.\n",
      "[I 2025-08-26 12:09:48,314] Trial 16 finished with value: -7813.418759599999 and parameters: {'learning_rate': 0.00026809566604470034, 'n_steps': 512, 'batch_size': 64, 'gamma': 0.9348183215162504, 'gae_lambda': 0.9591222568705962, 'clip_range': 0.36127725809341954, 'n_epochs': 8}. Best is trial 13 with value: -6984.079571200001.\n",
      "[I 2025-08-26 12:10:07,682] Trial 17 finished with value: -7194.8983108 and parameters: {'learning_rate': 0.0005286994609060696, 'n_steps': 512, 'batch_size': 64, 'gamma': 0.9066433972701133, 'gae_lambda': 0.9138765840700707, 'clip_range': 0.28734902235578663, 'n_epochs': 7}. Best is trial 13 with value: -6984.079571200001.\n",
      "[I 2025-08-26 12:10:26,205] Trial 18 finished with value: -8616.312911500001 and parameters: {'learning_rate': 0.0005751360974591736, 'n_steps': 512, 'batch_size': 128, 'gamma': 0.9329661013682318, 'gae_lambda': 0.9533519793192538, 'clip_range': 0.3016715298094592, 'n_epochs': 9}. Best is trial 13 with value: -6984.079571200001.\n",
      "[I 2025-08-26 12:10:41,587] Trial 19 finished with value: -7058.2282615 and parameters: {'learning_rate': 0.00030034734263957645, 'n_steps': 512, 'batch_size': 256, 'gamma': 0.9142241850706323, 'gae_lambda': 0.9401633023443584, 'clip_range': 0.3717194914814258, 'n_epochs': 5}. Best is trial 13 with value: -6984.079571200001.\n",
      "[I 2025-08-26 12:11:05,374] Trial 20 finished with value: -8055.354910700002 and parameters: {'learning_rate': 9.844712150110298e-05, 'n_steps': 512, 'batch_size': 64, 'gamma': 0.962637982268416, 'gae_lambda': 0.9237345483277993, 'clip_range': 0.31702474624514987, 'n_epochs': 10}. Best is trial 13 with value: -6984.079571200001.\n",
      "[I 2025-08-26 12:11:20,770] Trial 21 finished with value: -8169.1109876 and parameters: {'learning_rate': 0.00023298023263160596, 'n_steps': 512, 'batch_size': 256, 'gamma': 0.9126227138766176, 'gae_lambda': 0.944648162913509, 'clip_range': 0.3703442606612866, 'n_epochs': 5}. Best is trial 13 with value: -6984.079571200001.\n",
      "[I 2025-08-26 12:11:36,897] Trial 22 finished with value: -7941.1286986999985 and parameters: {'learning_rate': 0.00030301154083760815, 'n_steps': 512, 'batch_size': 256, 'gamma': 0.9122593620424476, 'gae_lambda': 0.9103175007460061, 'clip_range': 0.3835459290011822, 'n_epochs': 6}. Best is trial 13 with value: -6984.079571200001.\n",
      "[I 2025-08-26 12:11:53,396] Trial 23 finished with value: -7018.411126599999 and parameters: {'learning_rate': 0.0009969532451184764, 'n_steps': 512, 'batch_size': 256, 'gamma': 0.9250353313447005, 'gae_lambda': 0.9240748638252244, 'clip_range': 0.3500953141832236, 'n_epochs': 7}. Best is trial 13 with value: -6984.079571200001.\n",
      "[I 2025-08-26 12:12:09,646] Trial 24 finished with value: -7822.141378400001 and parameters: {'learning_rate': 0.0006097740914723775, 'n_steps': 512, 'batch_size': 256, 'gamma': 0.9271600219704514, 'gae_lambda': 0.9217414432315726, 'clip_range': 0.3448838437401294, 'n_epochs': 7}. Best is trial 13 with value: -6984.079571200001.\n",
      "[I 2025-08-26 12:12:30,751] Trial 25 finished with value: -8993.068674400001 and parameters: {'learning_rate': 0.0009365873570380464, 'n_steps': 512, 'batch_size': 64, 'gamma': 0.9409752897913267, 'gae_lambda': 0.928904448360099, 'clip_range': 0.25314462717679603, 'n_epochs': 9}. Best is trial 13 with value: -6984.079571200001.\n",
      "[I 2025-08-26 12:12:45,086] Trial 26 finished with value: -7116.5920011 and parameters: {'learning_rate': 0.0005247910696982221, 'n_steps': 512, 'batch_size': 256, 'gamma': 0.9248313013468303, 'gae_lambda': 0.9097647366546404, 'clip_range': 0.3970544908172474, 'n_epochs': 6}. Best is trial 13 with value: -6984.079571200001.\n",
      "[I 2025-08-26 12:13:01,819] Trial 27 finished with value: -8165.874354499999 and parameters: {'learning_rate': 0.000996858120354128, 'n_steps': 1024, 'batch_size': 128, 'gamma': 0.9001155460163384, 'gae_lambda': 0.9152955920104289, 'clip_range': 0.31585742948582946, 'n_epochs': 6}. Best is trial 13 with value: -6984.079571200001.\n",
      "[I 2025-08-26 12:13:24,816] Trial 28 finished with value: -9302.392572 and parameters: {'learning_rate': 0.0006459384064652935, 'n_steps': 4096, 'batch_size': 64, 'gamma': 0.920587087187689, 'gae_lambda': 0.906847056013005, 'clip_range': 0.3718465487323166, 'n_epochs': 8}. Best is trial 13 with value: -6984.079571200001.\n",
      "[I 2025-08-26 12:13:43,430] Trial 29 finished with value: -9140.950113 and parameters: {'learning_rate': 0.0001456072452775953, 'n_steps': 4096, 'batch_size': 512, 'gamma': 0.9070093751551767, 'gae_lambda': 0.9522306842858288, 'clip_range': 0.35033532606925266, 'n_epochs': 14}. Best is trial 13 with value: -6984.079571200001.\n",
      "[I 2025-08-26 12:14:00,442] Trial 30 finished with value: -7077.522669099999 and parameters: {'learning_rate': 0.00045823256933195234, 'n_steps': 512, 'batch_size': 256, 'gamma': 0.907207491997003, 'gae_lambda': 0.9352245014068732, 'clip_range': 0.2709528343487259, 'n_epochs': 9}. Best is trial 13 with value: -6984.079571200001.\n",
      "[I 2025-08-26 12:14:16,779] Trial 31 finished with value: -6994.274083300001 and parameters: {'learning_rate': 0.000347586871768132, 'n_steps': 512, 'batch_size': 256, 'gamma': 0.9129469854108995, 'gae_lambda': 0.9400668239260977, 'clip_range': 0.37348149982470186, 'n_epochs': 5}. Best is trial 13 with value: -6984.079571200001.\n",
      "[I 2025-08-26 12:14:32,242] Trial 32 finished with value: -7093.120588700001 and parameters: {'learning_rate': 0.0007389100034473108, 'n_steps': 512, 'batch_size': 256, 'gamma': 0.9092156826378996, 'gae_lambda': 0.9261643131528435, 'clip_range': 0.3832841079886429, 'n_epochs': 6}. Best is trial 13 with value: -6984.079571200001.\n",
      "[I 2025-08-26 12:14:47,686] Trial 33 finished with value: -8043.4903544 and parameters: {'learning_rate': 0.00011049806543163179, 'n_steps': 512, 'batch_size': 256, 'gamma': 0.9324940291193776, 'gae_lambda': 0.9173839163420523, 'clip_range': 0.32429622739313807, 'n_epochs': 7}. Best is trial 13 with value: -6984.079571200001.\n",
      "[I 2025-08-26 12:15:02,533] Trial 34 finished with value: -9040.5250236 and parameters: {'learning_rate': 6.121466574127085e-05, 'n_steps': 512, 'batch_size': 256, 'gamma': 0.9217964320090718, 'gae_lambda': 0.9492416689179842, 'clip_range': 0.20965372678129954, 'n_epochs': 5}. Best is trial 13 with value: -6984.079571200001.\n",
      "[I 2025-08-26 12:15:21,356] Trial 35 finished with value: -13220.643540800002 and parameters: {'learning_rate': 0.000207351942574028, 'n_steps': 2048, 'batch_size': 64, 'gamma': 0.9054226366182542, 'gae_lambda': 0.9409651549388917, 'clip_range': 0.3768609449148693, 'n_epochs': 6}. Best is trial 13 with value: -6984.079571200001.\n",
      "[I 2025-08-26 12:15:36,317] Trial 36 finished with value: -8005.233054900001 and parameters: {'learning_rate': 0.0007010729432123521, 'n_steps': 512, 'batch_size': 512, 'gamma': 0.9146223065380952, 'gae_lambda': 0.9701433081996502, 'clip_range': 0.34688817578287706, 'n_epochs': 5}. Best is trial 13 with value: -6984.079571200001.\n",
      "[I 2025-08-26 12:15:54,067] Trial 37 finished with value: -7932.235977600001 and parameters: {'learning_rate': 0.00037468105600637366, 'n_steps': 1024, 'batch_size': 128, 'gamma': 0.9427359033431157, 'gae_lambda': 0.9097568020673956, 'clip_range': 0.38573508736267864, 'n_epochs': 7}. Best is trial 13 with value: -6984.079571200001.\n",
      "[I 2025-08-26 12:16:15,421] Trial 38 finished with value: -8413.5426839 and parameters: {'learning_rate': 0.00016393486487910848, 'n_steps': 4096, 'batch_size': 256, 'gamma': 0.9592266385031328, 'gae_lambda': 0.9311718160679507, 'clip_range': 0.3585766123663498, 'n_epochs': 11}. Best is trial 13 with value: -6984.079571200001.\n",
      "[I 2025-08-26 12:16:43,560] Trial 39 finished with value: -8991.102507 and parameters: {'learning_rate': 0.0007675970579571239, 'n_steps': 2048, 'batch_size': 64, 'gamma': 0.928635828087242, 'gae_lambda': 0.9350849959803232, 'clip_range': 0.1365333528237206, 'n_epochs': 16}. Best is trial 13 with value: -6984.079571200001.\n",
      "[I 2025-08-26 12:16:59,397] Trial 40 finished with value: -7562.7214219 and parameters: {'learning_rate': 0.0004507913298234171, 'n_steps': 1024, 'batch_size': 512, 'gamma': 0.910218641556696, 'gae_lambda': 0.9201609736615267, 'clip_range': 0.3319587911510463, 'n_epochs': 10}. Best is trial 13 with value: -6984.079571200001.\n",
      "[I 2025-08-26 12:17:16,174] Trial 41 finished with value: -8419.517108799999 and parameters: {'learning_rate': 0.00028600102208999436, 'n_steps': 512, 'batch_size': 256, 'gamma': 0.9154560430883631, 'gae_lambda': 0.941293489972596, 'clip_range': 0.36519469057792175, 'n_epochs': 5}. Best is trial 13 with value: -6984.079571200001.\n",
      "[I 2025-08-26 12:17:31,608] Trial 42 finished with value: -7830.7105931 and parameters: {'learning_rate': 0.0003312910636733124, 'n_steps': 512, 'batch_size': 256, 'gamma': 0.9002423034810132, 'gae_lambda': 0.9381866111028005, 'clip_range': 0.18309845165103972, 'n_epochs': 6}. Best is trial 13 with value: -6984.079571200001.\n",
      "[I 2025-08-26 12:17:47,199] Trial 43 finished with value: -7953.8784132 and parameters: {'learning_rate': 0.00023178865701030523, 'n_steps': 512, 'batch_size': 256, 'gamma': 0.9203874162524678, 'gae_lambda': 0.9875967776279919, 'clip_range': 0.3860082472805113, 'n_epochs': 5}. Best is trial 13 with value: -6984.079571200001.\n",
      "[I 2025-08-26 12:18:02,731] Trial 44 finished with value: -7123.277095400001 and parameters: {'learning_rate': 0.0008583684892127477, 'n_steps': 512, 'batch_size': 256, 'gamma': 0.9044442532250827, 'gae_lambda': 0.9047799462374405, 'clip_range': 0.39968289722848915, 'n_epochs': 8}. Best is trial 13 with value: -6984.079571200001.\n",
      "[I 2025-08-26 12:18:19,164] Trial 45 finished with value: -6977.2433527 and parameters: {'learning_rate': 0.0004620062685465194, 'n_steps': 512, 'batch_size': 256, 'gamma': 0.9114951374562271, 'gae_lambda': 0.9275108690490733, 'clip_range': 0.3708642078285005, 'n_epochs': 6}. Best is trial 45 with value: -6977.2433527.\n",
      "[I 2025-08-26 12:18:38,385] Trial 46 finished with value: -8780.380791600002 and parameters: {'learning_rate': 0.00048289851787821465, 'n_steps': 2048, 'batch_size': 64, 'gamma': 0.9102209139017672, 'gae_lambda': 0.9266659883965659, 'clip_range': 0.2330298504184195, 'n_epochs': 7}. Best is trial 45 with value: -6977.2433527.\n",
      "[I 2025-08-26 12:18:57,831] Trial 47 finished with value: -12059.697605500001 and parameters: {'learning_rate': 0.0006660779005659832, 'n_steps': 512, 'batch_size': 64, 'gamma': 0.9037966735298811, 'gae_lambda': 0.9336464481837053, 'clip_range': 0.35873089795698543, 'n_epochs': 6}. Best is trial 45 with value: -6977.2433527.\n",
      "[I 2025-08-26 12:19:13,936] Trial 48 finished with value: -9293.800860500001 and parameters: {'learning_rate': 1.0988439504486194e-05, 'n_steps': 512, 'batch_size': 256, 'gamma': 0.9240773535359071, 'gae_lambda': 0.9204965654941722, 'clip_range': 0.3386867082047653, 'n_epochs': 8}. Best is trial 45 with value: -6977.2433527.\n",
      "[I 2025-08-26 12:19:35,775] Trial 49 finished with value: -9307.9337946 and parameters: {'learning_rate': 0.00039474890153030716, 'n_steps': 4096, 'batch_size': 128, 'gamma': 0.9175384299647181, 'gae_lambda': 0.9116693401816831, 'clip_range': 0.38446691248293074, 'n_epochs': 13}. Best is trial 45 with value: -6977.2433527.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- PPO Optimization Finished ---\n",
      "Number of finished trials:  50\n",
      "Best trial:\n",
      "  Value (Average Reward):  -6977.2433527\n",
      "  Params: \n",
      "    learning_rate: 0.0004620062685465194\n",
      "    n_steps: 512\n",
      "    batch_size: 256\n",
      "    gamma: 0.9114951374562271\n",
      "    gae_lambda: 0.9275108690490733\n",
      "    clip_range: 0.3708642078285005\n",
      "    n_epochs: 6\n",
      "\n",
      "==================================================\n",
      "\n",
      "--- Training the final PPO model with best hyperparameters ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "412323681ce548ba95535b1a93283343",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='Training final model: <b>PPO</b>'), IntProgress(value=0, description='Training PPO:…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved to: Output/PPO/best_model.pt\n",
      "\n",
      "--- Evaluating the final PPO model ---\n",
      "Evaluation logs saved to: Output/PPO/evaluation_logs.csv\n",
      "\n",
      "Final PPO Metrics (Optimized):\n",
      "Average Reward: -7.5397\n",
      "Average Battery Health: 100.0000\n",
      "Average Efficiency: 6.8092\n",
      "Average Regret: 8.2206\n",
      "Energy Fulfillment Rate: 17.8613\n",
      "Evaluation plots saved to: Output/PPO\n",
      "\n",
      "--- Training benchmark PPO model (default/minimal hyperparameters) ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8c7b89f056b4e4594633bb652e07549",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='Training benchmark model: <b>PPO</b>'), IntProgress(value=0, description='Benchmark…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Data/python_venv/ML_GPU_DL_PyTorch/lib/python3.12/site-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark model saved to: Output/PPO/Benchmark/benchmark_model.pt\n",
      "\n",
      "--- Evaluating benchmark PPO model ---\n",
      "Benchmark evaluation logs saved to: Output/PPO/Benchmark/evaluation_logs.csv\n",
      "\n",
      "Benchmark PPO Metrics:\n",
      "Average Reward: -12.9880\n",
      "Average Battery Health: 94.5504\n",
      "Average Efficiency: 6.8163\n",
      "Average Regret: 13.6696\n",
      "Energy Fulfillment Rate: 5.9212\n",
      "Benchmark plots saved to: Output/PPO/Benchmark\n",
      "Comparison plots saved to: Output/PPO\n",
      "\n",
      "==================================================\n",
      "\n",
      "--- Starting Optuna optimization for A2C with 50 trials ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "107002c527944e9aa6414bdaabf2da7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='Tuning model: <b>A2C</b>, Trial 0/50'), IntProgress(value=0, description='Tuning A2…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-26 12:20:46,102] A new study created in memory with name: A2C_optimization\n",
      "/mnt/c/Data/python_venv/ML_GPU_DL_PyTorch/lib/python3.12/site-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run A2C on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n",
      "[I 2025-08-26 12:21:03,092] Trial 0 finished with value: -9162.6472581 and parameters: {'learning_rate': 0.00023268761784683023, 'n_steps': 20, 'gamma': 0.9714136929048268, 'gae_lambda': 0.9801383779087736, 'ent_coef': 1.2909090953884528e-08, 'vf_coef': 0.16946478278187588}. Best is trial 0 with value: -9162.6472581.\n",
      "[I 2025-08-26 12:21:18,274] Trial 1 finished with value: -10102.768829099998 and parameters: {'learning_rate': 0.0002579622904316706, 'n_steps': 28, 'gamma': 0.9831748697662523, 'gae_lambda': 0.9224398671763733, 'ent_coef': 0.0009615780384582882, 'vf_coef': 0.830158413072062}. Best is trial 0 with value: -9162.6472581.\n",
      "[I 2025-08-26 12:21:35,436] Trial 2 finished with value: -9300.297980399999 and parameters: {'learning_rate': 5.8799274466408637e-05, 'n_steps': 24, 'gamma': 0.9535700363350819, 'gae_lambda': 0.9561674623055172, 'ent_coef': 0.001704093587091412, 'vf_coef': 0.8874494408918024}. Best is trial 0 with value: -9162.6472581.\n",
      "[I 2025-08-26 12:21:51,949] Trial 3 finished with value: -9339.122123799998 and parameters: {'learning_rate': 1.163360550043083e-05, 'n_steps': 28, 'gamma': 0.936582113407049, 'gae_lambda': 0.9286567869364857, 'ent_coef': 0.0008655566431192613, 'vf_coef': 0.3419206125054016}. Best is trial 0 with value: -9162.6472581.\n",
      "[I 2025-08-26 12:22:08,820] Trial 4 finished with value: -8999.286646899998 and parameters: {'learning_rate': 0.00022981680852140703, 'n_steps': 16, 'gamma': 0.9154897989360271, 'gae_lambda': 0.933566574512109, 'ent_coef': 0.006399392284763464, 'vf_coef': 0.6293984229943217}. Best is trial 4 with value: -8999.286646899998.\n",
      "[I 2025-08-26 12:22:26,062] Trial 5 finished with value: -9314.3013168 and parameters: {'learning_rate': 1.0765734324366496e-05, 'n_steps': 11, 'gamma': 0.9761437188517629, 'gae_lambda': 0.9140022651553531, 'ent_coef': 0.003780199252063881, 'vf_coef': 0.5657462232761071}. Best is trial 4 with value: -8999.286646899998.\n",
      "[I 2025-08-26 12:22:43,409] Trial 6 finished with value: -8022.508180300001 and parameters: {'learning_rate': 0.0007063192161665602, 'n_steps': 13, 'gamma': 0.9423421997360252, 'gae_lambda': 0.983198878696133, 'ent_coef': 3.933238705525758e-08, 'vf_coef': 0.8269773514386373}. Best is trial 6 with value: -8022.508180300001.\n",
      "[I 2025-08-26 12:23:00,245] Trial 7 finished with value: -8874.592296600002 and parameters: {'learning_rate': 0.0005426160569657445, 'n_steps': 14, 'gamma': 0.9448540158557219, 'gae_lambda': 0.9147254355190573, 'ent_coef': 0.00015966287572675536, 'vf_coef': 0.9438754351618298}. Best is trial 6 with value: -8022.508180300001.\n",
      "[I 2025-08-26 12:23:16,455] Trial 8 finished with value: -12725.784721400001 and parameters: {'learning_rate': 6.706168472326244e-05, 'n_steps': 23, 'gamma': 0.9765696997707822, 'gae_lambda': 0.9049891273089208, 'ent_coef': 0.09903950615810428, 'vf_coef': 0.7987765661057981}. Best is trial 6 with value: -8022.508180300001.\n",
      "[I 2025-08-26 12:23:33,538] Trial 9 finished with value: -13635.263797099999 and parameters: {'learning_rate': 0.000819448614894525, 'n_steps': 16, 'gamma': 0.9163569063141163, 'gae_lambda': 0.9850182509181837, 'ent_coef': 0.02170349213170436, 'vf_coef': 0.5689229879563784}. Best is trial 6 with value: -8022.508180300001.\n",
      "[I 2025-08-26 12:23:50,007] Trial 10 finished with value: -9276.3744968 and parameters: {'learning_rate': 3.1435618321802064e-05, 'n_steps': 42, 'gamma': 0.9962367188309571, 'gae_lambda': 0.9969914436045801, 'ent_coef': 2.5406795636159333e-07, 'vf_coef': 0.7101662377905131}. Best is trial 6 with value: -8022.508180300001.\n",
      "[I 2025-08-26 12:24:10,829] Trial 11 finished with value: -7059.7640827 and parameters: {'learning_rate': 0.0009884713217409198, 'n_steps': 7, 'gamma': 0.9416861160225107, 'gae_lambda': 0.9531385980263934, 'ent_coef': 6.5078829141555776e-06, 'vf_coef': 0.9498729474106143}. Best is trial 11 with value: -7059.7640827.\n",
      "[I 2025-08-26 12:24:30,308] Trial 12 finished with value: -7115.1008222 and parameters: {'learning_rate': 0.0008498066384377053, 'n_steps': 6, 'gamma': 0.937158508828949, 'gae_lambda': 0.9577925127348784, 'ent_coef': 4.776114867902333e-06, 'vf_coef': 0.9550597094138864}. Best is trial 11 with value: -7059.7640827.\n",
      "[I 2025-08-26 12:24:51,601] Trial 13 finished with value: -6981.1436487 and parameters: {'learning_rate': 0.0004025215959739869, 'n_steps': 5, 'gamma': 0.9010001523980229, 'gae_lambda': 0.9525188128604903, 'ent_coef': 4.378987999549591e-06, 'vf_coef': 0.9936629440738918}. Best is trial 13 with value: -6981.1436487.\n",
      "[I 2025-08-26 12:25:11,115] Trial 14 finished with value: -13101.6836299 and parameters: {'learning_rate': 0.0003688965017892292, 'n_steps': 7, 'gamma': 0.902615128368163, 'gae_lambda': 0.9422933217606351, 'ent_coef': 3.2631940408800506e-06, 'vf_coef': 0.9977699639227292}. Best is trial 13 with value: -6981.1436487.\n",
      "[I 2025-08-26 12:25:26,082] Trial 15 finished with value: -9198.8182853 and parameters: {'learning_rate': 0.00012339644046482657, 'n_steps': 39, 'gamma': 0.923173738013139, 'gae_lambda': 0.9655881595752889, 'ent_coef': 1.1936301633711017e-05, 'vf_coef': 0.3772582604168071}. Best is trial 13 with value: -6981.1436487.\n",
      "[I 2025-08-26 12:25:41,483] Trial 16 finished with value: -9182.7516076 and parameters: {'learning_rate': 0.000424461842963431, 'n_steps': 50, 'gamma': 0.9616615579585266, 'gae_lambda': 0.9441246886400362, 'ent_coef': 6.059959170398407e-07, 'vf_coef': 0.7097908510214833}. Best is trial 13 with value: -6981.1436487.\n",
      "[I 2025-08-26 12:26:01,304] Trial 17 finished with value: -9079.2590851 and parameters: {'learning_rate': 0.0001150333594861438, 'n_steps': 8, 'gamma': 0.9008767219805115, 'gae_lambda': 0.9688279599976682, 'ent_coef': 9.893455886246019e-05, 'vf_coef': 0.43819690115878585}. Best is trial 13 with value: -6981.1436487.\n",
      "[I 2025-08-26 12:26:23,272] Trial 18 finished with value: -7092.115879300001 and parameters: {'learning_rate': 0.0009777377999388279, 'n_steps': 5, 'gamma': 0.9291864855456834, 'gae_lambda': 0.9494811532407775, 'ent_coef': 6.404602392069724e-07, 'vf_coef': 0.7373724274524742}. Best is trial 13 with value: -6981.1436487.\n",
      "[I 2025-08-26 12:26:39,757] Trial 19 finished with value: -10889.5567668 and parameters: {'learning_rate': 0.00035228774720783164, 'n_steps': 20, 'gamma': 0.9566011386037511, 'gae_lambda': 0.9717110729075555, 'ent_coef': 4.73145815627814e-05, 'vf_coef': 0.12286592861762302}. Best is trial 13 with value: -6981.1436487.\n",
      "[I 2025-08-26 12:26:54,928] Trial 20 finished with value: -9254.8961707 and parameters: {'learning_rate': 0.0001636114182289253, 'n_steps': 33, 'gamma': 0.9146069012304415, 'gae_lambda': 0.9375913198725893, 'ent_coef': 2.3924904335102796e-06, 'vf_coef': 0.8994022023471093}. Best is trial 13 with value: -6981.1436487.\n",
      "[I 2025-08-26 12:27:17,160] Trial 21 finished with value: -6988.7713047 and parameters: {'learning_rate': 0.0009846865243058724, 'n_steps': 5, 'gamma': 0.9281843048175145, 'gae_lambda': 0.9511025810433463, 'ent_coef': 5.065293220304952e-07, 'vf_coef': 0.7335584941687723}. Best is trial 13 with value: -6981.1436487.\n",
      "[I 2025-08-26 12:27:34,560] Trial 22 finished with value: -7089.1851707000005 and parameters: {'learning_rate': 0.0005373447325871273, 'n_steps': 10, 'gamma': 0.9288790085796299, 'gae_lambda': 0.9539244567310857, 'ent_coef': 1.5470189708053072e-05, 'vf_coef': 0.9992432061401451}. Best is trial 13 with value: -6981.1436487.\n",
      "[I 2025-08-26 12:27:52,489] Trial 23 finished with value: -6996.613232799999 and parameters: {'learning_rate': 0.0005911429824241504, 'n_steps': 10, 'gamma': 0.9066109684278835, 'gae_lambda': 0.9625099093824332, 'ent_coef': 1.443261072482652e-07, 'vf_coef': 0.8762020628236606}. Best is trial 13 with value: -6981.1436487.\n",
      "[I 2025-08-26 12:28:09,768] Trial 24 finished with value: -8370.258238400002 and parameters: {'learning_rate': 0.0005726350228849751, 'n_steps': 11, 'gamma': 0.9032774679266043, 'gae_lambda': 0.9627474075936431, 'ent_coef': 9.820154199125198e-08, 'vf_coef': 0.7711919895544894}. Best is trial 13 with value: -6981.1436487.\n",
      "[I 2025-08-26 12:28:31,424] Trial 25 finished with value: -8899.5667531 and parameters: {'learning_rate': 0.0003123382256578978, 'n_steps': 5, 'gamma': 0.9103996240102799, 'gae_lambda': 0.9729403948604145, 'ent_coef': 8.201084478261608e-07, 'vf_coef': 0.6531636884861274}. Best is trial 13 with value: -6981.1436487.\n",
      "[I 2025-08-26 12:28:49,156] Trial 26 finished with value: -14121.6855515 and parameters: {'learning_rate': 0.0005120755830284803, 'n_steps': 17, 'gamma': 0.9231609900355716, 'gae_lambda': 0.9449797809506133, 'ent_coef': 1.666321784727676e-07, 'vf_coef': 0.8722207798620912}. Best is trial 13 with value: -6981.1436487.\n",
      "[I 2025-08-26 12:29:07,980] Trial 27 finished with value: -9289.698177 and parameters: {'learning_rate': 0.0001664703337539506, 'n_steps': 10, 'gamma': 0.908733597718421, 'gae_lambda': 0.9595949389823039, 'ent_coef': 3.855573359366489e-08, 'vf_coef': 0.6518205512679243}. Best is trial 13 with value: -6981.1436487.\n",
      "[I 2025-08-26 12:29:26,081] Trial 28 finished with value: -13123.775722700002 and parameters: {'learning_rate': 0.0007143222512370031, 'n_steps': 13, 'gamma': 0.9233255820593538, 'gae_lambda': 0.9776417588765387, 'ent_coef': 1.155852593200288e-06, 'vf_coef': 0.8578105777720357}. Best is trial 13 with value: -6981.1436487.\n",
      "[I 2025-08-26 12:29:42,179] Trial 29 finished with value: -9308.536875900001 and parameters: {'learning_rate': 0.00020167383669611919, 'n_steps': 20, 'gamma': 0.9076820251372077, 'gae_lambda': 0.9895651682305182, 'ent_coef': 1.2602461191217033e-08, 'vf_coef': 0.4500136167800225}. Best is trial 13 with value: -6981.1436487.\n",
      "[I 2025-08-26 12:29:58,572] Trial 30 finished with value: -12317.8531787 and parameters: {'learning_rate': 8.031554868141261e-05, 'n_steps': 33, 'gamma': 0.9304176364012756, 'gae_lambda': 0.9287428056471668, 'ent_coef': 2.8981696557440966e-07, 'vf_coef': 0.25362921014555306}. Best is trial 13 with value: -6981.1436487.\n",
      "[I 2025-08-26 12:30:16,746] Trial 31 finished with value: -9350.7048966 and parameters: {'learning_rate': 0.0009859575699073012, 'n_steps': 8, 'gamma': 0.9483549156028254, 'gae_lambda': 0.9505042016589469, 'ent_coef': 8.156623361734084e-06, 'vf_coef': 0.9282595501021077}. Best is trial 13 with value: -6981.1436487.\n",
      "[I 2025-08-26 12:30:37,394] Trial 32 finished with value: -14053.013533700001 and parameters: {'learning_rate': 0.0006533298182566783, 'n_steps': 5, 'gamma': 0.9641857502325698, 'gae_lambda': 0.9490766851225974, 'ent_coef': 4.389732893333325e-08, 'vf_coef': 0.800730538172834}. Best is trial 13 with value: -6981.1436487.\n",
      "[I 2025-08-26 12:30:56,098] Trial 33 finished with value: -7785.3077255 and parameters: {'learning_rate': 0.0002898508830496412, 'n_steps': 9, 'gamma': 0.9366163277776238, 'gae_lambda': 0.9617177836074524, 'ent_coef': 4.048554246479797e-05, 'vf_coef': 0.9225881693592605}. Best is trial 13 with value: -6981.1436487.\n",
      "[I 2025-08-26 12:31:13,092] Trial 34 finished with value: -7538.3784938 and parameters: {'learning_rate': 0.0004203546691056484, 'n_steps': 13, 'gamma': 0.9199617788179256, 'gae_lambda': 0.9395165258220919, 'ent_coef': 1.8108788561427234e-06, 'vf_coef': 0.8517203817334427}. Best is trial 13 with value: -6981.1436487.\n",
      "[I 2025-08-26 12:31:32,820] Trial 35 finished with value: -9236.8251815 and parameters: {'learning_rate': 2.2130959758208857e-05, 'n_steps': 8, 'gamma': 0.9103383531764442, 'gae_lambda': 0.954328799717054, 'ent_coef': 7.331113029436623e-08, 'vf_coef': 0.9699419517946246}. Best is trial 13 with value: -6981.1436487.\n",
      "[I 2025-08-26 12:31:48,020] Trial 36 finished with value: -13062.5205513 and parameters: {'learning_rate': 0.0009905086706384673, 'n_steps': 18, 'gamma': 0.9413191004427174, 'gae_lambda': 0.9347537542547049, 'ent_coef': 0.0003188709049971651, 'vf_coef': 0.8862368327982878}. Best is trial 13 with value: -6981.1436487.\n",
      "[I 2025-08-26 12:32:05,947] Trial 37 finished with value: -13255.955907800002 and parameters: {'learning_rate': 0.0004805023373612288, 'n_steps': 11, 'gamma': 0.9523854732670641, 'gae_lambda': 0.9769699880940438, 'ent_coef': 4.6491962779704374e-06, 'vf_coef': 0.7606335912200389}. Best is trial 13 with value: -6981.1436487.\n",
      "[I 2025-08-26 12:32:21,424] Trial 38 finished with value: -9309.983476899999 and parameters: {'learning_rate': 0.0007074557494971741, 'n_steps': 26, 'gamma': 0.9310243661220263, 'gae_lambda': 0.9656379868704756, 'ent_coef': 2.1255836517685876e-05, 'vf_coef': 0.8239058348364879}. Best is trial 13 with value: -6981.1436487.\n",
      "[I 2025-08-26 12:32:38,356] Trial 39 finished with value: -13444.6329812 and parameters: {'learning_rate': 0.00023323406274528504, 'n_steps': 14, 'gamma': 0.900071882690202, 'gae_lambda': 0.9554988443991875, 'ent_coef': 2.6224068408188e-07, 'vf_coef': 0.9052168043443859}. Best is trial 13 with value: -6981.1436487.\n",
      "[I 2025-08-26 12:32:58,898] Trial 40 finished with value: -6979.476916199999 and parameters: {'learning_rate': 0.0006597159874954087, 'n_steps': 7, 'gamma': 0.9160520109815136, 'gae_lambda': 0.9270021867568593, 'ent_coef': 2.769125701434046e-08, 'vf_coef': 0.5136202291429388}. Best is trial 40 with value: -6979.476916199999.\n",
      "[I 2025-08-26 12:33:18,419] Trial 41 finished with value: -7095.355323000001 and parameters: {'learning_rate': 0.000674238139767173, 'n_steps': 7, 'gamma': 0.9147677458060479, 'gae_lambda': 0.9222288905802816, 'ent_coef': 1.6237316010666272e-08, 'vf_coef': 0.5363847314016186}. Best is trial 40 with value: -6979.476916199999.\n",
      "[I 2025-08-26 12:33:36,989] Trial 42 finished with value: -7128.5112594 and parameters: {'learning_rate': 0.0007922661517039901, 'n_steps': 11, 'gamma': 0.9189617221275227, 'gae_lambda': 0.9010034738906846, 'ent_coef': 1.2837594248585194e-07, 'vf_coef': 0.5029128609846238}. Best is trial 40 with value: -6979.476916199999.\n",
      "[I 2025-08-26 12:33:58,157] Trial 43 finished with value: -7031.6463005 and parameters: {'learning_rate': 0.0005946843529910001, 'n_steps': 5, 'gamma': 0.9050678799732965, 'gae_lambda': 0.9310547226534245, 'ent_coef': 2.232604004211627e-08, 'vf_coef': 0.627204388071045}. Best is trial 40 with value: -6979.476916199999.\n",
      "[I 2025-08-26 12:34:19,438] Trial 44 finished with value: -7180.817579199999 and parameters: {'learning_rate': 0.00044501019632226256, 'n_steps': 5, 'gamma': 0.9062166873921654, 'gae_lambda': 0.9225292274118786, 'ent_coef': 2.402884294941637e-08, 'vf_coef': 0.6261849723265862}. Best is trial 40 with value: -6979.476916199999.\n",
      "[I 2025-08-26 12:34:37,198] Trial 45 finished with value: -7054.692546299999 and parameters: {'learning_rate': 0.0006075720011269182, 'n_steps': 15, 'gamma': 0.9065396793511962, 'gae_lambda': 0.9123467103482088, 'ent_coef': 6.40509215337362e-08, 'vf_coef': 0.6174612155962108}. Best is trial 40 with value: -6979.476916199999.\n",
      "[I 2025-08-26 12:34:54,626] Trial 46 finished with value: -11473.9831556 and parameters: {'learning_rate': 0.0003540305080534029, 'n_steps': 12, 'gamma': 0.9148864192625175, 'gae_lambda': 0.9295555712714737, 'ent_coef': 1.1208217554465032e-08, 'vf_coef': 0.6916978639015167}. Best is trial 40 with value: -6979.476916199999.\n",
      "[I 2025-08-26 12:35:13,355] Trial 47 finished with value: -9292.2769337 and parameters: {'learning_rate': 0.0002811266744968718, 'n_steps': 9, 'gamma': 0.9122619531476343, 'gae_lambda': 0.9175886085434926, 'ent_coef': 3.8041324702089437e-07, 'vf_coef': 0.5701549193667238}. Best is trial 40 with value: -6979.476916199999.\n",
      "[I 2025-08-26 12:35:29,221] Trial 48 finished with value: -9107.9727736 and parameters: {'learning_rate': 0.000809228814392218, 'n_steps': 23, 'gamma': 0.9038066107179569, 'gae_lambda': 0.9315644762655142, 'ent_coef': 2.7440093543271735e-08, 'vf_coef': 0.5137539454379969}. Best is trial 40 with value: -6979.476916199999.\n",
      "[I 2025-08-26 12:35:49,595] Trial 49 finished with value: -7172.6205533 and parameters: {'learning_rate': 0.000582372553726019, 'n_steps': 7, 'gamma': 0.9892050990351523, 'gae_lambda': 0.924534429773748, 'ent_coef': 1.0371818843301552e-07, 'vf_coef': 0.5856193438711571}. Best is trial 40 with value: -6979.476916199999.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- A2C Optimization Finished ---\n",
      "Number of finished trials:  50\n",
      "Best trial:\n",
      "  Value (Average Reward):  -6979.476916199999\n",
      "  Params: \n",
      "    learning_rate: 0.0006597159874954087\n",
      "    n_steps: 7\n",
      "    gamma: 0.9160520109815136\n",
      "    gae_lambda: 0.9270021867568593\n",
      "    ent_coef: 2.769125701434046e-08\n",
      "    vf_coef: 0.5136202291429388\n",
      "\n",
      "==================================================\n",
      "\n",
      "--- Training the final A2C model with best hyperparameters ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1d1b2c4759d4f6f99f82a178529a1e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='Training final model: <b>A2C</b>'), IntProgress(value=0, description='Training A2C:…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved to: Output/A2C/best_model.pt\n",
      "\n",
      "--- Evaluating the final A2C model ---\n",
      "Evaluation logs saved to: Output/A2C/evaluation_logs.csv\n",
      "\n",
      "Final A2C Metrics (Optimized):\n",
      "Average Reward: -7.1947\n",
      "Average Battery Health: 100.0000\n",
      "Average Efficiency: 6.8221\n",
      "Average Regret: 7.8769\n",
      "Energy Fulfillment Rate: 21.2051\n",
      "Evaluation plots saved to: Output/A2C\n",
      "\n",
      "--- Training benchmark A2C model (default/minimal hyperparameters) ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "723098e3c380422b842c50ab06adfcc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='Training benchmark model: <b>A2C</b>'), IntProgress(value=0, description='Benchmark…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Data/python_venv/ML_GPU_DL_PyTorch/lib/python3.12/site-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run A2C on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark model saved to: Output/A2C/Benchmark/benchmark_model.pt\n",
      "\n",
      "--- Evaluating benchmark A2C model ---\n",
      "Benchmark evaluation logs saved to: Output/A2C/Benchmark/evaluation_logs.csv\n",
      "\n",
      "Benchmark A2C Metrics:\n",
      "Average Reward: -14.0908\n",
      "Average Battery Health: 78.7854\n",
      "Average Efficiency: 6.8185\n",
      "Average Regret: 14.7726\n",
      "Energy Fulfillment Rate: 0.0000\n",
      "Benchmark plots saved to: Output/A2C/Benchmark\n",
      "Comparison plots saved to: Output/A2C\n",
      "\n",
      "==================================================\n",
      "\n",
      "--- Starting Optuna optimization for DDPG with 50 trials ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a17ae28a9e2d49c98eaebbcff9947608",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='Tuning model: <b>DDPG</b>, Trial 0/50'), IntProgress(value=0, description='Tuning D…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-26 12:37:01,601] A new study created in memory with name: DDPG_optimization\n",
      "[I 2025-08-26 12:37:48,688] Trial 0 finished with value: -7119.591135099999 and parameters: {'learning_rate': 0.0001008892452199044, 'buffer_size': 52635, 'learning_starts': 571, 'tau': 0.009770893865664355, 'gamma': 0.9325563751424523}. Best is trial 0 with value: -7119.591135099999.\n",
      "[I 2025-08-26 12:38:32,783] Trial 1 finished with value: -7023.557011 and parameters: {'learning_rate': 4.3695715606983177e-05, 'buffer_size': 15348, 'learning_starts': 676, 'tau': 0.004705931744096292, 'gamma': 0.9793005205246141}. Best is trial 1 with value: -7023.557011.\n",
      "[I 2025-08-26 12:39:18,401] Trial 2 finished with value: -14165.7258454 and parameters: {'learning_rate': 0.0009209760915027166, 'buffer_size': 37755, 'learning_starts': 333, 'tau': 0.009255658064327914, 'gamma': 0.9859772125523548}. Best is trial 1 with value: -7023.557011.\n",
      "[I 2025-08-26 12:40:01,800] Trial 3 finished with value: -7139.4182978 and parameters: {'learning_rate': 1.1219165887771043e-05, 'buffer_size': 26441, 'learning_starts': 846, 'tau': 0.009341765193305429, 'gamma': 0.916352304472803}. Best is trial 1 with value: -7023.557011.\n",
      "[I 2025-08-26 12:40:47,143] Trial 4 finished with value: -7091.532209200001 and parameters: {'learning_rate': 0.0002718748950811798, 'buffer_size': 77890, 'learning_starts': 176, 'tau': 0.006489760770843906, 'gamma': 0.9662170941551234}. Best is trial 1 with value: -7023.557011.\n",
      "[I 2025-08-26 12:41:43,061] Trial 5 finished with value: -8055.0966885 and parameters: {'learning_rate': 1.3436609468934954e-05, 'buffer_size': 34642, 'learning_starts': 102, 'tau': 0.007418522447059496, 'gamma': 0.9279984439409188}. Best is trial 1 with value: -7023.557011.\n",
      "[I 2025-08-26 12:42:36,551] Trial 6 finished with value: -9282.2549002 and parameters: {'learning_rate': 0.00034142657559167115, 'buffer_size': 38400, 'learning_starts': 178, 'tau': 0.009929539879919447, 'gamma': 0.9340049045910834}. Best is trial 1 with value: -7023.557011.\n",
      "[I 2025-08-26 12:43:26,417] Trial 7 finished with value: -6989.269995000001 and parameters: {'learning_rate': 7.8043464732067e-05, 'buffer_size': 48461, 'learning_starts': 182, 'tau': 0.0033621736560185927, 'gamma': 0.9017727077246802}. Best is trial 7 with value: -6989.269995000001.\n",
      "[I 2025-08-26 12:44:15,908] Trial 8 finished with value: -9304.9480534 and parameters: {'learning_rate': 0.0007026492914004686, 'buffer_size': 46391, 'learning_starts': 483, 'tau': 0.009650121404105084, 'gamma': 0.9145810541239927}. Best is trial 7 with value: -6989.269995000001.\n",
      "[I 2025-08-26 12:45:00,574] Trial 9 finished with value: -7079.9775735 and parameters: {'learning_rate': 7.214788983584543e-05, 'buffer_size': 23549, 'learning_starts': 794, 'tau': 0.004279735084800262, 'gamma': 0.9294978246383335}. Best is trial 7 with value: -6989.269995000001.\n",
      "[I 2025-08-26 12:45:42,281] Trial 10 finished with value: -7042.1006640000005 and parameters: {'learning_rate': 3.4859877643569886e-05, 'buffer_size': 78121, 'learning_starts': 991, 'tau': 0.001584417247706633, 'gamma': 0.9018638181075722}. Best is trial 7 with value: -6989.269995000001.\n",
      "[I 2025-08-26 12:46:27,705] Trial 11 finished with value: -7014.7370144999995 and parameters: {'learning_rate': 4.532626320857581e-05, 'buffer_size': 10732, 'learning_starts': 608, 'tau': 0.00383419402199039, 'gamma': 0.9989124569998153}. Best is trial 7 with value: -6989.269995000001.\n",
      "[I 2025-08-26 12:47:15,551] Trial 12 finished with value: -9325.8891948 and parameters: {'learning_rate': 0.0001243479139031902, 'buffer_size': 94045, 'learning_starts': 394, 'tau': 0.0027572618243195636, 'gamma': 0.9574569760203426}. Best is trial 7 with value: -6989.269995000001.\n",
      "[I 2025-08-26 12:47:57,144] Trial 13 finished with value: -7177.722540699999 and parameters: {'learning_rate': 2.4016796600743684e-05, 'buffer_size': 66520, 'learning_starts': 601, 'tau': 0.002856968667457616, 'gamma': 0.9503881007013888}. Best is trial 7 with value: -6989.269995000001.\n",
      "[I 2025-08-26 12:48:45,997] Trial 14 finished with value: -13041.235284800001 and parameters: {'learning_rate': 0.0002123490258641119, 'buffer_size': 12366, 'learning_starts': 326, 'tau': 0.0032638514350255424, 'gamma': 0.9967980498809765}. Best is trial 7 with value: -6989.269995000001.\n",
      "[I 2025-08-26 12:49:31,999] Trial 15 finished with value: -7076.9868556 and parameters: {'learning_rate': 5.6406368584578e-05, 'buffer_size': 60658, 'learning_starts': 447, 'tau': 0.0014033404432730427, 'gamma': 0.9767900780058221}. Best is trial 7 with value: -6989.269995000001.\n",
      "[I 2025-08-26 12:50:16,142] Trial 16 finished with value: -7178.147443299999 and parameters: {'learning_rate': 2.6078000999534585e-05, 'buffer_size': 98855, 'learning_starts': 710, 'tau': 0.005738481560238526, 'gamma': 0.9986579090640124}. Best is trial 7 with value: -6989.269995000001.\n",
      "[I 2025-08-26 12:51:05,820] Trial 17 finished with value: -12969.7208088 and parameters: {'learning_rate': 0.00016756510087209098, 'buffer_size': 69149, 'learning_starts': 273, 'tau': 0.004103800197603491, 'gamma': 0.9449338759712036}. Best is trial 7 with value: -6989.269995000001.\n",
      "[I 2025-08-26 12:51:49,432] Trial 18 finished with value: -9331.914250200001 and parameters: {'learning_rate': 7.302674489778656e-05, 'buffer_size': 24243, 'learning_starts': 537, 'tau': 0.005272436611364399, 'gamma': 0.9032056284457395}. Best is trial 7 with value: -6989.269995000001.\n",
      "[I 2025-08-26 12:52:39,176] Trial 19 finished with value: -9305.001258600001 and parameters: {'learning_rate': 2.61408429906865e-05, 'buffer_size': 47468, 'learning_starts': 671, 'tau': 0.002364530479615279, 'gamma': 0.9678515934371728}. Best is trial 7 with value: -6989.269995000001.\n",
      "[I 2025-08-26 12:53:25,590] Trial 20 finished with value: -7076.6747067999995 and parameters: {'learning_rate': 0.00043729602221245236, 'buffer_size': 10080, 'learning_starts': 824, 'tau': 0.0036107383976677256, 'gamma': 0.9432627012313639}. Best is trial 7 with value: -6989.269995000001.\n",
      "[I 2025-08-26 12:54:09,823] Trial 21 finished with value: -7155.164803099999 and parameters: {'learning_rate': 4.3903724193288355e-05, 'buffer_size': 15602, 'learning_starts': 669, 'tau': 0.004891663004531641, 'gamma': 0.9815688671651621}. Best is trial 7 with value: -6989.269995000001.\n",
      "[I 2025-08-26 12:54:57,198] Trial 22 finished with value: -7130.1733663 and parameters: {'learning_rate': 1.6805694629274042e-05, 'buffer_size': 19005, 'learning_starts': 745, 'tau': 0.006705606133232088, 'gamma': 0.9878820086527095}. Best is trial 7 with value: -6989.269995000001.\n",
      "[I 2025-08-26 12:55:40,309] Trial 23 finished with value: -7029.985925299999 and parameters: {'learning_rate': 5.800810999315803e-05, 'buffer_size': 30972, 'learning_starts': 931, 'tau': 0.004279243635852372, 'gamma': 0.9699246548735285}. Best is trial 7 with value: -6989.269995000001.\n",
      "[I 2025-08-26 12:56:26,509] Trial 24 finished with value: -8203.4798312 and parameters: {'learning_rate': 3.58837106276674e-05, 'buffer_size': 19339, 'learning_starts': 642, 'tau': 0.002108605484888005, 'gamma': 0.9951495671767244}. Best is trial 7 with value: -6989.269995000001.\n",
      "[I 2025-08-26 12:57:14,652] Trial 25 finished with value: -8196.8574415 and parameters: {'learning_rate': 0.00010092115597403914, 'buffer_size': 41879, 'learning_starts': 468, 'tau': 0.004868342190542105, 'gamma': 0.9770290208150278}. Best is trial 7 with value: -6989.269995000001.\n",
      "[I 2025-08-26 12:57:57,821] Trial 26 finished with value: -7091.4528147 and parameters: {'learning_rate': 0.0001483682543722849, 'buffer_size': 30284, 'learning_starts': 533, 'tau': 0.0038221079019761907, 'gamma': 0.9593176551673009}. Best is trial 7 with value: -6989.269995000001.\n",
      "[I 2025-08-26 12:58:43,847] Trial 27 finished with value: -7038.541078600001 and parameters: {'learning_rate': 1.902484541414614e-05, 'buffer_size': 10060, 'learning_starts': 754, 'tau': 0.007871245439694719, 'gamma': 0.9897003105139384}. Best is trial 7 with value: -6989.269995000001.\n",
      "[I 2025-08-26 12:59:29,454] Trial 28 finished with value: -7042.245128199999 and parameters: {'learning_rate': 4.6485127716843184e-05, 'buffer_size': 18134, 'learning_starts': 895, 'tau': 0.005951344256633867, 'gamma': 0.9151184695652942}. Best is trial 7 with value: -6989.269995000001.\n",
      "[I 2025-08-26 13:00:19,836] Trial 29 finished with value: -9304.352271 and parameters: {'learning_rate': 9.157643688358015e-05, 'buffer_size': 55763, 'learning_starts': 567, 'tau': 0.0032369290765451894, 'gamma': 0.9774626691754179}. Best is trial 7 with value: -6989.269995000001.\n",
      "[I 2025-08-26 13:01:09,253] Trial 30 finished with value: -8193.2052487 and parameters: {'learning_rate': 7.735341551637222e-05, 'buffer_size': 86794, 'learning_starts': 598, 'tau': 0.004813662188666336, 'gamma': 0.9917380286248382}. Best is trial 7 with value: -6989.269995000001.\n",
      "[I 2025-08-26 13:01:51,000] Trial 31 finished with value: -9311.0439707 and parameters: {'learning_rate': 5.917929116448849e-05, 'buffer_size': 32374, 'learning_starts': 988, 'tau': 0.004399589585079196, 'gamma': 0.967986842130375}. Best is trial 7 with value: -6989.269995000001.\n",
      "[I 2025-08-26 13:02:33,135] Trial 32 finished with value: -6979.450722299999 and parameters: {'learning_rate': 3.8254710251077613e-05, 'buffer_size': 29289, 'learning_starts': 882, 'tau': 0.0035445586679051016, 'gamma': 0.9715138072303137}. Best is trial 32 with value: -6979.450722299999.\n",
      "[I 2025-08-26 13:03:20,290] Trial 33 finished with value: -12997.693118399999 and parameters: {'learning_rate': 3.405466869526271e-05, 'buffer_size': 25156, 'learning_starts': 884, 'tau': 0.003432370626138376, 'gamma': 0.9848076976542512}. Best is trial 32 with value: -6979.450722299999.\n",
      "[I 2025-08-26 13:04:04,379] Trial 34 finished with value: -7135.930227700001 and parameters: {'learning_rate': 4.2991857294285314e-05, 'buffer_size': 51068, 'learning_starts': 786, 'tau': 0.0020781008836217637, 'gamma': 0.9592362813220708}. Best is trial 32 with value: -6979.450722299999.\n",
      "[I 2025-08-26 13:04:50,543] Trial 35 finished with value: -7031.5937951 and parameters: {'learning_rate': 9.867618098544908e-05, 'buffer_size': 41035, 'learning_starts': 415, 'tau': 0.00275509062041821, 'gamma': 0.9739031805700106}. Best is trial 32 with value: -6979.450722299999.\n",
      "[I 2025-08-26 13:05:38,280] Trial 36 finished with value: -6975.5928146999995 and parameters: {'learning_rate': 1.950643051379202e-05, 'buffer_size': 16607, 'learning_starts': 241, 'tau': 0.001035357693891918, 'gamma': 0.9809451042146053}. Best is trial 36 with value: -6975.5928146999995.\n",
      "[I 2025-08-26 13:06:26,526] Trial 37 finished with value: -7048.866646500001 and parameters: {'learning_rate': 1.0131349768573368e-05, 'buffer_size': 27805, 'learning_starts': 221, 'tau': 0.001142613295510327, 'gamma': 0.9836635533578006}. Best is trial 36 with value: -6975.5928146999995.\n",
      "[I 2025-08-26 13:07:19,707] Trial 38 finished with value: -6964.259726900001 and parameters: {'learning_rate': 1.4665648283400494e-05, 'buffer_size': 20301, 'learning_starts': 133, 'tau': 0.0018890922811299826, 'gamma': 0.9632626300967182}. Best is trial 38 with value: -6964.259726900001.\n",
      "[I 2025-08-26 13:08:11,420] Trial 39 finished with value: -7086.9555499 and parameters: {'learning_rate': 1.4117230507094515e-05, 'buffer_size': 37551, 'learning_starts': 112, 'tau': 0.001699895456684678, 'gamma': 0.9631542730411887}. Best is trial 38 with value: -6964.259726900001.\n",
      "[I 2025-08-26 13:09:01,650] Trial 40 finished with value: -7054.5000577 and parameters: {'learning_rate': 2.0123019843601233e-05, 'buffer_size': 23203, 'learning_starts': 180, 'tau': 0.0010806160400041558, 'gamma': 0.9486241336089682}. Best is trial 38 with value: -6964.259726900001.\n",
      "[I 2025-08-26 13:09:49,033] Trial 41 finished with value: -7131.7219004 and parameters: {'learning_rate': 1.2751035815971845e-05, 'buffer_size': 15213, 'learning_starts': 255, 'tau': 0.0023521395435131946, 'gamma': 0.9543788416455183}. Best is trial 38 with value: -6964.259726900001.\n",
      "[I 2025-08-26 13:10:36,865] Trial 42 finished with value: -8165.0536578 and parameters: {'learning_rate': 2.7701040659993077e-05, 'buffer_size': 20655, 'learning_starts': 130, 'tau': 0.0017125314435800264, 'gamma': 0.9377034262503268}. Best is trial 38 with value: -6964.259726900001.\n",
      "[I 2025-08-26 13:11:22,941] Trial 43 finished with value: -13052.479782899998 and parameters: {'learning_rate': 1.831382096633487e-05, 'buffer_size': 13620, 'learning_starts': 341, 'tau': 0.0029781655773731953, 'gamma': 0.923585747087337}. Best is trial 38 with value: -6964.259726900001.\n",
      "[I 2025-08-26 13:12:09,108] Trial 44 finished with value: -7033.4583745 and parameters: {'learning_rate': 1.5440826579209858e-05, 'buffer_size': 28462, 'learning_starts': 159, 'tau': 0.002475535656665543, 'gamma': 0.9717309231386323}. Best is trial 38 with value: -6964.259726900001.\n",
      "[I 2025-08-26 13:12:55,595] Trial 45 finished with value: -7021.667711300001 and parameters: {'learning_rate': 3.266993579184566e-05, 'buffer_size': 34670, 'learning_starts': 218, 'tau': 0.003838884034779454, 'gamma': 0.9638781247233109}. Best is trial 38 with value: -6964.259726900001.\n",
      "[I 2025-08-26 13:13:41,497] Trial 46 finished with value: -7072.1958932 and parameters: {'learning_rate': 2.328580579543944e-05, 'buffer_size': 16411, 'learning_starts': 294, 'tau': 0.0019711066863539327, 'gamma': 0.9917803092558263}. Best is trial 38 with value: -6964.259726900001.\n",
      "[I 2025-08-26 13:14:27,349] Trial 47 finished with value: -12881.184689 and parameters: {'learning_rate': 4.8312809139581225e-05, 'buffer_size': 22193, 'learning_starts': 359, 'tau': 0.0031165584337458333, 'gamma': 0.9100346885999069}. Best is trial 38 with value: -6964.259726900001.\n",
      "[I 2025-08-26 13:15:12,947] Trial 48 finished with value: -7006.736572900001 and parameters: {'learning_rate': 1.1348488311543738e-05, 'buffer_size': 65672, 'learning_starts': 217, 'tau': 0.0014127303118492492, 'gamma': 0.9817180490883083}. Best is trial 38 with value: -6964.259726900001.\n",
      "[I 2025-08-26 13:15:59,455] Trial 49 finished with value: -8242.8428549 and parameters: {'learning_rate': 1.1765469928926498e-05, 'buffer_size': 68888, 'learning_starts': 221, 'tau': 0.0014925105858300917, 'gamma': 0.9737196963739567}. Best is trial 38 with value: -6964.259726900001.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- DDPG Optimization Finished ---\n",
      "Number of finished trials:  50\n",
      "Best trial:\n",
      "  Value (Average Reward):  -6964.259726900001\n",
      "  Params: \n",
      "    learning_rate: 1.4665648283400494e-05\n",
      "    buffer_size: 20301\n",
      "    learning_starts: 133\n",
      "    tau: 0.0018890922811299826\n",
      "    gamma: 0.9632626300967182\n",
      "\n",
      "==================================================\n",
      "\n",
      "--- Training the final DDPG model with best hyperparameters ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4c6324baa2e47ca8ca2171401294ba5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='Training final model: <b>DDPG</b>'), IntProgress(value=0, description='Training DDP…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved to: Output/DDPG/best_model.pt\n",
      "\n",
      "--- Evaluating the final DDPG model ---\n",
      "Evaluation logs saved to: Output/DDPG/evaluation_logs.csv\n",
      "\n",
      "Final DDPG Metrics (Optimized):\n",
      "Average Reward: -7.1283\n",
      "Average Battery Health: 100.0000\n",
      "Average Efficiency: 6.8181\n",
      "Average Regret: 7.8101\n",
      "Energy Fulfillment Rate: 21.8359\n",
      "Evaluation plots saved to: Output/DDPG\n",
      "\n",
      "--- Training benchmark DDPG model (default/minimal hyperparameters) ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c97296b0da3143358c8145f485278dae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='Training benchmark model: <b>DDPG</b>'), IntProgress(value=0, description='Benchmar…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark model saved to: Output/DDPG/Benchmark/benchmark_model.pt\n",
      "\n",
      "--- Evaluating benchmark DDPG model ---\n",
      "Benchmark evaluation logs saved to: Output/DDPG/Benchmark/evaluation_logs.csv\n",
      "\n",
      "Benchmark DDPG Metrics:\n",
      "Average Reward: -12.9945\n",
      "Average Battery Health: 50.0007\n",
      "Average Efficiency: 6.8110\n",
      "Average Regret: 13.6756\n",
      "Energy Fulfillment Rate: 11.5377\n",
      "Benchmark plots saved to: Output/DDPG/Benchmark\n",
      "Comparison plots saved to: Output/DDPG\n",
      "\n",
      "==================================================\n",
      "\n",
      "--- Starting Optuna optimization for SAC with 50 trials ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1c5db9dc15043a68b7e08c0293628b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='Tuning model: <b>SAC</b>, Trial 0/50'), IntProgress(value=0, description='Tuning SA…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-26 13:18:06,385] A new study created in memory with name: SAC_optimization\n",
      "[I 2025-08-26 13:19:38,651] Trial 0 finished with value: -8209.032794 and parameters: {'learning_rate': 0.0004095202020890013, 'buffer_size': 39382, 'learning_starts': 233, 'gamma': 0.9077209257760507, 'tau': 0.00965887115362453, 'ent_coef': 1.9163201520389505e-07}. Best is trial 0 with value: -8209.032794.\n",
      "[I 2025-08-26 13:21:02,199] Trial 1 finished with value: -7039.5385857 and parameters: {'learning_rate': 0.0002496763363932993, 'buffer_size': 75889, 'learning_starts': 467, 'gamma': 0.9387805985116915, 'tau': 0.009171197707985967, 'ent_coef': 0.008172559803037962}. Best is trial 1 with value: -7039.5385857.\n",
      "[I 2025-08-26 13:22:32,506] Trial 2 finished with value: -6949.694054000001 and parameters: {'learning_rate': 0.00015111315571406605, 'buffer_size': 75822, 'learning_starts': 377, 'gamma': 0.9711861886667172, 'tau': 0.005050057360864647, 'ent_coef': 0.00028378203480776886}. Best is trial 2 with value: -6949.694054000001.\n",
      "[I 2025-08-26 13:24:07,485] Trial 3 finished with value: -6881.2547294999995 and parameters: {'learning_rate': 2.142911064059296e-05, 'buffer_size': 79402, 'learning_starts': 381, 'gamma': 0.9462135677920402, 'tau': 0.001512338806563059, 'ent_coef': 8.280331795414046e-05}. Best is trial 3 with value: -6881.2547294999995.\n",
      "[I 2025-08-26 13:25:45,251] Trial 4 finished with value: -7028.154932 and parameters: {'learning_rate': 3.4982860138965375e-05, 'buffer_size': 50441, 'learning_starts': 707, 'gamma': 0.9630031181222105, 'tau': 0.009642953669178313, 'ent_coef': 1.6850741728361605e-06}. Best is trial 3 with value: -6881.2547294999995.\n",
      "[I 2025-08-26 13:27:16,490] Trial 5 finished with value: -9325.1920358 and parameters: {'learning_rate': 0.0009245732856548462, 'buffer_size': 73942, 'learning_starts': 647, 'gamma': 0.9788883760034192, 'tau': 0.00391083432265244, 'ent_coef': 9.716606170051707e-08}. Best is trial 3 with value: -6881.2547294999995.\n",
      "[I 2025-08-26 13:28:49,750] Trial 6 finished with value: -7104.3563883 and parameters: {'learning_rate': 3.7066855034047525e-05, 'buffer_size': 65111, 'learning_starts': 881, 'gamma': 0.9391037540051173, 'tau': 0.008572356754019055, 'ent_coef': 5.567913201652974e-08}. Best is trial 3 with value: -6881.2547294999995.\n",
      "[I 2025-08-26 13:30:30,562] Trial 7 finished with value: -9255.709846000002 and parameters: {'learning_rate': 1.382324036284388e-05, 'buffer_size': 12840, 'learning_starts': 171, 'gamma': 0.9469906208165007, 'tau': 0.0071014185801412544, 'ent_coef': 2.9405527119267162e-06}. Best is trial 3 with value: -6881.2547294999995.\n",
      "[I 2025-08-26 13:32:02,575] Trial 8 finished with value: -7009.3541259 and parameters: {'learning_rate': 2.8080094233583775e-05, 'buffer_size': 74632, 'learning_starts': 291, 'gamma': 0.9523191825679261, 'tau': 0.006278006974353302, 'ent_coef': 1.4052550533209683e-06}. Best is trial 3 with value: -6881.2547294999995.\n",
      "[I 2025-08-26 13:33:38,947] Trial 9 finished with value: -7049.9119607 and parameters: {'learning_rate': 0.00011638896941814335, 'buffer_size': 27023, 'learning_starts': 207, 'gamma': 0.9416926588366777, 'tau': 0.009933220955695668, 'ent_coef': 0.0005675894683824944}. Best is trial 3 with value: -6881.2547294999995.\n",
      "[I 2025-08-26 13:35:12,573] Trial 10 finished with value: -7717.591495000001 and parameters: {'learning_rate': 1.0541497811024503e-05, 'buffer_size': 93183, 'learning_starts': 542, 'gamma': 0.9149840219413292, 'tau': 0.001044199815225323, 'ent_coef': 0.04066747228690041}. Best is trial 3 with value: -6881.2547294999995.\n",
      "[I 2025-08-26 13:36:47,354] Trial 11 finished with value: -6971.5051699000005 and parameters: {'learning_rate': 9.394274989043127e-05, 'buffer_size': 98894, 'learning_starts': 397, 'gamma': 0.9955350029049783, 'tau': 0.0036712074591386396, 'ent_coef': 0.0001623220555493408}. Best is trial 3 with value: -6881.2547294999995.\n",
      "[I 2025-08-26 13:38:18,080] Trial 12 finished with value: -7056.774064599999 and parameters: {'learning_rate': 9.385219326105262e-05, 'buffer_size': 86358, 'learning_starts': 353, 'gamma': 0.9733077836596489, 'tau': 0.0012776113389364697, 'ent_coef': 4.8359577070899816e-05}. Best is trial 3 with value: -6881.2547294999995.\n",
      "[I 2025-08-26 13:39:43,662] Trial 13 finished with value: -7021.8701006 and parameters: {'learning_rate': 0.00019326399677336043, 'buffer_size': 58544, 'learning_starts': 486, 'gamma': 0.9901305949086915, 'tau': 0.0031811486274662066, 'ent_coef': 0.0016668862046381797}. Best is trial 3 with value: -6881.2547294999995.\n",
      "[I 2025-08-26 13:41:08,913] Trial 14 finished with value: -6931.9885696 and parameters: {'learning_rate': 6.131940266098157e-05, 'buffer_size': 79166, 'learning_starts': 648, 'gamma': 0.9619922619016603, 'tau': 0.004714884130117305, 'ent_coef': 1.414080431284315e-05}. Best is trial 3 with value: -6881.2547294999995.\n",
      "[I 2025-08-26 13:42:37,136] Trial 15 finished with value: -7023.2760727 and parameters: {'learning_rate': 4.940353707952879e-05, 'buffer_size': 46540, 'learning_starts': 819, 'gamma': 0.9209526020288465, 'tau': 0.002571445788041098, 'ent_coef': 1.3840404557308876e-05}. Best is trial 3 with value: -6881.2547294999995.\n",
      "[I 2025-08-26 13:44:01,013] Trial 16 finished with value: -7023.5059198 and parameters: {'learning_rate': 1.6573530534573093e-05, 'buffer_size': 86269, 'learning_starts': 669, 'gamma': 0.9286257198817343, 'tau': 0.004853759304049927, 'ent_coef': 2.1262721781804105e-05}. Best is trial 3 with value: -6881.2547294999995.\n",
      "[I 2025-08-26 13:45:34,651] Trial 17 finished with value: -7028.9712656 and parameters: {'learning_rate': 6.0646439890602366e-05, 'buffer_size': 64270, 'learning_starts': 983, 'gamma': 0.9547344142671556, 'tau': 0.0021778529513264283, 'ent_coef': 0.004245272882743509}. Best is trial 3 with value: -6881.2547294999995.\n",
      "[I 2025-08-26 13:47:05,889] Trial 18 finished with value: -7089.540768000001 and parameters: {'learning_rate': 2.4042120369259263e-05, 'buffer_size': 89671, 'learning_starts': 564, 'gamma': 0.960651201330508, 'tau': 0.006803599482398001, 'ent_coef': 1.0359937440515154e-08}. Best is trial 3 with value: -6881.2547294999995.\n",
      "[I 2025-08-26 13:48:34,233] Trial 19 finished with value: -7131.4729387 and parameters: {'learning_rate': 6.070126891542711e-05, 'buffer_size': 64980, 'learning_starts': 782, 'gamma': 0.9309325687070593, 'tau': 0.007832101853203169, 'ent_coef': 6.214732034791317e-06}. Best is trial 3 with value: -6881.2547294999995.\n",
      "[I 2025-08-26 13:49:59,652] Trial 20 finished with value: -7143.069008700001 and parameters: {'learning_rate': 2.1616041646336798e-05, 'buffer_size': 82374, 'learning_starts': 596, 'gamma': 0.9000310112949788, 'tau': 0.005572102390102187, 'ent_coef': 9.193341223814281e-05}. Best is trial 3 with value: -6881.2547294999995.\n",
      "[I 2025-08-26 13:51:26,225] Trial 21 finished with value: -7130.902839800001 and parameters: {'learning_rate': 0.00016044732165321405, 'buffer_size': 77854, 'learning_starts': 105, 'gamma': 0.970929280620902, 'tau': 0.004747584627576667, 'ent_coef': 0.0003232755182562768}. Best is trial 3 with value: -6881.2547294999995.\n",
      "[I 2025-08-26 13:52:51,686] Trial 22 finished with value: -6998.6211916 and parameters: {'learning_rate': 0.0003208513550415752, 'buffer_size': 97620, 'learning_starts': 400, 'gamma': 0.9797677535630458, 'tau': 0.004638467962236782, 'ent_coef': 0.0006309571119214608}. Best is trial 3 with value: -6881.2547294999995.\n",
      "[I 2025-08-26 13:54:18,578] Trial 23 finished with value: -7082.654774099999 and parameters: {'learning_rate': 0.0005035215795562754, 'buffer_size': 67957, 'learning_starts': 308, 'gamma': 0.9624434193457061, 'tau': 0.005828619656062301, 'ent_coef': 5.459292394318658e-05}. Best is trial 3 with value: -6881.2547294999995.\n",
      "[I 2025-08-26 13:55:42,474] Trial 24 finished with value: -7024.8383495 and parameters: {'learning_rate': 0.00013671050980022922, 'buffer_size': 81460, 'learning_starts': 498, 'gamma': 0.9677661044126172, 'tau': 0.0023722047964467707, 'ent_coef': 1.2676989766202047e-05}. Best is trial 3 with value: -6881.2547294999995.\n",
      "[I 2025-08-26 13:57:06,139] Trial 25 finished with value: -7105.866827899999 and parameters: {'learning_rate': 7.111678676937507e-05, 'buffer_size': 57441, 'learning_starts': 432, 'gamma': 0.9846690683610765, 'tau': 0.004155336701936736, 'ent_coef': 0.0014347022911275877}. Best is trial 3 with value: -6881.2547294999995.\n",
      "[I 2025-08-26 13:58:32,770] Trial 26 finished with value: -7059.8612041 and parameters: {'learning_rate': 3.9255334151614184e-05, 'buffer_size': 70825, 'learning_starts': 294, 'gamma': 0.9569258418263477, 'tau': 0.0029801880666730897, 'ent_coef': 6.019007012382643e-07}. Best is trial 3 with value: -6881.2547294999995.\n",
      "[I 2025-08-26 13:59:59,605] Trial 27 finished with value: -7140.0326005 and parameters: {'learning_rate': 0.00020745390796086661, 'buffer_size': 92494, 'learning_starts': 722, 'gamma': 0.9468439957271879, 'tau': 0.0052672042903087754, 'ent_coef': 0.0001549200727061419}. Best is trial 3 with value: -6881.2547294999995.\n",
      "[I 2025-08-26 14:01:21,977] Trial 28 finished with value: -7130.724839899999 and parameters: {'learning_rate': 7.447302681006254e-05, 'buffer_size': 81614, 'learning_starts': 613, 'gamma': 0.9758679074651659, 'tau': 0.0017677212595956847, 'ent_coef': 0.009708916429929673}. Best is trial 3 with value: -6881.2547294999995.\n",
      "[I 2025-08-26 14:02:43,939] Trial 29 finished with value: -7137.4866113 and parameters: {'learning_rate': 1.7160718681133975e-05, 'buffer_size': 36208, 'learning_starts': 353, 'gamma': 0.9324880642949198, 'tau': 0.006426591420940192, 'ent_coef': 4.771774623384106e-07}. Best is trial 3 with value: -6881.2547294999995.\n",
      "[I 2025-08-26 14:04:17,390] Trial 30 finished with value: -7255.6861061 and parameters: {'learning_rate': 0.0005409409596516248, 'buffer_size': 51322, 'learning_starts': 237, 'gamma': 0.9673179793001799, 'tau': 0.007580302942569634, 'ent_coef': 0.09141266591754157}. Best is trial 3 with value: -6881.2547294999995.\n",
      "[I 2025-08-26 14:05:47,925] Trial 31 finished with value: -7127.7142586 and parameters: {'learning_rate': 0.00010576766199944298, 'buffer_size': 98573, 'learning_starts': 385, 'gamma': 0.9975099749380102, 'tau': 0.0036810915704581803, 'ent_coef': 0.0002079830614020068}. Best is trial 3 with value: -6881.2547294999995.\n",
      "[I 2025-08-26 14:07:16,822] Trial 32 finished with value: -7030.903942499999 and parameters: {'learning_rate': 9.033902747427548e-05, 'buffer_size': 99759, 'learning_starts': 446, 'gamma': 0.9969756754037499, 'tau': 0.003309379124425275, 'ent_coef': 3.5769370438338616e-05}. Best is trial 3 with value: -6881.2547294999995.\n",
      "[I 2025-08-26 14:08:48,229] Trial 33 finished with value: -7107.5356433 and parameters: {'learning_rate': 0.00025918740778118704, 'buffer_size': 88774, 'learning_starts': 481, 'gamma': 0.9853572436783105, 'tau': 0.004433090491637148, 'ent_coef': 0.00014846376735757626}. Best is trial 3 with value: -6881.2547294999995.\n",
      "[I 2025-08-26 14:10:15,802] Trial 34 finished with value: -7083.5552583 and parameters: {'learning_rate': 4.7702276514776095e-05, 'buffer_size': 77978, 'learning_starts': 541, 'gamma': 0.9866321679995403, 'tau': 0.0037583455335713067, 'ent_coef': 4.472663731434455e-06}. Best is trial 3 with value: -6881.2547294999995.\n",
      "[I 2025-08-26 14:11:45,166] Trial 35 finished with value: -6896.701434799999 and parameters: {'learning_rate': 0.00016668301128167117, 'buffer_size': 70633, 'learning_starts': 346, 'gamma': 0.9916027130071884, 'tau': 0.00511451014747493, 'ent_coef': 0.001176922722596272}. Best is trial 3 with value: -6881.2547294999995.\n",
      "[I 2025-08-26 14:13:10,614] Trial 36 finished with value: -7203.570965000001 and parameters: {'learning_rate': 0.00014779150283722995, 'buffer_size': 72076, 'learning_starts': 249, 'gamma': 0.9439468960817035, 'tau': 0.005246923630236959, 'ent_coef': 0.002625222276224635}. Best is trial 3 with value: -6881.2547294999995.\n",
      "[I 2025-08-26 14:14:35,881] Trial 37 finished with value: -7060.799798 and parameters: {'learning_rate': 0.00031879790945357463, 'buffer_size': 60966, 'learning_starts': 155, 'gamma': 0.9505691931538678, 'tau': 0.006072904275344794, 'ent_coef': 0.012504342996035771}. Best is trial 3 with value: -6881.2547294999995.\n",
      "[I 2025-08-26 14:16:10,983] Trial 38 finished with value: -7038.049814399999 and parameters: {'learning_rate': 3.338818284723426e-05, 'buffer_size': 70704, 'learning_starts': 336, 'gamma': 0.9793894277820283, 'tau': 0.008663090400655545, 'ent_coef': 0.0006329555356963373}. Best is trial 3 with value: -6881.2547294999995.\n",
      "[I 2025-08-26 14:17:42,299] Trial 39 finished with value: -7044.499046199999 and parameters: {'learning_rate': 0.0002252963384664296, 'buffer_size': 79273, 'learning_starts': 724, 'gamma': 0.9583339184313995, 'tau': 0.005076190481636294, 'ent_coef': 0.0004267300928587893}. Best is trial 3 with value: -6881.2547294999995.\n",
      "[I 2025-08-26 14:19:15,185] Trial 40 finished with value: -7015.1750988 and parameters: {'learning_rate': 0.0009100542678049693, 'buffer_size': 74797, 'learning_starts': 263, 'gamma': 0.9681420170746177, 'tau': 0.004269917566588791, 'ent_coef': 9.038629056921129e-06}. Best is trial 3 with value: -6881.2547294999995.\n",
      "[I 2025-08-26 14:20:48,763] Trial 41 finished with value: -7197.8287308 and parameters: {'learning_rate': 0.00011978416743712175, 'buffer_size': 83419, 'learning_starts': 412, 'gamma': 0.9920085406665757, 'tau': 0.0016016814723049293, 'ent_coef': 8.129363639583122e-05}. Best is trial 3 with value: -6881.2547294999995.\n",
      "[I 2025-08-26 14:22:20,323] Trial 42 finished with value: -6913.4810328 and parameters: {'learning_rate': 0.0001651087561652662, 'buffer_size': 94856, 'learning_starts': 378, 'gamma': 0.992956864532091, 'tau': 0.005669668277246858, 'ent_coef': 0.0010322031065002032}. Best is trial 3 with value: -6881.2547294999995.\n",
      "[I 2025-08-26 14:23:53,570] Trial 43 finished with value: -6986.5436707 and parameters: {'learning_rate': 0.00017732960400680344, 'buffer_size': 93512, 'learning_starts': 194, 'gamma': 0.9370155180294025, 'tau': 0.006681137228398675, 'ent_coef': 0.0013037620318759877}. Best is trial 3 with value: -6881.2547294999995.\n",
      "[I 2025-08-26 14:25:30,057] Trial 44 finished with value: -7049.2147119 and parameters: {'learning_rate': 0.00028943150942417644, 'buffer_size': 69097, 'learning_starts': 518, 'gamma': 0.9918312920364326, 'tau': 0.005634323688604578, 'ent_coef': 0.0052356330279080794}. Best is trial 3 with value: -6881.2547294999995.\n",
      "[I 2025-08-26 14:27:03,435] Trial 45 finished with value: -7114.877716100001 and parameters: {'learning_rate': 0.00012819152332603438, 'buffer_size': 85687, 'learning_starts': 360, 'gamma': 0.9835152587555751, 'tau': 0.0074190067220033275, 'ent_coef': 0.02373425027817602}. Best is trial 3 with value: -6881.2547294999995.\n",
      "[I 2025-08-26 14:28:39,312] Trial 46 finished with value: -8075.7699348 and parameters: {'learning_rate': 1.0746407616506273e-05, 'buffer_size': 74947, 'learning_starts': 437, 'gamma': 0.9753763179589974, 'tau': 0.006349963126583256, 'ent_coef': 0.0011220069602433814}. Best is trial 3 with value: -6881.2547294999995.\n",
      "[I 2025-08-26 14:30:13,640] Trial 47 finished with value: -7134.324214599999 and parameters: {'learning_rate': 0.0001658894748823226, 'buffer_size': 90497, 'learning_starts': 310, 'gamma': 0.9638462820397314, 'tau': 0.005761118834962053, 'ent_coef': 2.629055113092775e-05}. Best is trial 3 with value: -6881.2547294999995.\n",
      "[I 2025-08-26 14:31:38,491] Trial 48 finished with value: -7113.966473 and parameters: {'learning_rate': 9.073979002625462e-05, 'buffer_size': 62482, 'learning_starts': 634, 'gamma': 0.9541918563374212, 'tau': 0.008270005700077573, 'ent_coef': 0.0034670836483830523}. Best is trial 3 with value: -6881.2547294999995.\n",
      "[I 2025-08-26 14:33:02,179] Trial 49 finished with value: -7089.9798735 and parameters: {'learning_rate': 0.0004302135406382384, 'buffer_size': 43202, 'learning_starts': 683, 'gamma': 0.9897064496765455, 'tau': 0.009286079479221946, 'ent_coef': 1.8303103277019794e-06}. Best is trial 3 with value: -6881.2547294999995.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- SAC Optimization Finished ---\n",
      "Number of finished trials:  50\n",
      "Best trial:\n",
      "  Value (Average Reward):  -6881.2547294999995\n",
      "  Params: \n",
      "    learning_rate: 2.142911064059296e-05\n",
      "    buffer_size: 79402\n",
      "    learning_starts: 381\n",
      "    gamma: 0.9462135677920402\n",
      "    tau: 0.001512338806563059\n",
      "    ent_coef: 8.280331795414046e-05\n",
      "\n",
      "==================================================\n",
      "\n",
      "--- Training the final SAC model with best hyperparameters ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cffb08579357433fb7a5e28f92570c87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='Training final model: <b>SAC</b>'), IntProgress(value=0, description='Training SAC:…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved to: Output/SAC/best_model.pt\n",
      "\n",
      "--- Evaluating the final SAC model ---\n",
      "Evaluation logs saved to: Output/SAC/evaluation_logs.csv\n",
      "\n",
      "Final SAC Metrics (Optimized):\n",
      "Average Reward: -7.0327\n",
      "Average Battery Health: 99.9924\n",
      "Average Efficiency: 6.8322\n",
      "Average Regret: 7.7160\n",
      "Energy Fulfillment Rate: 22.8367\n",
      "Evaluation plots saved to: Output/SAC\n",
      "\n",
      "--- Training benchmark SAC model (default/minimal hyperparameters) ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aebd2bc507f94dbda91b143dc7ced1dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='Training benchmark model: <b>SAC</b>'), IntProgress(value=0, description='Benchmark…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark model saved to: Output/SAC/Benchmark/benchmark_model.pt\n",
      "\n",
      "--- Evaluating benchmark SAC model ---\n",
      "Benchmark evaluation logs saved to: Output/SAC/Benchmark/evaluation_logs.csv\n",
      "\n",
      "Benchmark SAC Metrics:\n",
      "Average Reward: -7.5198\n",
      "Average Battery Health: 98.1055\n",
      "Average Efficiency: 6.8108\n",
      "Average Regret: 8.2009\n",
      "Energy Fulfillment Rate: 21.3778\n",
      "Benchmark plots saved to: Output/SAC/Benchmark\n",
      "Comparison plots saved to: Output/SAC\n",
      "\n",
      "==================================================\n",
      "\n",
      "--- Starting Optuna optimization for DQN with 50 trials ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1cdad221a0d4985b0606b34dddfd1d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='Tuning model: <b>DQN</b>, Trial 0/50'), IntProgress(value=0, description='Tuning DQ…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-26 14:36:34,422] A new study created in memory with name: DQN_optimization\n",
      "[I 2025-08-26 14:36:48,920] Trial 0 finished with value: -11588.6732661 and parameters: {'learning_rate': 3.3288241145915604e-05, 'buffer_size': 21470, 'learning_starts': 763, 'gamma': 0.9194696055197532, 'exploration_fraction': 0.29541725941334473, 'exploration_final_eps': 0.09103434659324033, 'train_freq': 2, 'target_update_interval': 617}. Best is trial 0 with value: -11588.6732661.\n",
      "[I 2025-08-26 14:36:56,874] Trial 1 finished with value: -12252.3871977 and parameters: {'learning_rate': 1.953431469005632e-05, 'buffer_size': 90249, 'learning_starts': 153, 'gamma': 0.9818992141007812, 'exploration_fraction': 0.3859691526218201, 'exploration_final_eps': 0.018428518497275833, 'train_freq': 7, 'target_update_interval': 920}. Best is trial 0 with value: -11588.6732661.\n",
      "[I 2025-08-26 14:37:10,703] Trial 2 finished with value: -11625.3453415 and parameters: {'learning_rate': 0.0001442705217120918, 'buffer_size': 56260, 'learning_starts': 190, 'gamma': 0.9852463632922646, 'exploration_fraction': 0.31892895451994174, 'exploration_final_eps': 0.06301259080811052, 'train_freq': 2, 'target_update_interval': 963}. Best is trial 0 with value: -11588.6732661.\n",
      "[I 2025-08-26 14:37:16,888] Trial 3 finished with value: -11717.211314799999 and parameters: {'learning_rate': 8.224673273966634e-05, 'buffer_size': 94834, 'learning_starts': 834, 'gamma': 0.9142910711770701, 'exploration_fraction': 0.452502505601047, 'exploration_final_eps': 0.01267563990444622, 'train_freq': 10, 'target_update_interval': 576}. Best is trial 0 with value: -11588.6732661.\n",
      "[I 2025-08-26 14:37:23,982] Trial 4 finished with value: -11596.7648277 and parameters: {'learning_rate': 0.0004730517134115319, 'buffer_size': 79769, 'learning_starts': 755, 'gamma': 0.9307365044782522, 'exploration_fraction': 0.31728989399716057, 'exploration_final_eps': 0.08143677344410694, 'train_freq': 9, 'target_update_interval': 575}. Best is trial 0 with value: -11588.6732661.\n",
      "[I 2025-08-26 14:37:32,865] Trial 5 finished with value: -11626.8526345 and parameters: {'learning_rate': 0.00019671564942931325, 'buffer_size': 45568, 'learning_starts': 558, 'gamma': 0.9619507850893275, 'exploration_fraction': 0.4667191186182823, 'exploration_final_eps': 0.05740705822104816, 'train_freq': 5, 'target_update_interval': 970}. Best is trial 0 with value: -11588.6732661.\n",
      "[I 2025-08-26 14:37:40,157] Trial 6 finished with value: -11645.4984154 and parameters: {'learning_rate': 0.0008520793238386814, 'buffer_size': 53533, 'learning_starts': 734, 'gamma': 0.9297202887312758, 'exploration_fraction': 0.2953250989187781, 'exploration_final_eps': 0.06493731521151332, 'train_freq': 8, 'target_update_interval': 921}. Best is trial 0 with value: -11588.6732661.\n",
      "[I 2025-08-26 14:37:46,594] Trial 7 finished with value: -11688.4868892 and parameters: {'learning_rate': 3.094820642350786e-05, 'buffer_size': 58631, 'learning_starts': 676, 'gamma': 0.9688360850854058, 'exploration_fraction': 0.31676740581851975, 'exploration_final_eps': 0.02631539103543991, 'train_freq': 10, 'target_update_interval': 335}. Best is trial 0 with value: -11588.6732661.\n",
      "[I 2025-08-26 14:37:56,721] Trial 8 finished with value: -12731.656913499999 and parameters: {'learning_rate': 1.1523352591404347e-05, 'buffer_size': 48528, 'learning_starts': 676, 'gamma': 0.9358879802286182, 'exploration_fraction': 0.16132681080225753, 'exploration_final_eps': 0.05420613833984581, 'train_freq': 4, 'target_update_interval': 882}. Best is trial 0 with value: -11588.6732661.\n",
      "[I 2025-08-26 14:38:06,043] Trial 9 finished with value: -12135.5319175 and parameters: {'learning_rate': 0.0007628304574583484, 'buffer_size': 94916, 'learning_starts': 813, 'gamma': 0.9251784652672995, 'exploration_fraction': 0.1669611526288755, 'exploration_final_eps': 0.07827471022336736, 'train_freq': 4, 'target_update_interval': 912}. Best is trial 0 with value: -11588.6732661.\n",
      "[I 2025-08-26 14:38:27,376] Trial 10 finished with value: -11587.1973308 and parameters: {'learning_rate': 4.412592392182136e-05, 'buffer_size': 10186, 'learning_starts': 976, 'gamma': 0.9018141925612653, 'exploration_fraction': 0.22720861699115735, 'exploration_final_eps': 0.09914180564754664, 'train_freq': 1, 'target_update_interval': 126}. Best is trial 10 with value: -11587.1973308.\n",
      "[I 2025-08-26 14:38:48,985] Trial 11 finished with value: -11557.3301299 and parameters: {'learning_rate': 5.554823659181968e-05, 'buffer_size': 10669, 'learning_starts': 964, 'gamma': 0.9004755084937643, 'exploration_fraction': 0.22735378129506476, 'exploration_final_eps': 0.09992167723363055, 'train_freq': 1, 'target_update_interval': 119}. Best is trial 11 with value: -11557.3301299.\n",
      "[I 2025-08-26 14:39:10,904] Trial 12 finished with value: -11619.835898 and parameters: {'learning_rate': 5.9568185950998126e-05, 'buffer_size': 14539, 'learning_starts': 980, 'gamma': 0.9011789769991452, 'exploration_fraction': 0.21983551851457803, 'exploration_final_eps': 0.09411233853146506, 'train_freq': 1, 'target_update_interval': 136}. Best is trial 11 with value: -11557.3301299.\n",
      "[I 2025-08-26 14:39:33,575] Trial 13 finished with value: -11634.272895700002 and parameters: {'learning_rate': 4.7904337791323146e-05, 'buffer_size': 31335, 'learning_starts': 978, 'gamma': 0.9012255810220218, 'exploration_fraction': 0.22295860895843136, 'exploration_final_eps': 0.09931648189236204, 'train_freq': 1, 'target_update_interval': 126}. Best is trial 11 with value: -11557.3301299.\n",
      "[I 2025-08-26 14:39:45,468] Trial 14 finished with value: -11547.732114 and parameters: {'learning_rate': 0.0002475954532479415, 'buffer_size': 10718, 'learning_starts': 450, 'gamma': 0.9454871690055238, 'exploration_fraction': 0.11076776318701655, 'exploration_final_eps': 0.0407473644795911, 'train_freq': 3, 'target_update_interval': 313}. Best is trial 14 with value: -11547.732114.\n",
      "[I 2025-08-26 14:39:56,763] Trial 15 finished with value: -11566.0390755 and parameters: {'learning_rate': 0.0003333600003400329, 'buffer_size': 34728, 'learning_starts': 315, 'gamma': 0.9502559248575543, 'exploration_fraction': 0.10115199443555933, 'exploration_final_eps': 0.03544217834226692, 'train_freq': 3, 'target_update_interval': 305}. Best is trial 14 with value: -11547.732114.\n",
      "[I 2025-08-26 14:40:04,551] Trial 16 finished with value: -11539.4117023 and parameters: {'learning_rate': 0.00013849011319336143, 'buffer_size': 25421, 'learning_starts': 405, 'gamma': 0.9973918886232371, 'exploration_fraction': 0.13352434062824758, 'exploration_final_eps': 0.04298490024502239, 'train_freq': 6, 'target_update_interval': 344}. Best is trial 16 with value: -11539.4117023.\n",
      "[I 2025-08-26 14:40:12,371] Trial 17 finished with value: -11634.110510800001 and parameters: {'learning_rate': 0.0002107460062804237, 'buffer_size': 26041, 'learning_starts': 400, 'gamma': 0.9517321706541668, 'exploration_fraction': 0.10296856974464824, 'exploration_final_eps': 0.04109719623660936, 'train_freq': 6, 'target_update_interval': 365}. Best is trial 16 with value: -11539.4117023.\n",
      "[I 2025-08-26 14:40:20,235] Trial 18 finished with value: -11608.7597535 and parameters: {'learning_rate': 0.00011754448991541324, 'buffer_size': 38795, 'learning_starts': 446, 'gamma': 0.9957126604525285, 'exploration_fraction': 0.15858279215752116, 'exploration_final_eps': 0.048592396493760494, 'train_freq': 6, 'target_update_interval': 424}. Best is trial 16 with value: -11539.4117023.\n",
      "[I 2025-08-26 14:40:28,862] Trial 19 finished with value: -11949.8547703 and parameters: {'learning_rate': 0.00034308654227446376, 'buffer_size': 65370, 'learning_starts': 526, 'gamma': 0.9409656667372218, 'exploration_fraction': 0.13354965999892043, 'exploration_final_eps': 0.037820169695825155, 'train_freq': 5, 'target_update_interval': 249}. Best is trial 16 with value: -11539.4117023.\n",
      "[I 2025-08-26 14:40:36,105] Trial 20 finished with value: -13331.7395292 and parameters: {'learning_rate': 0.00019632612660332107, 'buffer_size': 18591, 'learning_starts': 269, 'gamma': 0.9626631972567541, 'exploration_fraction': 0.18380580688464734, 'exploration_final_eps': 0.027478824865652195, 'train_freq': 7, 'target_update_interval': 473}. Best is trial 16 with value: -11539.4117023.\n",
      "[I 2025-08-26 14:40:46,314] Trial 21 finished with value: -11623.2826876 and parameters: {'learning_rate': 8.266962268718169e-05, 'buffer_size': 10413, 'learning_starts': 525, 'gamma': 0.9794359353050432, 'exploration_fraction': 0.25919631599844556, 'exploration_final_eps': 0.04720556186177527, 'train_freq': 3, 'target_update_interval': 223}. Best is trial 16 with value: -11539.4117023.\n",
      "[I 2025-08-26 14:40:56,656] Trial 22 finished with value: -11595.214384300001 and parameters: {'learning_rate': 0.0001308463491500107, 'buffer_size': 26391, 'learning_starts': 364, 'gamma': 0.997896534745403, 'exploration_fraction': 0.12859118613807524, 'exploration_final_eps': 0.0706728888337518, 'train_freq': 3, 'target_update_interval': 690}. Best is trial 16 with value: -11539.4117023.\n",
      "[I 2025-08-26 14:41:10,920] Trial 23 finished with value: -11616.0073327 and parameters: {'learning_rate': 0.0003261465071896008, 'buffer_size': 20353, 'learning_starts': 447, 'gamma': 0.9446457336206852, 'exploration_fraction': 0.19478382792978455, 'exploration_final_eps': 0.02981678930392477, 'train_freq': 2, 'target_update_interval': 207}. Best is trial 16 with value: -11539.4117023.\n",
      "[I 2025-08-26 14:41:20,935] Trial 24 finished with value: -11619.9362507 and parameters: {'learning_rate': 7.640126265041352e-05, 'buffer_size': 17013, 'learning_starts': 584, 'gamma': 0.9132043142899616, 'exploration_fraction': 0.13795236853601717, 'exploration_final_eps': 0.04978771386233071, 'train_freq': 4, 'target_update_interval': 455}. Best is trial 16 with value: -11539.4117023.\n",
      "[I 2025-08-26 14:41:28,563] Trial 25 finished with value: -11548.739620299999 and parameters: {'learning_rate': 0.000595909607703693, 'buffer_size': 27741, 'learning_starts': 279, 'gamma': 0.9683358906908155, 'exploration_fraction': 0.2507518311027381, 'exploration_final_eps': 0.04223413590209519, 'train_freq': 7, 'target_update_interval': 271}. Best is trial 16 with value: -11539.4117023.\n",
      "[I 2025-08-26 14:41:35,915] Trial 26 finished with value: -11533.9658601 and parameters: {'learning_rate': 0.000487817190873354, 'buffer_size': 40026, 'learning_starts': 231, 'gamma': 0.9728269362236015, 'exploration_fraction': 0.38497799256059595, 'exploration_final_eps': 0.04183308372015461, 'train_freq': 7, 'target_update_interval': 289}. Best is trial 26 with value: -11533.9658601.\n",
      "[I 2025-08-26 14:41:42,969] Trial 27 finished with value: -11605.9539709 and parameters: {'learning_rate': 0.0004636487085891041, 'buffer_size': 41098, 'learning_starts': 102, 'gamma': 0.991378395087394, 'exploration_fraction': 0.41290076031821654, 'exploration_final_eps': 0.0331208802641714, 'train_freq': 8, 'target_update_interval': 379}. Best is trial 26 with value: -11533.9658601.\n",
      "[I 2025-08-26 14:41:51,193] Trial 28 finished with value: -11598.8734191 and parameters: {'learning_rate': 0.0002074803480301388, 'buffer_size': 34754, 'learning_starts': 225, 'gamma': 0.9773509163172698, 'exploration_fraction': 0.3742634378801484, 'exploration_final_eps': 0.019666510877108644, 'train_freq': 6, 'target_update_interval': 494}. Best is trial 26 with value: -11533.9658601.\n",
      "[I 2025-08-26 14:41:58,725] Trial 29 finished with value: -11582.5905828 and parameters: {'learning_rate': 0.00028240686514294065, 'buffer_size': 23530, 'learning_starts': 441, 'gamma': 0.9573705655991956, 'exploration_fraction': 0.49874464987929035, 'exploration_final_eps': 0.05650732338284963, 'train_freq': 8, 'target_update_interval': 716}. Best is trial 26 with value: -11533.9658601.\n",
      "[I 2025-08-26 14:42:07,104] Trial 30 finished with value: -11528.796316700002 and parameters: {'learning_rate': 0.0009886738995473101, 'buffer_size': 67441, 'learning_starts': 363, 'gamma': 0.9706259662154824, 'exploration_fraction': 0.3661820948786042, 'exploration_final_eps': 0.040167360611551094, 'train_freq': 5, 'target_update_interval': 405}. Best is trial 30 with value: -11528.796316700002.\n",
      "[I 2025-08-26 14:42:15,580] Trial 31 finished with value: -11597.574233199999 and parameters: {'learning_rate': 0.0009079921872957816, 'buffer_size': 74328, 'learning_starts': 360, 'gamma': 0.972642843902758, 'exploration_fraction': 0.3941227026671516, 'exploration_final_eps': 0.04378549943165191, 'train_freq': 5, 'target_update_interval': 398}. Best is trial 30 with value: -11528.796316700002.\n",
      "[I 2025-08-26 14:42:22,839] Trial 32 finished with value: -11656.4662315 and parameters: {'learning_rate': 0.0005628861305637051, 'buffer_size': 72428, 'learning_starts': 334, 'gamma': 0.9870706166612283, 'exploration_fraction': 0.3419270866303866, 'exploration_final_eps': 0.038418018222572535, 'train_freq': 7, 'target_update_interval': 314}. Best is trial 30 with value: -11528.796316700002.\n",
      "[I 2025-08-26 14:42:30,760] Trial 33 finished with value: -12640.6270416 and parameters: {'learning_rate': 0.0006946026704683222, 'buffer_size': 84664, 'learning_starts': 208, 'gamma': 0.9560538536480371, 'exploration_fraction': 0.3453955230824012, 'exploration_final_eps': 0.020969998042870336, 'train_freq': 6, 'target_update_interval': 510}. Best is trial 30 with value: -11528.796316700002.\n",
      "[I 2025-08-26 14:42:38,818] Trial 34 finished with value: -11628.6540275 and parameters: {'learning_rate': 0.00045869352740968226, 'buffer_size': 62938, 'learning_starts': 154, 'gamma': 0.98524826133052, 'exploration_fraction': 0.4259189393114734, 'exploration_final_eps': 0.05123410233989055, 'train_freq': 5, 'target_update_interval': 206}. Best is trial 30 with value: -11528.796316700002.\n",
      "[I 2025-08-26 14:42:45,768] Trial 35 finished with value: -11561.6300098 and parameters: {'learning_rate': 0.00016287972756098636, 'buffer_size': 46743, 'learning_starts': 487, 'gamma': 0.9729519458945295, 'exploration_fraction': 0.3681762106682482, 'exploration_final_eps': 0.03291387433927989, 'train_freq': 9, 'target_update_interval': 422}. Best is trial 30 with value: -11528.796316700002.\n",
      "[I 2025-08-26 14:42:54,814] Trial 36 finished with value: -11661.3505266 and parameters: {'learning_rate': 0.0009753748082208006, 'buffer_size': 40583, 'learning_starts': 273, 'gamma': 0.9418454206601089, 'exploration_fraction': 0.29206495944956695, 'exploration_final_eps': 0.060656799154587174, 'train_freq': 4, 'target_update_interval': 292}. Best is trial 30 with value: -11528.796316700002.\n",
      "[I 2025-08-26 14:43:01,780] Trial 37 finished with value: -11579.1859639 and parameters: {'learning_rate': 0.0002738656169662729, 'buffer_size': 50914, 'learning_starts': 596, 'gamma': 0.9642082225230229, 'exploration_fraction': 0.4371637084546871, 'exploration_final_eps': 0.01505448267146979, 'train_freq': 7, 'target_update_interval': 362}. Best is trial 30 with value: -11528.796316700002.\n",
      "[I 2025-08-26 14:43:08,223] Trial 38 finished with value: -11670.778301199998 and parameters: {'learning_rate': 0.00039016151225184156, 'buffer_size': 67210, 'learning_starts': 408, 'gamma': 0.9908076455955009, 'exploration_fraction': 0.39673515817699917, 'exploration_final_eps': 0.04412362514303341, 'train_freq': 9, 'target_update_interval': 182}. Best is trial 30 with value: -11528.796316700002.\n",
      "[I 2025-08-26 14:43:21,310] Trial 39 finished with value: -13056.390841 and parameters: {'learning_rate': 0.0006287838624149961, 'buffer_size': 31030, 'learning_starts': 306, 'gamma': 0.9558836635866055, 'exploration_fraction': 0.3457921453831917, 'exploration_final_eps': 0.02529487943624673, 'train_freq': 2, 'target_update_interval': 549}. Best is trial 30 with value: -11528.796316700002.\n",
      "[I 2025-08-26 14:43:29,027] Trial 40 finished with value: -11558.008506 and parameters: {'learning_rate': 0.00011241085689270367, 'buffer_size': 60195, 'learning_starts': 239, 'gamma': 0.975188439285224, 'exploration_fraction': 0.27030911026181437, 'exploration_final_eps': 0.07037116955446107, 'train_freq': 6, 'target_update_interval': 345}. Best is trial 30 with value: -11528.796316700002.\n",
      "[I 2025-08-26 14:43:36,389] Trial 41 finished with value: -12283.015889700002 and parameters: {'learning_rate': 0.0005691962509048628, 'buffer_size': 27331, 'learning_starts': 284, 'gamma': 0.9668823515411928, 'exploration_fraction': 0.26193603433784274, 'exploration_final_eps': 0.04176018984745543, 'train_freq': 7, 'target_update_interval': 271}. Best is trial 30 with value: -11528.796316700002.\n",
      "[I 2025-08-26 14:43:43,763] Trial 42 finished with value: -11571.861929499999 and parameters: {'learning_rate': 0.0007561330744867939, 'buffer_size': 22626, 'learning_starts': 162, 'gamma': 0.9805127269855575, 'exploration_fraction': 0.11738786416245614, 'exploration_final_eps': 0.05339860780685383, 'train_freq': 8, 'target_update_interval': 261}. Best is trial 30 with value: -11528.796316700002.\n",
      "[I 2025-08-26 14:43:51,391] Trial 43 finished with value: -12741.578607 and parameters: {'learning_rate': 0.0002588261706801191, 'buffer_size': 16010, 'learning_starts': 410, 'gamma': 0.9703761933242858, 'exploration_fraction': 0.3236572731880509, 'exploration_final_eps': 0.04555940768316168, 'train_freq': 7, 'target_update_interval': 172}. Best is trial 30 with value: -11528.796316700002.\n",
      "[I 2025-08-26 14:43:58,136] Trial 44 finished with value: -11641.5795346 and parameters: {'learning_rate': 0.00016053699124778228, 'buffer_size': 56161, 'learning_starts': 350, 'gamma': 0.946819086154654, 'exploration_fraction': 0.4623703010016599, 'exploration_final_eps': 0.03843950087978981, 'train_freq': 8, 'target_update_interval': 316}. Best is trial 30 with value: -11528.796316700002.\n",
      "[I 2025-08-26 14:44:06,318] Trial 45 finished with value: -9511.901341 and parameters: {'learning_rate': 0.00047844368913275726, 'buffer_size': 30365, 'learning_starts': 493, 'gamma': 0.9371702930251026, 'exploration_fraction': 0.19626344221088687, 'exploration_final_eps': 0.0313666910435807, 'train_freq': 5, 'target_update_interval': 622}. Best is trial 45 with value: -9511.901341.\n",
      "[I 2025-08-26 14:44:15,457] Trial 46 finished with value: -13316.3138557 and parameters: {'learning_rate': 0.0004885331763116917, 'buffer_size': 36281, 'learning_starts': 490, 'gamma': 0.9305154020289083, 'exploration_fraction': 0.14898040993567466, 'exploration_final_eps': 0.03061287281870257, 'train_freq': 4, 'target_update_interval': 648}. Best is trial 45 with value: -9511.901341.\n",
      "[I 2025-08-26 14:44:23,421] Trial 47 finished with value: -11621.633350799999 and parameters: {'learning_rate': 0.0004123794818025779, 'buffer_size': 87981, 'learning_starts': 640, 'gamma': 0.9382722102440015, 'exploration_fraction': 0.18039037522601908, 'exploration_final_eps': 0.035049158158697274, 'train_freq': 5, 'target_update_interval': 762}. Best is trial 45 with value: -9511.901341.\n",
      "[I 2025-08-26 14:44:34,326] Trial 48 finished with value: -11724.9270013 and parameters: {'learning_rate': 0.0002513396712245938, 'buffer_size': 44772, 'learning_starts': 383, 'gamma': 0.9252481078110313, 'exploration_fraction': 0.20568400920147772, 'exploration_final_eps': 0.011565944385155433, 'train_freq': 3, 'target_update_interval': 617}. Best is trial 45 with value: -9511.901341.\n",
      "[I 2025-08-26 14:44:44,001] Trial 49 finished with value: -11553.4541892 and parameters: {'learning_rate': 2.7972304991783872e-05, 'buffer_size': 30801, 'learning_starts': 495, 'gamma': 0.9364430305197489, 'exploration_fraction': 0.17018485725221558, 'exploration_final_eps': 0.026845587387285298, 'train_freq': 4, 'target_update_interval': 801}. Best is trial 45 with value: -9511.901341.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- DQN Optimization Finished ---\n",
      "Number of finished trials:  50\n",
      "Best trial:\n",
      "  Value (Average Reward):  -9511.901341\n",
      "  Params: \n",
      "    learning_rate: 0.00047844368913275726\n",
      "    buffer_size: 30365\n",
      "    learning_starts: 493\n",
      "    gamma: 0.9371702930251026\n",
      "    exploration_fraction: 0.19626344221088687\n",
      "    exploration_final_eps: 0.0313666910435807\n",
      "    train_freq: 5\n",
      "    target_update_interval: 622\n",
      "\n",
      "==================================================\n",
      "\n",
      "--- Training the final DQN model with best hyperparameters ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c0bb850e0274e0e9a34a966146ae126",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='Training final model: <b>DQN</b>'), IntProgress(value=0, description='Training DQN:…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved to: Output/DQN/best_model.pt\n",
      "\n",
      "--- Evaluating the final DQN model ---\n",
      "Evaluation logs saved to: Output/DQN/evaluation_logs.csv\n",
      "\n",
      "Final DQN Metrics (Optimized):\n",
      "Average Reward: -12.5406\n",
      "Average Battery Health: 60.7256\n",
      "Average Efficiency: 6.8291\n",
      "Average Regret: 13.2235\n",
      "Energy Fulfillment Rate: 6.2503\n",
      "Evaluation plots saved to: Output/DQN\n",
      "\n",
      "--- Training benchmark DQN model (default/minimal hyperparameters) ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bee1648f1c848d9b3f2b52322460a7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='Training benchmark model: <b>DQN</b>'), IntProgress(value=0, description='Benchmark…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark model saved to: Output/DQN/Benchmark/benchmark_model.pt\n",
      "\n",
      "--- Evaluating benchmark DQN model ---\n",
      "Benchmark evaluation logs saved to: Output/DQN/Benchmark/evaluation_logs.csv\n",
      "\n",
      "Benchmark DQN Metrics:\n",
      "Average Reward: -11.5859\n",
      "Average Battery Health: 90.0000\n",
      "Average Efficiency: 6.8117\n",
      "Average Regret: 12.2671\n",
      "Energy Fulfillment Rate: 17.7735\n",
      "Benchmark plots saved to: Output/DQN/Benchmark\n",
      "Comparison plots saved to: Output/DQN\n",
      "\n",
      "==================================================\n",
      "\n",
      "\n",
      "--- All models have been processed. The script has completed. ---\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    models_to_tune = ['PPO', 'A2C', 'DDPG', 'SAC', 'DQN']\n",
    "    n_trials = 50\n",
    "    timesteps_per_trial =  10000\n",
    "    evaluation_timesteps = 10000\n",
    "\n",
    "    # Create the main output directory\n",
    "    output_dir = \"Output\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # --- Setup Overall Progress Bar ---\n",
    "    overall_progress = IntProgress(min=0, max=len(models_to_tune), description='Overall Progress:')\n",
    "    overall_description = HTML('Overall Progress: <b>0</b>/5 models completed.')\n",
    "    overall_vbox = VBox([overall_description, overall_progress])\n",
    "    display(overall_vbox)\n",
    "\n",
    "    for i, model_name in enumerate(models_to_tune):\n",
    "        # Create a specific directory for the current model\n",
    "        model_output_dir = os.path.join(output_dir, model_name)\n",
    "        os.makedirs(model_output_dir, exist_ok=True)\n",
    "        # Create a directory for benchmark outputs\n",
    "        benchmark_output_dir = os.path.join(model_output_dir, \"Benchmark\")\n",
    "        os.makedirs(benchmark_output_dir, exist_ok=True)\n",
    "\n",
    "        overall_description.value = f'Overall Progress: Currently processing <b>{model_name}</b> ({i+1}/{len(models_to_tune)})'\n",
    "        \n",
    "        print(f\"--- Starting Optuna optimization for {model_name} with {n_trials} trials ---\")\n",
    "\n",
    "        optuna_progress = IntProgress(min=0, max=n_trials, description=f'Tuning {model_name}:')\n",
    "        optuna_description = HTML(f'Tuning model: <b>{model_name}</b>, Trial 0/{n_trials}')\n",
    "        optuna_vbox = VBox([optuna_description, optuna_progress])\n",
    "        display(optuna_vbox)\n",
    "\n",
    "        study = optuna.create_study(\n",
    "            direction='maximize',\n",
    "            study_name=f'{model_name}_optimization'\n",
    "        )\n",
    "        # Pass the timesteps variable to the objective function\n",
    "        func = lambda trial: objective(trial, model_name, timesteps_per_trial)\n",
    "        study.optimize(func, n_trials=n_trials, callbacks=[OptunaProgressCallback(optuna_progress, optuna_description, n_trials)])\n",
    "        \n",
    "        optuna_progress.value = n_trials\n",
    "        optuna_description.value = f'Tuning model: <b>{model_name}</b>, Trial {n_trials}/{n_trials} - Complete!'\n",
    "\n",
    "        print(f\"\\n--- {model_name} Optimization Finished ---\")\n",
    "        print(\"Number of finished trials: \", len(study.trials))\n",
    "        best_trial = study.best_trial\n",
    "        best_trial_params = best_trial.params\n",
    "\n",
    "        print(\"Best trial:\")\n",
    "        print(\"  Value (Average Reward): \", best_trial.value)\n",
    "        print(\"  Params: \")\n",
    "        for key, value in best_trial_params.items():\n",
    "            print(f\"    {key}: {value}\")\n",
    "\n",
    "        print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "        print(f\"--- Training the final {model_name} model with best hyperparameters ---\")\n",
    "\n",
    "        training_progress = IntProgress(min=0, max=evaluation_timesteps, description=f'Training {model_name}:')\n",
    "        training_description = HTML(f'Training final model: <b>{model_name}</b>')\n",
    "        training_vbox = VBox([training_description, training_progress])\n",
    "        display(training_vbox)\n",
    "\n",
    "        if model_name == \"DQN\":\n",
    "            env_class = TEGDiscreteEnvironment\n",
    "            policy_name = \"MlpPolicy\"\n",
    "        else:\n",
    "            env_class = TEGEnvironment\n",
    "            policy_name = \"MlpPolicy\"\n",
    "\n",
    "        model_class = globals()[model_name]\n",
    "\n",
    "        final_env = make_vec_env(lambda: env_class(), n_envs=1)\n",
    "        final_model = model_class(\n",
    "            policy_name,\n",
    "            final_env,\n",
    "            **best_trial_params,\n",
    "            verbose=0,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        final_model.learn(total_timesteps=evaluation_timesteps, callback=TrainingProgressCallback(training_progress))\n",
    "\n",
    "        training_progress.value = evaluation_timesteps\n",
    "        training_description.value = f'Training final model: <b>{model_name}</b> - Complete!'\n",
    "\n",
    "        # Save the best model\n",
    "        model_save_path = os.path.join(model_output_dir, \"best_model.pt\")\n",
    "        final_model.save(model_save_path)\n",
    "        print(f\"Best model saved to: {model_save_path}\")\n",
    "\n",
    "        print(f\"\\n--- Evaluating the final {model_name} model ---\")\n",
    "        eval_env_final = make_vec_env(lambda: env_class(), n_envs=1)\n",
    "        \n",
    "        reset_output = eval_env_final.reset()\n",
    "        if isinstance(reset_output, tuple):\n",
    "            obs, info = reset_output\n",
    "        else:\n",
    "            obs = reset_output\n",
    "            info = {}\n",
    "        \n",
    "        all_logs_opt = []\n",
    "        for _ in range(evaluation_timesteps):\n",
    "            action, _states = final_model.predict(obs, deterministic=True)\n",
    "            \n",
    "            step_output = eval_env_final.step(action)\n",
    "            \n",
    "            if len(step_output) == 5:\n",
    "                obs, reward, terminated, truncated, info = step_output\n",
    "            else:\n",
    "                obs, reward, done, info = step_output\n",
    "                terminated = done\n",
    "                truncated = False\n",
    "\n",
    "            df_logs = eval_env_final.envs[0].env.get_logs()\n",
    "            if not df_logs.empty:\n",
    "                all_logs_opt.append(df_logs.iloc[-1])\n",
    "            \n",
    "            if terminated or truncated:\n",
    "                reset_output = eval_env_final.reset()\n",
    "                if isinstance(reset_output, tuple):\n",
    "                    obs, info = reset_output\n",
    "                else:\n",
    "                    obs = reset_output\n",
    "                    info = {}\n",
    "        \n",
    "        df_final_opt = None\n",
    "        if all_logs_opt:\n",
    "            df_final_opt = pd.DataFrame(all_logs_opt)\n",
    "\n",
    "            # Save the final logs to a CSV file\n",
    "            logs_save_path = os.path.join(model_output_dir, \"evaluation_logs.csv\")\n",
    "            df_final_opt.to_csv(logs_save_path, index=False)\n",
    "            print(f\"Evaluation logs saved to: {logs_save_path}\")\n",
    "\n",
    "            metrics_final = calculate_metrics(df_final_opt)\n",
    "\n",
    "            print(f\"\\nFinal {model_name} Metrics (Optimized):\")\n",
    "            for key, value in metrics_final.items():\n",
    "                if value is not None:\n",
    "                    print(f\"{key}: {value:.4f}\")\n",
    "            \n",
    "            # Save the plots\n",
    "            plot_results(df_final_opt, f\"Final Optimized {model_name}\", model_output_dir, eval_env_final.envs[0].env.max_steps)\n",
    "            print(f\"Evaluation plots saved to: {model_output_dir}\")\n",
    "        else:\n",
    "            print(f\"No logs were collected for {model_name} optimized model due to early termination.\")\n",
    "\n",
    "        # -------------------- BENCHMARK TRAINING (Minimal/No tuning) --------------------\n",
    "        print(f\"\\n--- Training benchmark {model_name} model (default/minimal hyperparameters) ---\")\n",
    "        bench_training_progress = IntProgress(min=0, max=evaluation_timesteps, description=f'Benchmark {model_name}:')\n",
    "        bench_training_description = HTML(f'Training benchmark model: <b>{model_name}</b>')\n",
    "        bench_training_vbox = VBox([bench_training_description, bench_training_progress])\n",
    "        display(bench_training_vbox)\n",
    "\n",
    "        # Define minimal hyperparameters (none or one key param, keep consistent across models)\n",
    "        benchmark_params = {}\n",
    "        # For PPO, set only learning_rate; others default\n",
    "        if model_name == \"PPO\":\n",
    "            benchmark_params = {\"learning_rate\": 3e-4}\n",
    "        elif model_name == \"A2C\":\n",
    "            benchmark_params = {\"learning_rate\": 7e-4}\n",
    "        elif model_name == \"DDPG\":\n",
    "            benchmark_params = {\"learning_rate\": 1e-3}\n",
    "        elif model_name == \"SAC\":\n",
    "            benchmark_params = {\"learning_rate\": 3e-4}\n",
    "        elif model_name == \"DQN\":\n",
    "            benchmark_params = {\"learning_rate\": 1e-3}\n",
    "\n",
    "        bench_env = make_vec_env(lambda: env_class(), n_envs=1)\n",
    "        bench_model = model_class(\n",
    "            policy_name,\n",
    "            bench_env,\n",
    "            **benchmark_params,\n",
    "            verbose=0,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        bench_model.learn(total_timesteps=evaluation_timesteps, callback=TrainingProgressCallback(bench_training_progress))\n",
    "        bench_training_progress.value = evaluation_timesteps\n",
    "        bench_training_description.value = f'Training benchmark model: <b>{model_name}</b> - Complete!'\n",
    "\n",
    "        # Save the benchmark model\n",
    "        bench_model_path = os.path.join(benchmark_output_dir, \"benchmark_model.pt\")\n",
    "        bench_model.save(bench_model_path)\n",
    "        print(f\"Benchmark model saved to: {bench_model_path}\")\n",
    "\n",
    "        # Evaluate benchmark\n",
    "        print(f\"\\n--- Evaluating benchmark {model_name} model ---\")\n",
    "        bench_eval_env = make_vec_env(lambda: env_class(), n_envs=1)\n",
    "        reset_output = bench_eval_env.reset()\n",
    "        if isinstance(reset_output, tuple):\n",
    "            obs_b, info_b = reset_output\n",
    "        else:\n",
    "            obs_b = reset_output\n",
    "            info_b = {}\n",
    "\n",
    "        all_logs_bench = []\n",
    "        for _ in range(evaluation_timesteps):\n",
    "            action_b, _states_b = bench_model.predict(obs_b, deterministic=True)\n",
    "            step_output_b = bench_eval_env.step(action_b)\n",
    "\n",
    "            if len(step_output_b) == 5:\n",
    "                obs_b, reward_b, terminated_b, truncated_b, info_b = step_output_b\n",
    "            else:\n",
    "                obs_b, reward_b, done_b, info_b = step_output_b\n",
    "                terminated_b = done_b\n",
    "                truncated_b = False\n",
    "\n",
    "            df_logs_b = bench_eval_env.envs[0].env.get_logs()\n",
    "            if not df_logs_b.empty:\n",
    "                all_logs_bench.append(df_logs_b.iloc[-1])\n",
    "\n",
    "            if terminated_b or truncated_b:\n",
    "                reset_output = bench_eval_env.reset()\n",
    "                if isinstance(reset_output, tuple):\n",
    "                    obs_b, info_b = reset_output\n",
    "                else:\n",
    "                    obs_b = reset_output\n",
    "                    info_b = {}\n",
    "\n",
    "        df_final_bench = None\n",
    "        if all_logs_bench:\n",
    "            df_final_bench = pd.DataFrame(all_logs_bench)\n",
    "            bench_logs_path = os.path.join(benchmark_output_dir, \"evaluation_logs.csv\")\n",
    "            df_final_bench.to_csv(bench_logs_path, index=False)\n",
    "            print(f\"Benchmark evaluation logs saved to: {bench_logs_path}\")\n",
    "\n",
    "            metrics_bench = calculate_metrics(df_final_bench)\n",
    "            print(f\"\\nBenchmark {model_name} Metrics:\")\n",
    "            for key, value in metrics_bench.items():\n",
    "                if value is not None:\n",
    "                    print(f\"{key}: {value:.4f}\")\n",
    "\n",
    "            # Individual plots for benchmark\n",
    "            plot_results(df_final_bench, f\"Benchmark {model_name}\", benchmark_output_dir, bench_eval_env.envs[0].env.max_steps)\n",
    "            print(f\"Benchmark plots saved to: {benchmark_output_dir}\")\n",
    "        else:\n",
    "            print(f\"No logs were collected for {model_name} benchmark due to early termination.\")\n",
    "\n",
    "        # -------------------- COMPARISON PLOTS --------------------\n",
    "        if df_final_opt is not None and df_final_bench is not None:\n",
    "            plot_comparison(\n",
    "                df_final_opt,\n",
    "                df_final_bench,\n",
    "                labels=(\"Optimized\", \"Benchmark\"),\n",
    "                save_path=model_output_dir,\n",
    "                max_steps=eval_env_final.envs[0].env.max_steps\n",
    "            )\n",
    "            print(f\"Comparison plots saved to: {model_output_dir}\")\n",
    "\n",
    "        print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "        \n",
    "        overall_progress.value += 1\n",
    "        overall_description.value = f'Overall Progress: Finished <b>{model_name}</b> ({i+1}/{len(models_to_tune)})'\n",
    "    \n",
    "    overall_description.value = f'Overall Progress: All {len(models_to_tune)} models completed!'\n",
    "    print(\"\\n--- All models have been processed. The script has completed. ---\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_GPU_DL_PyTorch (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
